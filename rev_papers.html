<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CNN Change Detection - Revisión papers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./Ms-CapsNet.html" rel="next">
<link href="./data-cube.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./rev_papers.html">Revisión papers</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CNN Change Detection</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./abstract.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Abstract</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a-estudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Área de Estudio</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adquisicion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Adquisición de Datos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metodologia.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Metodologia</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusiones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conclusiones</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Rezagos</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./convencionales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Métodos Convencionales</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./recursos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recursos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./problema.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Problema u Oportunidad</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./e-arte.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Estado del Arte</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hipotesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hipótesis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./objetivos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Objetivos</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./marco.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Marco Conceptual</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./insumos.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Tratamiento de Insumos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./remote_sensing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Percepción Remota</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sar_sentinel_s1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sentinel 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./semantic-seg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Segmentación Semántica</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ecosistemas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ecosistemas Protegidos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-cube.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Cube</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./rev_papers.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Revisión papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Ms-CapsNet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Ms-CapsNet</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RUSACD.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RUSACD</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#detección-de-cambios-con-imágenes-radar" id="toc-detección-de-cambios-con-imágenes-radar" class="nav-link active" data-scroll-target="#detección-de-cambios-con-imágenes-radar">Detección de Cambios con Imágenes Radar</a>
  <ul class="collapse">
  <li><a href="#ms-capsnet" id="toc-ms-capsnet" class="nav-link" data-scroll-target="#ms-capsnet">Ms-CapsNet</a></li>
  </ul></li>
  <li><a href="#graph-based-knowledge-supplement" id="toc-graph-based-knowledge-supplement" class="nav-link" data-scroll-target="#graph-based-knowledge-supplement">Graph-Based Knowledge Supplement</a></li>
  <li><a href="#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022" id="toc-fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022" class="nav-link" data-scroll-target="#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022">Fully Transformer Network for Change Detection of Remote Sensing Images <span class="citation" data-cites="yan_fully_2022">(Yan, Wan, and Zhang 2022)</span></a></li>
  <li><a href="#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022" id="toc-self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022" class="nav-link" data-scroll-target="#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022">Self-supervised Vision Transformers for Joint SAR-optical Representation Learning <span class="citation" data-cites="wang_self-supervised_2022">(Y. Wang, Albrecht, and Zhu 2022)</span></a></li>
  <li><a href="#detección-de-cambios-con-imágenes-de-alta-resolución" id="toc-detección-de-cambios-con-imágenes-de-alta-resolución" class="nav-link" data-scroll-target="#detección-de-cambios-con-imágenes-de-alta-resolución">Detección de Cambios con Imágenes de Alta Resolución</a>
  <ul class="collapse">
  <li><a href="#dasnet" id="toc-dasnet" class="nav-link" data-scroll-target="#dasnet">DASNet</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-papers" class="quarto-section-identifier">Revisión papers</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>En está sección se registrará los aspectos generales de la revisión bibliográfica a modo de conocer el estado del arte en técnicas de identificación de cambios en imagenes satelitales utilizado redes neuronales convolucionales.</p>
<p><strong>Principales Desafíos:</strong></p>
<ol type="1">
<li>Ruidos Moteado <span class="citation" data-cites="gao2021">(<a href="references.html#ref-gao2021" role="doc-biblioref">Gao et al. 2021</a>)</span></li>
<li>Sesibilidad a la Deformación <span class="citation" data-cites="gao2021">(<a href="references.html#ref-gao2021" role="doc-biblioref">Gao et al. 2021</a>)</span></li>
<li>Cantidad de Muestras de Entrenamiento de Cambios <span class="citation" data-cites="wang2022">(<a href="references.html#ref-wang2022" role="doc-biblioref">J. Wang et al. 2022</a>)</span></li>
<li>Limitada capacidad de representación de las características visuales extraídas <span class="citation" data-cites="yan_fully_2022">(<a href="references.html#ref-yan_fully_2022" role="doc-biblioref">Yan, Wan, and Zhang 2022</a>)</span></li>
</ol>
<p><strong>Posibles Soluciones</strong></p>
<section id="detección-de-cambios-con-imágenes-radar" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="detección-de-cambios-con-imágenes-radar">Detección de Cambios con Imágenes Radar</h2>
<section id="ms-capsnet" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="ms-capsnet">Ms-CapsNet</h3>
<p>SAR Image Change Detection Based on Multiscale Capsule Network <span class="citation" data-cites="gao2021">(<a href="references.html#ref-gao2021" role="doc-biblioref">Gao et al. 2021</a>)</span></p>
<dl>
<dt>Resumen:</dt>
<dd>
Los métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los <strong>retos del ruido de moteado y la sensibilidad a la deformación</strong>. Para mitigar estos problemas, propone una red de cápsulas multiescala (<strong>Ms-CapsNet</strong>) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se ha diseñado un módulo de convolución de <strong>fusión adaptativa (AFC)</strong> para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.
</dd>
<dt>Index Terms:</dt>
<dd>
<p>Change detection, multiscale capsule network, synthetic aperture radar, deep learning.</p>
</dd>
</dl>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/Ms-CapsNet_network.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Ms-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala</figcaption>
</figure>
</div>
<p>Resultados y Análisis de Experimentos.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/Ms-CapsNet-results.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Visualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet.</figcaption>
</figure>
</div>
</section>
</section>
<section id="graph-based-knowledge-supplement" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="graph-based-knowledge-supplement">Graph-Based Knowledge Supplement</h2>
<p>Change Detection From Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network <span class="citation" data-cites="wang2022">(<a href="references.html#ref-wang2022" role="doc-biblioref">J. Wang et al. 2022</a>)</span></p>
<dl>
<dt>Resumen:</dt>
<dd>
La detección de cambios en las imágenes del radar de apertura sintética (SAR) es una tarea vital pero difícil en el campo del análisis de imágenes de teledetección. La mayoría de los trabajos anteriores adoptan un método auto-supervisado que utiliza muestras pseudo-etiquetadas para guiar el entrenamiento y las pruebas subsecuentes. Sin embargo, las redes profundas suelen requerir muchas muestras de alta calidad para la optimización de los parámetros. El ruido en las pseudo-etiquetas afecta inevitablemente al rendimiento final de la detección de cambios. Para resolver el problema, proponemos una red de complemento de conocimiento basada en <strong>grafos (GKSNet)</strong>. Para ser más específicos, extraemos información discriminatoria del conjunto de datos etiquetados existente como conocimiento adicional, para suprimir hasta cierto punto los efectos adversos de las muestras ruidosas. A continuación, diseñamos un módulo de transferencia de grafos para destilar información contextual de forma atenta desde el conjunto de datos etiquetados al conjunto de datos de destino, lo que permite salvar la correlación de características entre los conjuntos de datos. Para validar el método propuesto, realizamos amplios experimentos con cuatro conjuntos de datos de SAR, que demostraron la superioridad de la GKSNet propuesta en comparación con varias líneas de base del estado de la técnica.
</dd>
<dt>Index Terms:</dt>
<dd>
<p>Change detection, graph dependency fusion, knowledge supplement network, synthetic aperture radar (SAR).</p>
</dd>
</dl>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/GKSNet.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Schematic illustration of the proposed GKSNet. The image features extracted by deep CNNs are projected into a graph representation. Then, the graph representations are transferred and fused via graph transfer module across different datasets. Finally, features from different graphs are fused via intergraph fusion module. Through feature fusion, the model exploits the common knowledge and bridge the feature correlation from different datasets. The obtained evolved features are capable of improving the change detection performance.</figcaption>
</figure>
</div>
</section>
<section id="fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022">Fully Transformer Network for Change Detection of Remote Sensing Images <span class="citation" data-cites="yan_fully_2022">(<a href="references.html#ref-yan_fully_2022" role="doc-biblioref">Yan, Wan, and Zhang 2022</a>)</span></h2>
<dl>
<dt>Resumen:</dt>
<dd>
Recientemente, la detección de cambios (CD) de las imágenes de teledetección ha logrado un gran progreso con los avances del aprendizaje profundo. Sin embargo, los métodos actuales generalmente ofrecen regiones de CD incompletas y límites de CD irregulares debido a la limitada capacidad de representación de las características visuales extraídas. Para aliviar estos problemas, en este trabajo proponemos un novedoso marco de aprendizaje llamado Fully Transformer Network (FTN) para la CD de imágenes de teledetección, que mejora la extracción de características desde una vista global y combina características visuales de varios niveles de forma piramidal. Más concretamente, el marco propuesto utiliza en primer lugar las ventajas de los transformadores en el modelado de dependencias de largo alcance. Puede ayudar a aprender más características discriminativas de nivel global y obtener regiones completas de CD. A continuación, introducimos una estructura piramidal para agregar características visuales multinivel a partir de Transformers para mejorar las características. La estructura piramidal injertada con un Módulo de Atención Progresiva (PAM) puede mejorar la capacidad de representación de características con interdependencias adicionales a través de atenciones de canal. Por último, para entrenar mejor el marco, utilizamos el aprendizaje supervisado en profundidad con múltiples funciones de pérdida de límites. Amplios experimentos demuestran que nuestro método propuesto alcanza un nuevo rendimiento de vanguardia en cuatro puntos de referencia públicos de CD.
</dd>
</dl>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/ftn_cd.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption margin-caption">La estructura general del marco propuesto. Fully Transformer Network for Change Detection of Remote Sensing Images</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Index Terms</strong>: Fully Transformer Network, Change Detection, Remote Sensing Image.</p>
</section>
<section id="self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022">Self-supervised Vision Transformers for Joint SAR-optical Representation Learning <span class="citation" data-cites="wang_self-supervised_2022">(<a href="references.html#ref-wang_self-supervised_2022" role="doc-biblioref">Y. Wang, Albrecht, and Zhu 2022</a>)</span></h2>
<dl>
<dt>Resumen:</dt>
<dd>
El aprendizaje autosupervisado (SSL) ha suscitado un gran interés en la teledetección y la observación de la Tierra debido a su capacidad para aprender representaciones independientes de la tarea sin necesidad de anotaciones humanas. Mientras que la mayoría de los trabajos existentes sobre SSL en teledetección utilizan columnas vertebrales ConvNet y se centran en una sola modalidad, nosotros exploramos el potencial de los transformadores de visión (ViTs) para el aprendizaje conjunto de representaciones SAR-ópticas. Basándonos en DINO, un algoritmo SSL de última generación que destila conocimiento de dos vistas aumentadas de una imagen de entrada, combinamos imágenes SAR y ópticas concatenando todos los canales en una entrada unificada. Posteriormente, enmascaramos aleatoriamente los canales de una modalidad como estrategia de aumento de datos. Durante el entrenamiento, el modelo se alimenta de pares de imágenes ópticas, SAR y SAR-ópticas para aprender representaciones internas e intra-modales. Los resultados experimentales que emplean el conjunto de datos BigEarthNet-MM demuestran las ventajas tanto de los ejes ViT como del algoritmo SSL multimodal propuesto DINO-MM.
</dd>
</dl>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/DINO-MM.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption margin-caption">DINO-MM: el algoritmo SSL conjunto SAR-óptico propuesto. La imagen SAR-óptica concatenada se toma como entrada bruta. Se transforma aleatoriamente en dos vistas aumentadas y se introduce en una red maestro-estudiante basada en DINO.</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Index Terms</strong>: Self-supervised learning, vision trans- former, multimodal representation learning</p>
</section>
<section id="detección-de-cambios-con-imágenes-de-alta-resolución" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="detección-de-cambios-con-imágenes-de-alta-resolución">Detección de Cambios con Imágenes de Alta Resolución</h2>
<section id="dasnet" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="dasnet">DASNet</h3>
<p>DASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images<span class="citation" data-cites="chen_dasnet_2021">(<a href="references.html#ref-chen_dasnet_2021" role="doc-biblioref">Chen et al. 2021</a>)</span></p>
<dl>
<dt>Resumen:</dt>
<dd>
La detección de cambios es una tarea básica del procesamiento de imágenes por teledetección. El objetivo de la investigación es identificar la información de cambio de interés y filtrar la información de cambio irrelevante como factores de interferencia. Recientemente, el aumento del aprendizaje profundo ha proporcionado nuevas herramientas para la detección de cambios, que han dado resultados impresionantes. Sin embargo, los métodos disponibles se centran principalmente en la información de diferencia entre las imágenes de teledetección multitemporal y carecen de robustez ante la información de pseudocambio. Para superar la falta de resistencia de los métodos actuales a los pseudocambios, en este trabajo proponemos un nuevo método, a saber, las redes siamesas totalmente convolucionales de atención dual (DASNet), para la detección de cambios en imágenes de alta resolución. A través del mecanismo de atención dual, se capturan las dependencias de largo alcance para obtener representaciones de características más discriminantes para mejorar el rendimiento de reconocimiento del modelo. Además, la muestra desequilibrada es un problema grave en la detección de cambios, es decir, las muestras sin cambios son mucho más abundantes que las muestras con cambios, lo que constituye una de las principales razones de los pseudocambios. Proponemos la pérdida contrastiva ponderada de doble margen para abordar este problema, castigando la atención a los pares de características sin cambios y aumentando la atención a los pares de características con cambios. Los resultados experimentales de nuestro método en el conjunto de datos de detección de cambios (CDD) y en el conjunto de datos de detección de cambios en edificios (BCDD) demuestran que, en comparación con otros métodos de referencia, el método propuesto consigue mejoras máximas del 2,9% y el 4,2%, respectivamente, en la puntuación F1. Nuestra implementación de PyTorch está disponible en <a href="https://github.com/lehaifeng/DASNet" class="uri">https://github.com/lehaifeng/DASNet</a>.
</dd>
<dt>Index Terms:</dt>
<dd>
<p>Change detection, high-resolution images, dual attention, Siamese network, weighted double-margin contrastive loss.</p>
</dd>
</dl>
<div class="cell page-columns page-full" data-layout-align="center">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="images/esq-DASNet.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption margin-caption">DASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite</figcaption>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-chen_dasnet_2021" class="csl-entry" role="listitem">
Chen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, and Haifeng Li. 2021. <span>“<span>DASNet</span>: <span>Dual</span> Attentive Fully Convolutional Siamese Networks for Change Detection of High Resolution Satellite Images.”</span> <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em> 14: 1194–1206. <a href="https://doi.org/10.1109/JSTARS.2020.3037893">https://doi.org/10.1109/JSTARS.2020.3037893</a>.
</div>
<div id="ref-gao2021" class="csl-entry" role="listitem">
Gao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. <span>“SAR Image Change Detection Based on Multiscale Capsule Network.”</span> <em>IEEE Geoscience and Remote Sensing Letters</em> 18 (3): 484–88. <a href="https://doi.org/10.1109/LGRS.2020.2977838">https://doi.org/10.1109/LGRS.2020.2977838</a>.
</div>
<div id="ref-wang2022" class="csl-entry" role="listitem">
Wang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022. <span>“Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network.”</span> <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em> 15: 1823–36. <a href="https://doi.org/10.1109/JSTARS.2022.3146167">https://doi.org/10.1109/JSTARS.2022.3146167</a>.
</div>
<div id="ref-wang_self-supervised_2022" class="csl-entry" role="listitem">
Wang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022. <span>“Self-Supervised <span>Vision Transformers</span> for <span class="nocase">Joint SAR-optical Representation Learning</span>.”</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2204.05381">http://arxiv.org/abs/2204.05381</a>.
</div>
<div id="ref-yan_fully_2022" class="csl-entry" role="listitem">
Yan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. <span>“Fully <span>Transformer Network</span> for <span>Change Detection</span> of <span>Remote Sensing Images</span>.”</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2210.00757">http://arxiv.org/abs/2210.00757</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./data-cube.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Data Cube</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Ms-CapsNet.html" class="pagination-link">
        <span class="nav-page-text">Ms-CapsNet</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2022, Denis Berroeta</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>