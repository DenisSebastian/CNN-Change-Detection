[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CNN Change Detection",
    "section": "",
    "text": "Este documento tiene como objetivo inicial ir registrando los avances en el proceso de experimentación de la Tesis de Master of Sciesnce in Data Science 2022 de la Universidad Adolfo Ibáñez.\nSe pretende un sistema sistema de detección de cambio secuencia a diferente nivles.\n\nDetección de diferencias con imágenes tipo radar Sentinel S1\nClasificación Tipo de cambio (Forestal, Turberas, Humedales, etc.) Sentinel S2\nSegmentación Semántica para identificar pertubacionales y poligonizar"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "TO-DO: Definición del problema de investigación, alinear con proyecto SAMSARA"
  },
  {
    "objectID": "intro.html#proyecto-samsara",
    "href": "intro.html#proyecto-samsara",
    "title": "1  Introducción",
    "section": "1.1 Proyecto SAMSARA",
    "text": "1.1 Proyecto SAMSARA\n\n1.1.1 Descripción\nSamsara es un Sistema de Alerta y Monitoreo Satelital de Áreas de Relevancia Ambiental, que identificará cambios en la estructura de la vegetación en humedales urbanos, turberas de Chiloé y el bosque y matorral esclerófilo de la región Metropolitana para ayudar a su preservación. La investigación es financiada por la ANID, enmarcada en el Proyecto Fondef IDEA I+D (ID21I10102) y ejecutada por Ingeniería UAI en conjunto con GobLab UAI —el laboratorio de innovación pública de la Escuela de Gobierno—, la Superintendencia de Medio Ambiente, el Servicio de Evaluación Ambiental y el Ministerio de Medio Ambiente.\nEl proyecto es liderado por investigadores de la Facultad de Ingeniería y Ciencias y la Escuela de Gobierno de la Universidad Adolfo Ibáñez (UAI) y es desarrollado en conjunto con la Superintendencia del Medio Ambiente, el Servicio de Evaluación Ambiental y el Ministerio del Medio Ambiente gracias a la adjudicación de un proyecto Fondef.\nEl proyecto Samsara, cuya adjudicación se realiza en el marco del Fondef IDEA I+D, buscará durante dos años, desarrollar un sistema automático de monitoreo de imágenes satelitales de humedales urbanos, turberas de Chiloé y el bosque y matorral esclerófilo de la Región Metropolitana, para mejorar la cobertura espacial y temporal de fiscalización de la Superintendencia del Medio Ambiente, detectando si existen irregularidades en la conservación de las zonas analizadas, ayudando así a su preservación.\n\n\n1.1.2 Etapas del proyecto\n\nDesarrollar un método automatizado de detección en tiempo real de cambios en la estructura de la vegetación de humedales urbanos, turberas y bosque esclerófilo basado en productos satelitales y algoritmos de inteligencia artificial.\nGenerar un sistema de alerta temprana utilizable por entes fiscalizadores acorde a los requerimientos de la SMA. Generar una estructura computacional que dé soporte a las necesidades de funcionamiento del Data Cube a nivel nacional, incluyendo la asimilación automática de nuevos productos satelitales.\nCrear una plataforma automatizada que permita el monitoreo de los cambios en la estructura de la vegetación de forma amigable y operativa por parte de la SMA.\nIdentificar necesidades de uso de imágenes satelitales para la protección del medio ambiente en organismos públicos.\n\nLinks de Interés:\n\nhttps://goblab.uai.cl/proyecto-samsara/\nhttps://noticias.uai.cl/ingenieria-uai-y-goblab-lanzan-encuesta-para-conocer-como-los-funcionarios-publicos-usan-imagenes-satelitales/"
  },
  {
    "objectID": "objetivos.html#objetivo-general",
    "href": "objetivos.html#objetivo-general",
    "title": "2  Objetivos",
    "section": "2.1 Objetivo General:",
    "text": "2.1 Objetivo General:\nCrear Sistema de Alerta y Monitoreo Satelital de Áreas de Relevancia Ambiental, identificará cambios en la estructura de la vegetación en humedales urbanos, turberas de Chiloé y el bosque y matorral esclerófilo de la región Metropolitana para ayudar a su preservación."
  },
  {
    "objectID": "objetivos.html#objetivos-específicos",
    "href": "objetivos.html#objetivos-específicos",
    "title": "2  Objetivos",
    "section": "2.2 Objetivos Específicos:",
    "text": "2.2 Objetivos Específicos:\nEn desarrollo…."
  },
  {
    "objectID": "marco.html",
    "href": "marco.html",
    "title": "3  Marco de Referencia",
    "section": "",
    "text": "Las turberas solo cubren el 3 % de la superficie terrestre del planeta pero almacenan más carbono que todos los bosques de la Tierra – si se mantienen humedas.\n\nDefinición:\n\nEl término turba debe ser entendido como un sedimento natural de tipo fitógeno, poroso, no consolidado, constituido por materia orgánica parcialmente descompuesta, acumulada en un ambiente saturado de agua. De esta forma, se puede entender al concepto de turbera como un depósito de turba con un espesor de, al menos, 30 cm (Hauser 1996)\n\nFormación:\n\nSegún (Hauser 1996), el origen de las turberas se encuentra en las eras glaciares del Pleistoceno, cuando grandes extensiones de casquetes glaciares cubrieron el valle central de la Región de Los Lagos, incluyendo a la Isla Grande de Chiloé. El posterior retiro de los glaciares dejó masas de agua tierra adentro, formando los grandes lagos y lagunas glaciares que en la actualidad componen el paisaje de la región.\nEn el caso de Chiloé, zona en la que se establecieron las condiciones climáticas ideales para el desarrollo del musgo del género Sphagnum, lo que permitió la acumulación de materia orgánica en depresiones del relieve de la isla con alto contenido de humedad Figure 3.1 (a). Este proceso de acumulación del musgo se consolidó en la formación de extensas turberas Figure 3.1 (b) y Figure 3.1 (c) . [Hauser (1996)]\n\n\n\n\n\nProceso de formación de turberas de origen glaciar (caso de Chiloé). Fuente: (Schofield W.B 1985)\n\n\n\nBotánicamente (Chiloé):\n\nBotánicamente, el pompón pertenece al Reino de las Plantas, a la División Bryophyta, a la Clase Musci y a la Familia de las Sphagnaceas. Esta familia comprende sólo un género, Sphagnum, compuesto por más de 300 especies descritas. En el archipiélago de Chiloé conviven varias especies de este género. La más abundante es S. magellanicum, que se caracteriza por su color rojo, talla relativamente robusta y hojas con ápice obtuso. Suele cubrir grandes superficies con mal drenaje en terrenos abiertos o cubriendo el suelo de los tepuales (bosques formados por la mirtácea Tepualia stipularis), donde se desarrolla con extraordinario vigor. Existen además, al menos 4 especies que se han identificado en la zona norte de la Isla: S. fimbriatum, S. falcatulum, S. recurvum y S. cuspidatum var. cuspidatum. Adicionalmente, la literatura cita otras 2 especies para la Isla: S. acutifolium y S. subnitens.Todas estas especies son de difícil identificación, siendo su morfología celular y la anatomía foliar la base de su clasificación (Zegers et al. 2006)\n\nCaracterización:\n\nUna de las características relevantes de las turberas de Sphagnum es que presenta una matriz continua superficial de musgos sobre una capa de turba que puede alcanzar varios metros de profundidad (Díaz et al. 2008). Según el mismo autor, entre otras características relevantes de este tipo de turberas se encuentran:\n\nLa turba que la compone es de origen vegetal y se encuentra en distintos estados de descomposición anaeróbica. Figure 3.1\nEl estrato superficial es biológicamente activo, conformado por asociaciones de especies, entre las que predominan plantas con gran capacidad para retener humedad Figure 3.1 (b).\nEl musgo Sphagnum forma un ambiente pobre en nutrientes (baja concentración de nitrógeno), ácido, anóxico y frío, lo que previene la presencia de hongos y bacterias que descomponen al material muerto Figure 3.1 (c).\nTiene una gran capacidad de absorción de agua (hasta 20 veces su peso seco en agua) Figure 3.1 (e)\nSu fuente de agua proviene de ríos y/o de la lluvia Figure 3.1 (d).\nEs un ecosistema de humedal con flora y fauna única y especializada.\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\nFigure 3.1: Fuente: (Treimun 2017)\n\n\n\n\nExtracción:\n\nComo Problema …\n\n\nTO-DO:\n\nDescripción de prdocesod e extracción y conexión con problema medio ambiental\nDescripción Completa Bosque Esclerofilo (? ROI)\nDescripción Completa Humedales Urbanos\n\nAgregar conceptos papers:\n\ncabezas_evaluation_2015 Evaluation of impacts of management in an anthropogenic peatland using field and remote sensing data (Cabezas et al. 2015)\nLopatin et al. - 2019 - Using aboveground vegetation attributes as proxies.pdf (Lopatin et al. 2019)\nLopatin et al. - 2022 - Disturbance alters relationships between soil carb.pdf (Lopatin et al. 2022)\n\n\n\n\n\nCabezas, Juli’an, Mauricio Galleguillos, Ariel Vald’es, Juan P. Fuentes, Cecilia P’erez, and Jorge F. Perez-Quezada. 2015. “Evaluation of Impacts of Management in an Anthropogenic Peatland Using Field and Remote Sensing Data.” Ecosphere 6 (12): 1–24. https://doi.org/10.1890/ES15-00232.1.\n\n\nDíaz, María F., Juan Larraín, Gabriela Zegers, and Carolina Tapia. 2008. “Floristic and Hydrological Characterization of Chiloé Island Peatlands, Chile.” Revista Chilena de Historia Natural 81 (4): 455–68. https://doi.org/10.4067/S0716-078X2008000400002.\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus perspectivas de utilización.” Revista Geológica de Chile 23 (2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208.\n\n\nLopatin, Javier, Roc’ıo ArayaNANAL’opez, Mauricio Galleguillos, and Jorge F. PereNAzNAQueza. 2022. “Disturbance Alters Relationships Between Soil Carbon Pools and Aboveground Vegetation Attributes in an Anthropogenic Peatland in Patagonia.” Ecology and Evolution 12 (3). https://doi.org/10.1002/ece3.8694.\n\n\nLopatin, Javier, Teja Kattenborn, Mauricio Galleguillos, Jorge F. Perez-Quezada, and Sebastian Schmidtlein. 2019. “Using Aboveground Vegetation Attributes as Proxies for Mapping Peatland Belowground Carbon Stocks.” Remote Sensing of Environment 231 (September): 111217. https://doi.org/10.1016/j.rse.2019.111217.\n\n\nSchofield W.B. 1985. “Introduction to Bryology.”\n\n\nTreimun, John. 2017. “Turberas de Chiloé, Ministerio del Medio Ambiente, Chile.”\n\n\nZegers, Gabriela, Juan Larraín, María Francisca Díaz, and Juan Armesto. 2006. “Impacto ecológico y social de la explotación de pomponales y turberas de Sphagnum en la Isla Grande de Chiloé.” http://biblioteca.cehum.org/handle/CEHUM2018/1389."
  },
  {
    "objectID": "rev_biblio.html",
    "href": "rev_biblio.html",
    "title": "4  Revisión Bibliográfica",
    "section": "",
    "text": "En está sección se registrará los aspectos generales de la revisión bibliográfica a modo de conocer el estado del arte en técnicas de identificación de cambios en imagenes satelitales utilizado redes neuronales convolucionales.\nPrincipales Desafíos:\nPosibles Soluciones"
  },
  {
    "objectID": "rev_biblio.html#detección-de-cambios-con-imágenes-radar",
    "href": "rev_biblio.html#detección-de-cambios-con-imágenes-radar",
    "title": "4  Revisión Bibliográfica",
    "section": "4.1 Detección de Cambios con Imágenes Radar",
    "text": "4.1 Detección de Cambios con Imágenes Radar\n\n4.1.1 Ms-CapsNet\nSAR Image Change Detection Based on Multiscale Capsule Network (Gao et al. 2021)\n\nResumen:\n\nLos métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los retos del ruido de moteado y la sensibilidad a la deformación. Para mitigar estos problemas, propone una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se ha diseñado un módulo de convolución de fusión adaptativa (AFC) para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.\n\nIndex Terms:\n\nChange detection, multiscale capsule network, synthetic aperture radar, deep learning.\n\n\n\n\n\nMs-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala\n\n\nResultados y Análisis de Experimentos.\n\n\n\nVisualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet."
  },
  {
    "objectID": "rev_biblio.html#graph-based-knowledge-supplement",
    "href": "rev_biblio.html#graph-based-knowledge-supplement",
    "title": "4  Revisión Bibliográfica",
    "section": "4.2 Graph-Based Knowledge Supplement",
    "text": "4.2 Graph-Based Knowledge Supplement\nChange Detection From Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network (J. Wang et al. 2022)\n\nResumen:\n\nLa detección de cambios en las imágenes del radar de apertura sintética (SAR) es una tarea vital pero difícil en el campo del análisis de imágenes de teledetección. La mayoría de los trabajos anteriores adoptan un método auto-supervisado que utiliza muestras pseudo-etiquetadas para guiar el entrenamiento y las pruebas subsecuentes. Sin embargo, las redes profundas suelen requerir muchas muestras de alta calidad para la optimización de los parámetros. El ruido en las pseudo-etiquetas afecta inevitablemente al rendimiento final de la detección de cambios. Para resolver el problema, proponemos una red de complemento de conocimiento basada en grafos (GKSNet). Para ser más específicos, extraemos información discriminatoria del conjunto de datos etiquetados existente como conocimiento adicional, para suprimir hasta cierto punto los efectos adversos de las muestras ruidosas. A continuación, diseñamos un módulo de transferencia de grafos para destilar información contextual de forma atenta desde el conjunto de datos etiquetados al conjunto de datos de destino, lo que permite salvar la correlación de características entre los conjuntos de datos. Para validar el método propuesto, realizamos amplios experimentos con cuatro conjuntos de datos de SAR, que demostraron la superioridad de la GKSNet propuesta en comparación con varias líneas de base del estado de la técnica.\n\nIndex Terms:\n\nChange detection, graph dependency fusion, knowledge supplement network, synthetic aperture radar (SAR).\n\n\n\n\n\nSchematic illustration of the proposed GKSNet. The image features extracted by deep CNNs are projected into a graph representation. Then, the graph representations are transferred and fused via graph transfer module across different datasets. Finally, features from different graphs are fused via intergraph fusion module. Through feature fusion, the model exploits the common knowledge and bridge the feature correlation from different datasets. The obtained evolved features are capable of improving the change detection performance."
  },
  {
    "objectID": "rev_biblio.html#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022",
    "href": "rev_biblio.html#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022",
    "title": "4  Revisión Bibliográfica",
    "section": "4.3 Fully Transformer Network for Change Detection of Remote Sensing Images (Yan, Wan, and Zhang 2022)",
    "text": "4.3 Fully Transformer Network for Change Detection of Remote Sensing Images (Yan, Wan, and Zhang 2022)\n\nResumen:\n\nRecientemente, la detección de cambios (CD) de las imágenes de teledetección ha logrado un gran progreso con los avances del aprendizaje profundo. Sin embargo, los métodos actuales generalmente ofrecen regiones de CD incompletas y límites de CD irregulares debido a la limitada capacidad de representación de las características visuales extraídas. Para aliviar estos problemas, en este trabajo proponemos un novedoso marco de aprendizaje llamado Fully Transformer Network (FTN) para la CD de imágenes de teledetección, que mejora la extracción de características desde una vista global y combina características visuales de varios niveles de forma piramidal. Más concretamente, el marco propuesto utiliza en primer lugar las ventajas de los transformadores en el modelado de dependencias de largo alcance. Puede ayudar a aprender más características discriminativas de nivel global y obtener regiones completas de CD. A continuación, introducimos una estructura piramidal para agregar características visuales multinivel a partir de Transformers para mejorar las características. La estructura piramidal injertada con un Módulo de Atención Progresiva (PAM) puede mejorar la capacidad de representación de características con interdependencias adicionales a través de atenciones de canal. Por último, para entrenar mejor el marco, utilizamos el aprendizaje supervisado en profundidad con múltiples funciones de pérdida de límites. Amplios experimentos demuestran que nuestro método propuesto alcanza un nuevo rendimiento de vanguardia en cuatro puntos de referencia públicos de CD.\n\n\n\n\n\n\n\nLa estructura general del marco propuesto. Fully Transformer Network for Change Detection of Remote Sensing Images\n\n\n\n\nIndex Terms: Fully Transformer Network, Change Detection, Remote Sensing Image."
  },
  {
    "objectID": "rev_biblio.html#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022",
    "href": "rev_biblio.html#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022",
    "title": "4  Revisión Bibliográfica",
    "section": "4.4 Self-supervised Vision Transformers for Joint SAR-optical Representation Learning (Y. Wang, Albrecht, and Zhu 2022)",
    "text": "4.4 Self-supervised Vision Transformers for Joint SAR-optical Representation Learning (Y. Wang, Albrecht, and Zhu 2022)\n\nResumen:\n\nEl aprendizaje autosupervisado (SSL) ha suscitado un gran interés en la teledetección y la observación de la Tierra debido a su capacidad para aprender representaciones independientes de la tarea sin necesidad de anotaciones humanas. Mientras que la mayoría de los trabajos existentes sobre SSL en teledetección utilizan columnas vertebrales ConvNet y se centran en una sola modalidad, nosotros exploramos el potencial de los transformadores de visión (ViTs) para el aprendizaje conjunto de representaciones SAR-ópticas. Basándonos en DINO, un algoritmo SSL de última generación que destila conocimiento de dos vistas aumentadas de una imagen de entrada, combinamos imágenes SAR y ópticas concatenando todos los canales en una entrada unificada. Posteriormente, enmascaramos aleatoriamente los canales de una modalidad como estrategia de aumento de datos. Durante el entrenamiento, el modelo se alimenta de pares de imágenes ópticas, SAR y SAR-ópticas para aprender representaciones internas e intra-modales. Los resultados experimentales que emplean el conjunto de datos BigEarthNet-MM demuestran las ventajas tanto de los ejes ViT como del algoritmo SSL multimodal propuesto DINO-MM.\n\n\n\n\n\n\n\nDINO-MM: el algoritmo SSL conjunto SAR-óptico propuesto. La imagen SAR-óptica concatenada se toma como entrada bruta. Se transforma aleatoriamente en dos vistas aumentadas y se introduce en una red maestro-estudiante basada en DINO.\n\n\n\n\nIndex Terms: Self-supervised learning, vision trans- former, multimodal representation learning"
  },
  {
    "objectID": "rev_biblio.html#detección-de-cambios-con-imágenes-de-alta-resolución",
    "href": "rev_biblio.html#detección-de-cambios-con-imágenes-de-alta-resolución",
    "title": "4  Revisión Bibliográfica",
    "section": "4.5 Detección de Cambios con Imágenes de Alta Resolución",
    "text": "4.5 Detección de Cambios con Imágenes de Alta Resolución\n\n4.5.1 DASNet\nDASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images(Chen et al. 2021)\n\nResumen:\n\nLa detección de cambios es una tarea básica del procesamiento de imágenes por teledetección. El objetivo de la investigación es identificar la información de cambio de interés y filtrar la información de cambio irrelevante como factores de interferencia. Recientemente, el aumento del aprendizaje profundo ha proporcionado nuevas herramientas para la detección de cambios, que han dado resultados impresionantes. Sin embargo, los métodos disponibles se centran principalmente en la información de diferencia entre las imágenes de teledetección multitemporal y carecen de robustez ante la información de pseudocambio. Para superar la falta de resistencia de los métodos actuales a los pseudocambios, en este trabajo proponemos un nuevo método, a saber, las redes siamesas totalmente convolucionales de atención dual (DASNet), para la detección de cambios en imágenes de alta resolución. A través del mecanismo de atención dual, se capturan las dependencias de largo alcance para obtener representaciones de características más discriminantes para mejorar el rendimiento de reconocimiento del modelo. Además, la muestra desequilibrada es un problema grave en la detección de cambios, es decir, las muestras sin cambios son mucho más abundantes que las muestras con cambios, lo que constituye una de las principales razones de los pseudocambios. Proponemos la pérdida contrastiva ponderada de doble margen para abordar este problema, castigando la atención a los pares de características sin cambios y aumentando la atención a los pares de características con cambios. Los resultados experimentales de nuestro método en el conjunto de datos de detección de cambios (CDD) y en el conjunto de datos de detección de cambios en edificios (BCDD) demuestran que, en comparación con otros métodos de referencia, el método propuesto consigue mejoras máximas del 2,9% y el 4,2%, respectivamente, en la puntuación F1. Nuestra implementación de PyTorch está disponible en https://github.com/lehaifeng/DASNet.\n\nIndex Terms:\n\nChange detection, high-resolution images, dual attention, Siamese network, weighted double-margin contrastive loss.\n\n\n\n\n\n\n\nDASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite\n\n\n\n\n\n\n\n\nChen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, and Haifeng Li. 2021. “DASNet: Dual Attentive Fully Convolutional Siamese Networks for Change Detection of High Resolution Satellite Images.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14: 1194–1206. https://doi.org/10.1109/JSTARS.2020.3037893.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022. “Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167.\n\n\nWang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022. “Self-Supervised Vision Transformers for Joint SAR-optical Representation Learning.” arXiv. http://arxiv.org/abs/2204.05381.\n\n\nYan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. “Fully Transformer Network for Change Detection of Remote Sensing Images.” arXiv. http://arxiv.org/abs/2210.00757."
  },
  {
    "objectID": "Ms-CapsNet.html",
    "href": "Ms-CapsNet.html",
    "title": "5  Ms-CapsNet",
    "section": "",
    "text": "Los métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los retos del ruido de moteado y la sensibilidad a la deformación. Para mitigar estos problemas, propusimos una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se diseña un módulo de convolución de fusión adaptativa (AFC) para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.\nRespositorio: https://github.com/summitgao/SAR_CD_MS_CapsNet.\n\n\n\nAunque se han propuesto muchas técnicas, la detección de cambios en las imágenes SAR sigue siendo una tarea difícil. La calidad de la imagen se ve deteriorada por el ruido de moteado que dificulta la interpretación meticulosa de los datos SAR. Se han implementado muchos métodos para abordar el problema del ruido de moteado. Suelen constar de tres pasos:\n\nCoregistro de imágenes: El corregistro de imágenes es una tarea fundamental para establecer las correspondencias espaciales entre las imágenes SAR multitemporales.\nGeneración de imágenes de diferencia (DI) : La DI se genera habitualmente mediante los operadores log- ratio, Gauss-ratio [5] y neighborhood-ratio [6].\nClasificación de DI: la mayoría de las investigaciones se dedican a construir un clasificador robusto. Se trata de una tarea no trivial, ya que un clasificador potente determina directamente la precisión de la detección de cambios.\n\nMuchos investigadores se dedican a desarrollar clasificadores potentes para la detección de cambios. Li et al. [7] diseñaron un algoritmo de clustering de dos niveles para la detección de cambios sin supervisión. En [8], la información de vecindad local se incorpora a la función objetivo de clustering para mejorar el rendimiento de la detección de cambios. Gong et al. [9] desarrollaron un campo aleatorio de Markov (MRF) mejorado basado en el clustering de c-medias difusas (FCM) para suprimir el ruido de moteado. En [4], se emplearon las máquinas de Boltzmann restringidas (RBM) apiladas para la detección de cambios en las imágenes SAR. Aunque los métodos anteriores lograron un rendimiento prometedor, las capacidades de representación de características siguen siendo limitadas.\nEn los últimos años, las redes neuronales convolucionales (CNN) han mejorado mucho el rendimiento de muchas tareas visuales. Se ha demostrado que es bastante eficaz para el aprendizaje robusto de características. Los modelos basados en CNN se han aplicado con éxito en la detección de cambios en imágenes de teledetección [10]. Wang et al. [11] propusieron un marco de CNN de extremo a extremo para aprender características discriminativas de la matriz de afinidad mixta para la detección de cambios.\nMás tarde, se desarrolló el modelado de ruido profundo no supervisado para la detección de cambios en imágenes hiperespectrales [12]. Liu et al. [13] propusieron una elegante CNN local restringida (LR-CNN) para la detección de cambios polarimétricos en SAR. En [14], se aplicó el aprendizaje profundo transferido a la detección de cambios en imágenes SAR de hielo marino basado en CNN. Aunque los métodos basados en CNN han logrado un excelente rendimiento en la detección de cambios, la precisión a veces se deteriora en el caso de la transformación, como las inclinaciones y rotaciones. En concreto, la CNN es incapaz de modelar la relación posicional entre los objetos del suelo.\nMás recientemente, Sabour y Hinton propusieron la red Capsule (CapsNet) para dar solución a los problemas en los que los modelos CNN son inadecuados [15]. En CapsNet, un vector de actividad de cápsulas representa los parámetros de instanciación de la entidad, como la pose, la textura y la deformación. La existencia de entidades se expresa mediante la longitud de los parámetros de instanciación. El mecanismo de enrutamiento dinámico se utiliza para la propagación de la información. Se ha comprobado empíricamente que CapsNet es eficaz para el análisis de imágenes de teledetección [16] [17]. Hasta donde sabemos, la literatura sobre la detección de cambios en SAR basada en CapsNet es muy escasa.\n\n\n\nMs-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala\n\n\nSostenemos que la debilidad de los enfoques existentes de detección de cambios en imágenes SAR proviene principalmente de dos aspectos: Uno es que la correlación de las características de diferentes posiciones no se puede modelar de forma efectiva. El otro es el ruido intrínseco del moteado en las imágenes SAR. Para hacer frente a los problemas mencionados, se propone una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre las imágenes SAR multitemporales. La Ms-CapsNet propuesta tiene una estructura similar a la Red de Cápsulas [15] sin el operador multiescala y el módulo de Convolución de Fusión Adaptativa (AFC). La Ms-CapsNet proporciona un grupo de parámetros de instanciación para capturar características de diferentes posiciones. Para hacer frente al problema del ruido de moteado, el módulo AFC está diseñado para convertir las intensidades de los píxeles en actividades de las características locales. De este modo, las características locales se vuelven robustas al ruido. Se realizan amplios experimentos con tres conjuntos de datos reales para demostrar la superioridad de nuestro método propuesto sobre cuatro trabajos del estado del arte.\nPara mayor claridad, las principales contribuciones se resumen como sigue:\n\nLa Ms-CapsNet propuesta tiene la capacidad de extraer características robustas de diferentes posiciones. Las propiedades equivariantes se pueden conseguir mediante el módulo de cápsulas. Por lo tanto, la demanda de una gran cantidad de muestras de entrenamiento se reduce por la información correlativa y completa.\nSe diseña un módulo AFC sencillo pero eficaz, que puede convertir eficazmente las intensidades de los píxeles en actividades de características locales. El módulo AFC extrae las características semánticas superiores y enfatiza las significativas mediante una estrategia basada en la atención. Por lo tanto, las características locales de actividad se vuelven más resistentes al ruido y se aceptan inmediatamente como entrada de la cápsula primaria.\nSe han realizado amplios experimentos con tres conjuntos de datos de SAR para validar la eficacia del método propuesto. Además, hemos publicado los códigos y la configuración para facilitar futuras investigaciones en el análisis de imágenes de teledetección multitemporal."
  },
  {
    "objectID": "Ms-CapsNet.html#metodología",
    "href": "Ms-CapsNet.html#metodología",
    "title": "5  Ms-CapsNet",
    "section": "5.2 Metodología",
    "text": "5.2 Metodología\n\n5.2.1 A. Adaptive Fusion Convolution Module (AFC)\n\n\n\nIllustration of the Adaptive Fusion Convolution (AFC) module.\n\n\n\n\n5.2.2 B. Capsule Module"
  },
  {
    "objectID": "Ms-CapsNet.html#resultados-y-análisis-de-experimentos.",
    "href": "Ms-CapsNet.html#resultados-y-análisis-de-experimentos.",
    "title": "5  Ms-CapsNet",
    "section": "5.3 Resultados y Análisis de Experimentos.",
    "text": "5.3 Resultados y Análisis de Experimentos.\n\n\n\nVisualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet.\n\n\n\n5.3.1 A. Dataset and Evaluation Criteria\n\n\n5.3.2 B. Parameters Analysis of the Proposed Ms-CapsNet\n\n\n5.3.3 C. Change Detection Results on Three Datasets"
  },
  {
    "objectID": "Ms-CapsNet.html#conclusion",
    "href": "Ms-CapsNet.html#conclusion",
    "title": "5  Ms-CapsNet",
    "section": "5.4 Conclusion",
    "text": "5.4 Conclusion\n\n5.4.1 Referencias\n[1] D. Burnner, G. Lemonie, and L. Bruzzone, “Earthquake damage assess- ment of buildings using VHR optical and SAR imagery,” IEEE Trans. Geosci. Remote Sens., vol. 48, no. 5, pp. 2403–2420, May 2010.\n[2] S. Quan, B. Xiong, D. Xiang, L. Zhao, S. Zhang, and G. Kuang, “Eigenvalue-based urban area extraction using polarimetric SAR data,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 11, no. 2, pp. 458–471, Feb. 2018.\n[3] R. J. Radke, S. Andra, O. Al-Kofahi, and B. Roysam, “Image change detection algorithms: A systematic survey,” IEEE Trans. Image Process., vol. 14, no. 3, pp. 294–307, Mar. 2005. [4] M. Gong, J. Zhao, J. Liu, Q. Miao, and L. Jiao, “Change detection in synthetic aperture radar images based on deep neural networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 1, pp. 125–138, Jan. 2016.\n[5] B. Hou et al., “Unsupervised change detection in SAR image based on gauss-log ratio image fusion and compressed projection,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 7, no. 8, pp. 3297–3317, 2014.\n[6] M. Gong, Y. Cao, and Q. Wu, “A neighborhood-based ratio approach for change detection in SAR images,” IEEE Geosci. Remote Sens. Lett., vol. 9, no. 2, pp. 307–311, 2012.\n[7] H. Li, T. Celik, N. Longbotham, and W. J. Emery, “Gabor feature based unsupervised change detection of multitemporal SAR images based on two-level clustering,” IEEE Geosci. Remote Sens. Lett., vol. 12, no. 12, pp. 2458–2462, Dec. 2015.\n[8] L. Jia, M. Li, P. Zhang, Y. Wu, and H. Zhu, “SAR image change detection based on multiple kernel k-means clustering with local- neighborhood information,” IEEE Geosci Remote Sens. Lett., vol. 13, no. 6, pp. 856–860, Jun. 2016.\n[9] M. Gong, Z. Zhou, and J. Ma. “Change detection in synthetic aperture radar images based on image fusion and fuzzy clustering,” IEEE Trans. Image Process., vol. 21, no. 4, pp. 2141–2151, Apr. 2012.\n[10] Q.Liu,R.Hang,H.Song,andZ.Li,“Learningmultiscaledeepfeatures for high-resolution satellite image scene classification,” IEEE Tran. Geosci. Remote Sens., vol. 56, no. 1, pp. 117–126, Jan. 2018.\n[11] Q. Wang, Z. Yuan, Q. Du, and X. Li, “GETNET: a general end-to-end 2-D CNN framework for hyperspectral image change detection,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 3–13, Jan. 2019.\n[12] X. Li, Z. Yuan, and Q. Wang, “Unsupervised deep noise modeling for hyperspectral image change detection,” Remote Sens., vol. 11, no. 3, 258, Jan. 2019.\n[13] F. Liu, L. Jiao, X. Tang, S. Yang, W. Ma, and B. Hou, “Local restricted convolutional neural network for change detection in polarimetric SAR images,” IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 3, pp. 1–16, Mar. 2019.\n[14] Y. Gao, F. Gao, J. Dong, and S. Wang. “Transferred deep learning for sea ice change detection from synthetic aperture radar images,” IEEE Geosci. Remote Sens. Lett., vol. 16, no. 10, pp. 1655–1659, Oct. 2019.\n[15] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3859— 3869.\n[16] M. E. Paoletti et al., “Capsule networks for hyperspectral image classi- fication,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 4, pp. 2145– 2160, Apr. 2019.\n[17] K. Zhu et al., “Deep convolutional capsule network for hyperspectral image spectral and spectral-spatial classification,” Remote Sens., vol. 11, no. 3, pp. 1–28, Mar. 2019, Art. no. 223.\n[18] J.Hu,L.Shen,andG.Sun,“Squeeze-and-excitationnetworks,”inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018, pp. 7132–7141.\n[19] F. Gao, J. Dong, B. Li, and Q. Xu, “Automatic change detection in synthetic aperture radar images based on PCANet,” IEEE Geosci. Remote Sens. Lett., vol. 13, no. 12, pp. 1792–1796, Dec. 2016.\n[20] W. Song, S. Li, L. Fang, and T. Lu, “Hyperspectral image classification with deep feature fusion network,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 6, pp. 3173–3184, Jun. 2018.\n\n\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838."
  },
  {
    "objectID": "RUSACD.html",
    "href": "RUSACD.html",
    "title": "6  RUSACD",
    "section": "",
    "text": "RUSACD: Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning (Zhang et al. 2020)"
  },
  {
    "objectID": "RUSACD.html#abstract",
    "href": "RUSACD.html#abstract",
    "title": "6  RUSACD",
    "section": "6.1 Abstract",
    "text": "6.1 Abstract\nLa detección de cambios en áreas pequeñas a partir de un radar de apertura sintética (SAR) es una tarea muy difícil. En este trabajo, se propone un enfoque robusto no supervisado para la detección de cambios en áreas pequeñas a partir de imágenes SAR multitemporales utilizando el aprendizaje profundo. En primer lugar, se desarrolla un método de reconstrucción de superpíxeles multiescala para generar una imagen de diferencia (DI), que puede suprimir el ruido de moteado de forma efectiva y mejorar los bordes mediante la explotación de información local y espacialmente homogénea. En segundo lugar, se propone un algoritmo de clustering de c-means difuso de dos etapas para dividir los píxeles de la DI en clases cambiadas, no cambiadas e intermedias con una estrategia de clustering paralela. A continuación, se construyen parches de imagen pertenecientes a las dos primeras clases como muestras de entrenamiento de pseudoetiquetas, y los parches de imagen de la clase intermedia se tratan como muestras de prueba. Por último, se diseña y entrena una red neuronal convolucional wavelet (CWNN) para clasificar las muestras de prueba en clases modificadas o no modificadas, Se combina con una red convolucional generativa adversarial (DCGAN) para aumentar el número de clases cambiadas dentro de las muestras de entrenamiento de las pseudo-etiquetas. Los experimentos numéricos en cuatro conjuntos de datos reales de SAR demuestran la validez y la solidez del enfoque propuesto, logrando una precisión de hasta el 99,61% para la detección de cambios en áreas pequeñas.\n\nKeywords:\n\nChange detection; Synthetic aperture radar; Difference image; Fuzzy c-means algorithm; Deep learning.\n\n\nRepostorio: https://github.com/River-sh/A-robust-unsupervised-small-area-change-detection"
  },
  {
    "objectID": "RUSACD.html#diagrama-general",
    "href": "RUSACD.html#diagrama-general",
    "title": "6  RUSACD",
    "section": "6.2 Diagrama General",
    "text": "6.2 Diagrama General\n\n\n\nFlowchart illustrating the proposed RUSACD methodology."
  },
  {
    "objectID": "RUSACD.html#pros-y-contras",
    "href": "RUSACD.html#pros-y-contras",
    "title": "6  RUSACD",
    "section": "6.3 Pros y Contras",
    "text": "6.3 Pros y Contras\nPros:\n\nNo supervisado\nÁreas pequeñas\nMejor performance casos con bajo ruido (moteado)\nEvaluación de resultados con otros métodos\n\nContras:\n\nNo testeado con Sentine-1 (SAR COSMO-SkyMed entre 2016 y 2017 y SAR ERS-2 entre 2003 y 2004)\nRepositorio en matlab"
  },
  {
    "objectID": "RUSACD.html#datasets",
    "href": "RUSACD.html#datasets",
    "title": "6  RUSACD",
    "section": "6.4 Datasets",
    "text": "6.4 Datasets\nSe utilizaron cuatro conjuntos de datos SAR multitemporales reales (Tabla 1) para evaluar el rendimiento del enfoque propuesto. Tres de estos cuatro conjuntos de datos fueron adquiridos sobre la provincia de Guizhou en China por el sensor SAR COSMO-SkyMed en junio de 2016 y abril de 2017. Como se muestra en la Fig. 6, el primer par de imágenes (conjunto de datos A) con el mapa de referencia del suelo consiste en montañas y un río, el segundo par de imágenes (conjunto de datos B) con el mapa de referencia del suelo incluye colinas, llanuras y edificios, y el tercer par de imágenes (conjunto de datos C) con el mapa de referencia del suelo es principalmente llanuras. El último par de imágenes (conjunto de datos D) fue adquirido sobre la ciudad de San Francisco, Estados Unidos, por el sensor SAR ERS-2 en agosto de 2003 y mayo de 2004 (Fig. 7). En los conjuntos de datos A, B y C se aprecia el ruido de moteado, que supone un gran reto para la detección de cambios. A partir de los mapas de referencia redondos de la Fig. 6-7, está claro que la proporción de píxeles cambiados es extremadamente pequeña en comparación con los píxeles no cambiados. El conjunto de datos D es un punto de referencia en el que hay menos ruido y los cambios no pueden considerarse como cambios de área pequeños. Este conjunto de datos se utiliza para demostrar la solidez de nuestro enfoque propuesto.\n\n\n\nDataset A, B and C. (a) Image acquired in June 2016, (b) Image acquired in April 2017. (c) Ground\n\n\n\n\n\nDataset D. (a) Image acquired in August 2003. (b) Image acquired in May 2004. (c) Ground reference map."
  },
  {
    "objectID": "RUSACD.html#resultados",
    "href": "RUSACD.html#resultados",
    "title": "6  RUSACD",
    "section": "6.5 Resultados",
    "text": "6.5 Resultados\nPara demostrar la eficacia del RUSACD propuesto, se compararon cuatro métodos de referencia, entre ellos: PCA k-means (PCAK) (Celik, 2009), relación basada en la vecindad y máquina de aprendizaje extremo (NRELM) (Gao et al., 2016), extracción de características de Gabor y FCM con PCANet (GFPCANet) (Gao et al., 2016), y FCM con CWNN (FCWNN) (Gao et al., 2019). También aplicamos TCCFCM para agrupar el MSRDI en dos categorías (cambiado y sin cambios) como método de detección de cambios MTCCFCM de referencia. Los resultados experimentales se muestran en la Fig. 8, mientras que las métricas de precisión cuantitativas se indican en la Tabla 2. Además, otros métodos utilizados para la comparación en el conjunto de datos D (Tabla 3) incluyen la detección guiada por saliencia con agrupación de k-means (SGK) (Zheng et al., 2017), el autoencoder apilado y FCM con CNN (SAEFCNN) (Gong et al., 2017), la red neuronal profunda guiada por saliencia (SGDNN) (Geng et al., 2019) y la prueba de relación de verosimilitud generalizada adaptativa (AGLRT) (Zhuang et al., 2020).\nTODO: Revisar y agregar bibliografia\n\n\n\nTable 2. Comparison of the final change maps on dataset A-D. (a) PCAK. (b) NRELM. (c) GFPCANet. (d) FCWNN. (e) MTCCFCM. (f) RUSACD. (g) Ground reference map.\n\n\n\n\n\nAccuracy metrics for the different change detection methods. Best results are shown in bold.\n\n\n\n\n\nTable 3. Accuracy metrics for the different change detection methods on dataset D. Best results are shown in bold.\n\n\n\n\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter M. Atkinson. 2020. “Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning.” arXiv. http://arxiv.org/abs/2011.11005."
  },
  {
    "objectID": "insumos.html",
    "href": "insumos.html",
    "title": "7  Insumos",
    "section": "",
    "text": "A continuación se explica los procesos realizados a la base recibida llamada Turberas_COT_MMA_CHILOE.kml correspondiente a un archivo espacial de polígonos que representan tu turberas (?sec-turba) en el Archipiélago de Chiloé, en la Región de los Lagos Chile.\n\n\n\nLectura inicial\n\nlibrary(sf)\nlibrary(dplyr)\n\nturberas <-  st_read(\"data/turberas/Turberas_COT_MMA_CHILOE.kml\", \n                     quiet =T) %>% \n  st_zm()\n\nExtración de la Data anexa de kml file\n\n# pendiente\n\n\n\nCorresponde a la revisión de todos los poligonos y registrar y poligonizar los cambios observados que correspnde a actividades antrópicas, para lo cual se se utilizó el software de Google Earth Pro valiendose de la funcionalidad de serie de tiempo.\n\n\n\n\nchanges_sf <- readRDS(\"data/turberas/rds/changes_sf.rds\") \n\nTablas de Cambios\n\n\n\n\nTabla de Cambios por actividades antrópicas\n \n  \n    Name \n    Description \n    ID \n    Origen_ID \n    Mes \n    Year \n    Observacion \n  \n \n\n  \n    change_1 \n     \n    1 \n    149 \n    NA \n    2020 \n    Construcción \n  \n  \n    change_2 \n     \n    2 \n    197 \n    10 \n    2020 \n    Desforestación \n  \n  \n    change_3 \n     \n    3 \n    234 \n    12 \n    2021 \n    Desforestación \n  \n  \n    change_4 \n     \n    4 \n    232 \n    10 \n    2019 \n    Camino \n  \n  \n    change_5 \n     \n    5 \n    231 \n    10 \n    2019 \n    Camino \n  \n  \n    change_6 \n     \n    6 \n    249 \n    01 \n    2022 \n    Parcelación \n  \n  \n    change_7 \n     \n    7 \n    363 \n    03 \n    2021 \n    Agua \n  \n  \n    change_8 \n     \n    8 \n    372 \n    10 \n    2013 \n    Extracción \n  \n  \n    change_9 \n     \n    9 \n    372 \n    09 \n    2013 \n    Por Definir \n  \n  \n    change_10 \n     \n    10 \n    479 \n    03 \n    2021 \n    Deforestación \n  \n  \n    change_11 \n     \n    11 \n    423 \n    NA \n    NA \n    Por Definir \n  \n  \n    change_13 \n     \n    13 \n    439 \n    NA \n    NA \n    Empresa \n  \n  \n    change_14 \n     \n    14 \n    612 \n    01 \n    2017 \n    Parcelación \n  \n  \n    change_15 \n     \n    15 \n    554 \n    11 \n    2019 \n    Extracción \n  \n  \n    change_16 \n     \n    16 \n    551 \n    11 \n    2019 \n    Extracción \n  \n  \n    change_17 \n     \n    17 \n    521 \n    01 \n    2017 \n    Extracción \n  \n  \n    change_18 \n     \n    18 \n    582 \n    04 \n    2018 \n    Extracción \n  \n  \n    change_19 \n     \n    19 \n    565 \n    03 \n    2021 \n    Cambio Color \n  \n  \n    change_20 \n     \n    20 \n    595 \n    03 \n    2021 \n    Extracción \n  \n  \n    change_21 \n     \n    21 \n    680 \n    03 \n    2021 \n    Cambio Color \n  \n  \n    change_22 \n     \n    22 \n    685 \n    03 \n    2021 \n    Alta Tensión \n  \n  \n    change_23 \n     \n    23 \n    720 \n    03 \n    2021 \n    Extracción \n  \n  \n    change_24 \n     \n    24 \n    658 \n    10 \n    2011 \n    Cultivo \n  \n  \n    change_25 \n     \n    25 \n    689 \n    03 \n    2021 \n    Por Definir \n  \n  \n    change_26 \n     \n    26 \n    NA \n    03 \n    2021 \n    Extracción \n  \n  \n    change_27 \n     \n    27 \n    829 \n    08 \n    2021 \n    Construcción \n  \n  \n    change_28 \n     \n    28 \n    844 \n    01 \n    2018 \n    Extracción \n  \n\n\n\n\n\n\n\n\n\n\n\n\nextraccion <-  changes_sf %>% filter(Observacion == \"Extracción\")\n\n\nmapview(extraccion, zcol= \"ID\")+\n  mapview(turberas, hide=T)"
  },
  {
    "objectID": "data-cube.html",
    "href": "data-cube.html",
    "title": "Data Cube",
    "section": "",
    "text": "Repositorio contiene una serie de notebooks para funcionar con el DataCube Chile. Está dividido en niveles de dificultad y por ahora, sólo está disponible el entrenamiento básico, que sienta los primeros lineamientos para comenzar a trabajar en el cubo de datos.\nhttps://github.com/Data-Observatory/DataCubeTrainingBasic"
  },
  {
    "objectID": "recursos.html",
    "href": "recursos.html",
    "title": "Recursos",
    "section": "",
    "text": "Geospatial deep learning with TorchGeo\n\nTorchGeo es una biblioteca de dominio de PyTorch que proporciona conjuntos de datos, muestreadores, transformaciones y modelos pre-entrenados específicos para datos geoespaciales.\nLink:\n\nPágina Oficial\nGithub Proyecto"
  },
  {
    "objectID": "recursos.html#raster-vision",
    "href": "recursos.html#raster-vision",
    "title": "Recursos",
    "section": "Raster Vision",
    "text": "Raster Vision\nGithub"
  },
  {
    "objectID": "recursos.html#links-por-explorar",
    "href": "recursos.html#links-por-explorar",
    "title": "Recursos",
    "section": "Links Por explorar",
    "text": "Links Por explorar\nhttps://dymaxionlabs-eng.medium.com/how-to-create-a-land-cover-model-for-south-america-in-4-steps-67e57d3dae64\n\nhttps://courses.spatialthoughts.com/end-to-end-gee.html#module-4-change-detection\nhttps://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1\nhttps://www.youtube.com/results?search_query=change+detection+gee\narquiologíahttps://code.earthengine.google.com/?scriptPath=users%2Fdenisberroeta%2FGEE_CIT_dbg%3Acc\nhttps://www.youtube.com/watch?v=wDBcTOTAwOc\nhttps://www.youtube.com/watch?v=5oONMB0UPWc\nhttps://developers.google.cn/earth-engine/tutorials/tutorial_forest_01\nhttps://appliedsciences.nasa.gov/join-mission/training/english/arset-using-google-earth-engine-land-monitoring-applications\nhttps://appliedsciences.nasa.gov/sites/default/files/2021-06/GEE_Land_Part3.pdf\nhttps://www.youtube.com/watch?v=KyjNhAvQS2s\nhttps://paperswithcode.com/task/change-detection-for-remote-sensing-images\n\nhttps://github.com/likyoo/Siam-NestedUNet\ncontaminación\n\nhttps://appliedsciences.nasa.gov/join-mission/training/english/arset-high-resolution-no2-monitoring-space-tropomi\n\nPrograma de la NASA https://appliedsciences.nasa.gov/join-mission/training/english/arset-using-google-earth-engine-land-monitoring-applications\n\nbigearth net\nsentinel 1 Change detection no supervisado S1 si existe cambio algorimo global, unet - rmask\ntorch geo"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cabezas, Juli’an, Mauricio Galleguillos, Ariel Vald’es, Juan P. Fuentes,\nCecilia P’erez, and Jorge F. Perez-Quezada. 2015. “Evaluation of\nImpacts of Management in an Anthropogenic Peatland Using Field and\nRemote Sensing Data.” Ecosphere 6 (12): 1–24. https://doi.org/10.1890/ES15-00232.1.\n\n\nChen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu\nLiu, and Haifeng Li. 2021. “DASNet: Dual\nAttentive Fully Convolutional Siamese Networks for Change Detection of\nHigh Resolution Satellite Images.” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing 14:\n1194–1206. https://doi.org/10.1109/JSTARS.2020.3037893.\n\n\nDíaz, María F., Juan Larraín, Gabriela Zegers, and Carolina Tapia. 2008.\n“Floristic and Hydrological Characterization of Chiloé Island\nPeatlands, Chile.” Revista Chilena de Historia Natural\n81 (4): 455–68. https://doi.org/10.4067/S0716-078X2008000400002.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR\nImage Change Detection Based on Multiscale Capsule Network.”\nIEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus\nperspectivas de utilización.” Revista Geológica de Chile 23\n(2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208.\n\n\nLopatin, Javier, Roc’ıo ArayaNANAL’opez, Mauricio Galleguillos, and\nJorge F. PereNAzNAQueza. 2022. “Disturbance Alters Relationships\nBetween Soil Carbon Pools and Aboveground Vegetation Attributes in an\nAnthropogenic Peatland in Patagonia.” Ecology\nand Evolution 12 (3). https://doi.org/10.1002/ece3.8694.\n\n\nLopatin, Javier, Teja Kattenborn, Mauricio Galleguillos, Jorge F.\nPerez-Quezada, and Sebastian Schmidtlein. 2019. “Using Aboveground\nVegetation Attributes as Proxies for Mapping Peatland Belowground Carbon\nStocks.” Remote Sensing of Environment 231 (September):\n111217. https://doi.org/10.1016/j.rse.2019.111217.\n\n\nSchofield W.B. 1985. “Introduction to Bryology.”\n\n\nTreimun, John. 2017. “Turberas de Chiloé, Ministerio del Medio\nAmbiente, Chile.”\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022.\n“Change Detection from Synthetic Aperture Radar Images via\nGraph-Based Knowledge Supplement Network.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167.\n\n\nWang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022.\n“Self-Supervised Vision Transformers for Joint SAR-optical Representation Learning.”\narXiv. http://arxiv.org/abs/2204.05381.\n\n\nYan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. “Fully\nTransformer Network for Change Detection of\nRemote Sensing Images.” arXiv. http://arxiv.org/abs/2210.00757.\n\n\nZegers, Gabriela, Juan Larraín, María Francisca Díaz, and Juan Armesto.\n2006. “Impacto ecológico y social de la explotación de pomponales\ny turberas de Sphagnum en la Isla Grande de Chiloé.” http://biblioteca.cehum.org/handle/CEHUM2018/1389.\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter\nM. Atkinson. 2020. “Robust Unsupervised Small Area Change\nDetection from SAR Imagery Using Deep\nLearning.” arXiv. http://arxiv.org/abs/2011.11005."
  }
]