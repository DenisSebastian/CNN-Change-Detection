[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CNN Change Detection",
    "section": "",
    "text": "Este documento tiene como objetivo principal de ir registrando los avances en el proceso de desarrollo de la Tesis: “Sistema de Detección de cambios en imágenes satelitales tipo Radar en ambientes naturales protegidos en Chile utilizando redes neuronales convolucionales.”, para optar al Master of Science in Data Science 2022-2023 de la Universidad Adolfo Ibáñez."
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "1  Abstract",
    "section": "",
    "text": "Para abordar estos problemas y detectar cambios en ecosistemas naturales protegidos, se propone utilizar redes neuronales convolucionales para transformar las imágenes en un nuevo espacio de características que facilite la selección de áreas candidatas a haber sufrido cambios debidos a la intervención humana en una estructura de clasificación no supervisada.\nEstos métodos se compararán con técnicas convencionales de filtrado y reducción de dimensionalidad, utilizando métricas de precisión, rendimiento y escalabilidad en diferentes condiciones climáticas."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introducción",
    "section": "",
    "text": "Uno de los grandes problemas de actualmente en el mundo es la degradación de sistemas naturales, como los bosques y matorrales mediterráneos y humedales, los que en muchos casos contienen especies vegetales de especial interés por su endemismo, y alto nivel de amenaza y presión antrópica (Araya-López et al. 2018), (Fassnacht et al. 2021), las que pueden causar cambios en el hábitat, los cuales pueden llevar a la extinción de especies, así como a la reducción de la biodiversidad. Por lo anterior se hace necesario monitorear las áreas naturales protegidas en el país.\nEn Chile, la Superintendencia del Medio Ambiente (SMA) es el principal órgano estatal encargado de velar por el cumplimiento de la normativa ambiental. En sus 8 años de vida ha tenido el gran desafío de fiscalizar más de 17.887 unidades a lo largo del país que tienen al menos una normativa ambiental que los regula. Sin embargo, la institución no cuenta con la infraestructura o los recursos necesarios para fiscalizar adecuadamente todas estas unidades. Para que el rol fiscalizador de la SMA sea eficaz y continuo en el tiempo, se requiere de una infraestructura de monitoreo de gran escala a nivel país.\nEntonces, se hace necesario contar con insumos satelitales de fuentes abiertas y cuyo análisis se automatizado. \nLa creciente disponibilidad de productos satelitales y los avances tecnológicos han fomentado una nueva era de observación de la tierra y monitoreo de los recursos naturales por parte de los gobiernos, creando nuevas oportunidades y desafíos que deben ser abordados. Debido a esta motivación, surge la necesidad de crear un sistema monitoreo satelital, que permita identificar cambios en la estructura de la vegetación en humedales urbanos, turberas de Chiloé y el bosque y matorral esclerófilo de la región Metropolitana para ayudar a su preservación. \nLa utilización de metodologías de análisis de imágenes satelitales para detectar cambios en ecosistemas naturales se ha hecho más común debido al acceso de fuentes abiertas de productos satelitales, y la capacidad de procesamiento computacional. La aplicabilidad de productos satelitales en detección de cambios en el tiempo, destaca el satélite Sentinel-1 de tipo SAR, tiene la ventaja de operar en longitudes de onda que no se ven obstaculizadas por la nubosidad, y puede adquirir datos sobre un lugar durante el día o la noche en todas las condiciones meteorológicas (Wang et al. 2022). Lo anterior lo hace idóneo para la detección de cambios en el territorio, como lo son ecosistemas de turberas, bosques esclerófilos y humedales urbanos, los cuales se ubican en diferentes zonas del país con diversas condiciones climáticas y topográficas. \nAl trabajar con imágenes SAR presentan dos tipos de dificultades para las tarea de detección de cambios, que son el ruido de moteado (speckle noise) y la deformación en los extremos, lo que se puede solucionar con un post procesamiento (Gao et al. 2021) o en su defecto evaluar diferentes técnicas que permitan reducir el efecto de estas dificultados. \nLos investigadores han dedicado grandes esfuerzos a proponer métodos robustos de detección de cambios. Estos métodos pueden clasificarse a grandes rasgos en dos corrientes principales: 1) métodos supervisados y 2) métodos no supervisados. Un método supervisado requiere un conocimiento previo sobre los tipos de cobertura del suelo o un gran número de muestras etiquetadas de alta calidad (Volpi et al. 2013, wang2022). Para el caso del proyecto SAMSARA y en específico en la primera etapa se necesita que sea un método , ya que el método constantemente necesita estar capturando información satelital y comparándola con registros históricos, proceso que debe ser eficiente computacionalmente y sin supervisión experta. \nPara cumplir con lo anterior, en esta tesis se propone la utilización de una red neuronal convolucional del tipo no supervisada, pueda obtener información que permita información relevante y útil de la imágenes satelitales, que facilite el proceso de detección diferencias, a una escala territorial variable de acuerdo a las carctersticas territoriales de los ecosistemas a monitorear. \n\n\n\n\nAraya-López, Rocío A., Javier Lopatin, Fabian E. Fassnacht, and H. Jaime Hernández. 2018. “Monitoring Andean High Altitude Wetlands in Central Chile with Seasonal Optical Data: A Comparison Between Worldview-2 and Sentinel-2 Imagery.” ISPRS Journal of Photogrammetry and Remote Sensing, SI: Latin America Issue, 145 (November): 213–24. https://doi.org/10.1016/j.isprsjprs.2018.04.001.\n\n\nFassnacht, Fabian Ewald, Javiera Poblete-Olivares, Lucas Rivero, Javier Lopatin, Andrés Ceballos-Comisso, and Mauricio Galleguillos. 2021. “Using Sentinel-2 and Canopy Height Models to Derive a Landscape-Level Biomass Map Covering Multiple Vegetation Types.” International Journal of Applied Earth Observation and Geoinformation 94 (February): 102236. https://doi.org/10.1016/j.jag.2020.102236.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nVolpi, Michele, Devis Tuia, Francesca Bovolo, Mikhail Kanevski, and Lorenzo Bruzzone. 2013. “Supervised Change Detection in VHR Images Using Contextual Information and Support Vector Machines.” International Journal of Applied Earth Observation and Geoinformation, Earth Observation and Geoinformation for Environmental Monitoring, 20 (February): 77–85. https://doi.org/10.1016/j.jag.2011.10.013.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022. “Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167."
  },
  {
    "objectID": "problema.html",
    "href": "problema.html",
    "title": "3  Problema u Oportunidad",
    "section": "",
    "text": "El bosque y matorral esclerófilo de Chile es uno de los hotspot mundiales de biodiversidad debido a su gran cantidad de especies endémicas (Myers et al. 2000). Estos ambientes han sido tradicionalmente amenazados y su cobertura ha sido reemplazada por otros usos productivos, como la industria forestal o la producción agrícola (Miranda et al. 2017).\nMonitorear los humedales es desafiante ya que son sistemas en constante cambio debido al efecto de inundaciones y sus procesos internos (Gallant 2015). Esto hace que analizar series de datos temporales sea complejo, ya que cambios en la señal satelital puede ser debido a estos factores y no a una perturbación que se necesite fiscalizar. \nEntre los desafíos más relevantes se encuentran para las instituciones del estado para una eficiente fiscalización y protección de ecosistemas protegidos se encuentran : a) la limitada capacidad de la institución para desarrollar proyectos de este tipo, con alrededor de dos aplicaciones por año (número por debajo a la demanda existente); y b) el procesamiento de las imágenes satelitales sigue siendo costoso. Esto último se debe a que incluso cuando se ha privilegiado el uso de datos libres y herramientas de procesamiento en la nube, con un nivel de automatización que aún no alcanza la madurez. Por lo anterior, se hace necesario diseñar y crear sistemas de monitoreo robustos metodológicamente y eficiente, con capacidad de escalamiento.\nEl análisis de imágenes satelitales permite obtener datos espaciales de manera masiva y remota a lo largo de todo el país y permiten generar alertas de cambios ambientales, bajo una arquitectura de clasificación no supervisada, utilizando principalmente datos de SAR, e.g. Sentinel-1, ya que permite identificar el grado de humedad del humedal y a evitar problemas con la alta presencia de días nublados en el sur del país (i.e., los datos de SAR son capaces de atravesar las nubes, por lo que el clima no es un problema para su uso e implementación) . Sin embargo, la combinación e integración de estos dos tipos de datos satelitales es poco común, por lo que no hay metodologías estándares para implementar de forma directa para cada situación (Gallant 2015).\nEn los últimos años, las redes neuronales convolucionales (CNN) han mejorado mucho el rendimiento de muchas tareas visuales. Se ha demostrado que es bastante eficaz para el aprendizaje robusto de características. Los modelos basados en CNN se han aplicado con éxito en la detección de cambios en imágenes de teledetección (Bazi, Bruzzone, and Melgani 2006, wang2022). En este sentido, (Gong, Cao, and Wu 2012) propone un marco de CNN de extremo a extremo para aprender características discriminativas de la matriz de afinidad mixta para la detección de cambios.\nConsiderando los objetivos de la presente tesis es desarrollar un método automatizado de detección en tiempo real de cambios en la estructura de la vegetación de humedales urbanos, turberas y bosque esclerófilo basado en productos satelitales y algoritmos de inteligencia artificial, a modo de trabajos futuros y si los resultados de detección de cambios automatizados son suficientemente precisos y suficientemente eficiente, estos serán insumos para un siguiente sistema identifique y segmente la zona que se considera como un cambio resultante por intervención humana, puedan ser visualizados en una plataforma de monitoreo que apoye la gestión de Superintendencia del Medio Ambiente, como sistema de aleta temprana. \n\n\n\n\nBazi, Y., L. Bruzzone, and F. Melgani. 2006. “Automatic Identification of the Number and Values of Decision Thresholds in the Log-Ratio Image for Change Detection in SAR Images.” IEEE Geoscience and Remote Sensing Letters 3 (3): 349–53. https://doi.org/10.1109/LGRS.2006.869973.\n\n\nGallant, Alisa L. 2015. “The Challenges of Remote Monitoring of Wetlands.” Remote Sensing 7 (8): 10938–50. https://doi.org/10.3390/rs70810938.\n\n\nGong, Maoguo, Yu Cao, and Qiaodi Wu. 2012. “A Neighborhood-Based Ratio Approach for Change Detection in SAR Images.” IEEE Geoscience and Remote Sensing Letters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\nMiranda, Alejandro, Adison Altamirano, Luis Cayuela, Antonio Lara, and Mauro González. 2017. “Native Forest Loss in the Chilean Biodiversity Hotspot: Revealing the Evidence.” Regional Environmental Change 17 (1): 285–97. https://doi.org/10.1007/s10113-016-1010-7.\n\n\nMyers, Norman, Russell A. Mittermeier, Cristina G. Mittermeier, Gustavo A. B. da Fonseca, and Jennifer Kent. 2000. “Biodiversity Hotspots for Conservation Priorities.” Nature 403 (6772): 853–58. https://doi.org/10.1038/35002501.\n\n\nSteffen, Will, Wendy Broadgate, Lisa Deutsch, Owen Gaffney, and Cornelia Ludwig. 2015. “The Trajectory of the Anthropocene: The Great Acceleration.” The Anthropocene Review 2 (1): 81–98. https://doi.org/10.1177/2053019614564785."
  },
  {
    "objectID": "e-arte.html",
    "href": "e-arte.html",
    "title": "4  Estado del Arte",
    "section": "",
    "text": "En un proceso general de detección de cambios SAR, primero se crea la imagen de diferencia (DI), y la DI se clasifica en píxeles cambiados o no cambiados de forma supervisada o no supervisada (Sumaiya and Shantha Selva Kumari 2016). Sin embargo, la existencia de ruido de moteado es un problema no despreciable en la tarea de detección de cambios SAR. El ruido moteado (speckle) en imágenes SAR aparece como una forma de ruido multiplicativo y degrada la calidad de la imagen. Normalmente se producen falsas alarmas debido a la existencia de ruido speckle (Wu et al. 2019). Por lo tanto, la clasificación de DI para imágenes SAR sigue siendo una tarea difícil (Pham, Mercier, and Michel 2016). Por lo tanto , es crítico diseñar técnicas robustas de detección de cambios que sean efectivas en la supresión del ruido de moteado (speckle) (Saha, Bovolo, and Bruzzone 2021). \nEn los últimos años, los investigadores han dedicado grandes esfuerzos a resolver o paliar el efecto del ruido de moteado. Existen dos tipos de técnicas de detección de cambios: métodos supervisados y no supervisados. En comparación, los métodos no supervisados son más populares ya que comparan dos imágenes SAR multitemporales sin ningún conocimiento previo o muestras etiquetadas manualmente (Gong, Yang, and Zhang 2017). \nLos métodos existentes de detección de cambios sin supervisión se centran principalmente en la generación y clasificación de DI. En la generación de DI se utilizan los operadores log-ratio (Bovolo and Bruzzone 2005), Gauss-ratio (Biao Hou et al. 2014) o neighborhood-based ratio (Gong, Cao, and Wu 2012). Además, se utiliza el coeficiente de variación basado en series temporales de imágenes SAR para evitar el ruido de moteado (Jiang et al. 2020). Para la clasificación de DI, se han empleado muchos métodos de clustering, como fuzzy c-means (FCM) (Mishra, Ghosh, and Ghosh 2012), k- means clustering (Celik 2009), multiple kernel clustering (Jia et al. 2016), y algoritmo de mean-shift (Aiazzi et al. 2013). Además, los métodos de aprendizaje automático basados en campos aleatorios de Markov también se aplican a la tarea del moteado (Bruzzone and Prieto 2000). \nPara mejorar aún más el rendimiento de la clasificación DI, los investigadores incorporan clasificadores basados en aprendizaje profundo al modelo tradicional no supervisado. Gong et al. asignaron primero pseudoetiquetas a los píxeles en DI mediante un clasificador conjunto basado en FCM ((Gong et al. 2016). A continuación, se entrenaron máquinas de Boltzmann restringidas (RBM) para la generación de mapas de cambio. Hou et al. (Bin Hou, Wang, and Liu 2017) presentaron un método de detección de cambios combinando el cálculo de saliencia y el algoritmo de bajo rango. Zhan et al. (Zhan et al. 2017) propusieron un modelo CNN siamés profundo para extraer características discriminantes para la detección de cambios. Zhang et al. (Zhang et al. 2021) presentaron una CNN profunda de co-ocurrencia espacio-temporal de nivel de gris, que es capaz de explotar la información espacio-temporal de imágenes mutlitemporales. Dong et al. (Dong et al. 2022) integraron clustering no supervisado con CNN para aprender representaciones de características amigables con el clustering a partir de imágenes SAR multitemporales. Qu et al. (Qu et al. 2021) propusieron una red de dominio dual. Las características de los dominios de frecuencia y espacial se explotan para mitigar el ruido de moteado.\n\n\n\n\nAiazzi, Bruno, Luciano Alparone, Stefano Baronti, Andrea Garzelli, and Claudia Zoppetti. 2013. “Nonparametric Change Detection in Multitemporal SAR Images Based on Mean-Shift Clustering.” IEEE Transactions on Geoscience and Remote Sensing 51: 2022–31. https://doi.org/10.1109/TGRS.2013.2238946.\n\n\nAmitrano, Donato, Raffaella Guida, and Pasquale Iervolino. 2021. “Semantic Unsupervised Change Detection of Natural Land Cover with Multitemporal Object-Based Analysis on SAR Images.” IEEE Transactions on Geoscience and Remote Sensing 59 (7): 5494–514. https://doi.org/10.1109/TGRS.2020.3029841.\n\n\nBovolo, Francesca, and Lorenzo Bruzzone. 2005. “A Detail-Preserving Scale-Driven Approach to Change Detection in Multitemporal SAR Images.” IEEE T. Geoscience and Remote Sensing 43 (January): 2963–72.\n\n\nBruzzone, L., and D. F. Prieto. 2000. “Automatic Analysis of the Difference Image for Unsupervised Change Detection.” IEEE Transactions on Geoscience and Remote Sensing 38 (3): 1171–82. https://doi.org/10.1109/36.843009.\n\n\nCelik, Turgay. 2009. “Unsupervised Change Detection in Satellite Images Using Principal Component Analysis and k-Means Clustering.” IEEE Geoscience and Remote Sensing Letters 6 (4): 772–76. https://doi.org/10.1109/LGRS.2009.2025059.\n\n\nDong, Huihui, Wenping Ma, Licheng Jiao, Fang Liu, and LingLing Li. 2022. “A Multiscale Self-Attention Deep Clustering for Change Detection in SAR Images.” IEEE Transactions on Geoscience and Remote Sensing 60: 1–16. https://doi.org/10.1109/TGRS.2021.3073562.\n\n\nGong, Maoguo, Yu Cao, and Qiaodi Wu. 2012. “A Neighborhood-Based Ratio Approach for Change Detection in SAR Images.” IEEE Geoscience and Remote Sensing Letters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\nGong, Maoguo, Hailun Yang, and Puzhao Zhang. 2017. “Feature Learning and Change Feature Classification Based on Deep Learning for Ternary Change Detection in SAR Images.” ISPRS Journal of Photogrammetry and Remote Sensing 129: 212–25. https://doi.org/10.1016/j.isprsjprs.2017.05.001.\n\n\nGong, Maoguo, Jiaojiao Zhao, Jia Liu, Qiguang Miao, and Licheng Jiao. 2016. “Change Detection in Synthetic Aperture Radar Images Based on Deep Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems 27 (1): 125–38. https://doi.org/10.1109/TNNLS.2015.2435783.\n\n\nHou, Biao, Qian Wei, Yaoguo Zheng, and Shuang Wang. 2014. “Unsupervised Change Detection in SAR Image Based on Gauss-Log Ratio Image Fusion and Compressed Projection.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 7 (8): 3297–317. https://doi.org/10.1109/JSTARS.2014.2328344.\n\n\nHou, Bin, Yunhong Wang, and Qingjie Liu. 2017. “Change Detection Based on Deep Features and Low Rank.” IEEE Geoscience and Remote Sensing Letters 14 (12): 2418–22. https://doi.org/10.1109/LGRS.2017.2766840.\n\n\nJia, Lu, Ming Li, Peng Zhang, Yan Wu, and Huahui Zhu. 2016. “SAR Image Change Detection Based on Multiple Kernel k-Means Clustering with Local-Neighborhood Information.” IEEE Geoscience and Remote Sensing Letters 13 (6): 856–60. https://doi.org/10.1109/LGRS.2016.2550666.\n\n\nJiang, Mi, Andy Hooper, Xin Tian, Jia Xu, Sai-Nan Chen, Zhang-Feng Ma, and Xiao Cheng. 2020. “Delineation of Built-up Land Change from SAR Stack by Analysing the Coefficient of Variation.” ISPRS Journal of Photogrammetry and Remote Sensing 169: 93–108. https://doi.org/10.1016/j.isprsjprs.2020.08.023.\n\n\nMishra, Niladri, Susmita Ghosh, and Ashish Ghosh. 2012. “Fuzzy Clustering Algorithms Incorporating Local Information for Change Detection in Remotely Sensed Images.” Applied Soft Computing 12 (August): 2683–92. https://doi.org/10.1016/j.asoc.2012.03.060.\n\n\nPham, Minh-Tan, Grégoire Mercier, and Julien Michel. 2016. “Change Detection Between SAR Images Using a Pointwise Approach and Graph Theory.” IEEE Transactions on Geoscience and Remote Sensing 54 (4): 2020–32. https://doi.org/10.1109/TGRS.2015.2493730.\n\n\nQu, Xiaofan, Feng Gao, Junyu Dong, Qian Du, and Heng-Chao Li. 2021. “Change Detection in Synthetic Aperture Radar Images Using a Dual-Domain Network,” April.\n\n\nSaha, Sudipan, Francesca Bovolo, and Lorenzo Bruzzone. 2021. “Building Change Detection in VHR SAR Images via Unsupervised Deep Transcoding.” IEEE Transactions on Geoscience and Remote Sensing 59 (3): 1917–29. https://doi.org/10.1109/TGRS.2020.3000296.\n\n\nSumaiya, M. N., and R. Shantha Selva Kumari. 2016. “Logarithmic Mean-Based Thresholding for SAR Image Change Detection.” IEEE Geoscience and Remote Sensing Letters 13 (11): 1726–28. https://doi.org/10.1109/LGRS.2016.2606119.\n\n\nWu, Hui, Heng-Chao Lit, Gang Yang, and Wen Yang. 2019. “Change Detection of Remote Sensing Images Based on Weighted Nonnegative Matrix Factorization.” In 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), 1–4. https://doi.org/10.1109/Multi-Temp.2019.8866946.\n\n\nZhan, Yang, Kun Fu, Menglong Yan, Xian Sun, Hongqi Wang, and Xiaosong Qiu. 2017. “Change Detection Based on Deep Siamese Convolutional Network for Optical Aerial Images.” IEEE Geoscience and Remote Sensing Letters 14 (10): 1845–49. https://doi.org/10.1109/LGRS.2017.2738149.\n\n\nZhang, Xiao, Xin Su, Qiangqiang Yuan, and Qing Wang. 2021. “Spatial-Temporal Gray-Level Co-Occurrence Aware CNN for SAR Image Change Detection.” IEEE Geoscience and Remote Sensing Letters PP (September): 1–5. https://doi.org/10.1109/LGRS.2021.3110302."
  },
  {
    "objectID": "hipotesis.html",
    "href": "hipotesis.html",
    "title": "5  Hipótesis",
    "section": "",
    "text": "El uso de redes neuronales convolucionales en imágenes satelitales tipo radar de apertura sintética (SAR) mejorará la detección de cambios y selección de zonas afectadas por la intervención humana en ecosistemas naturales protegidos en comparación con técnicas convencionales como los métodos algebraicos y de reducción de dimensionalidad."
  },
  {
    "objectID": "objetivos.html#Objetivos-Generales",
    "href": "objetivos.html#Objetivos-Generales",
    "title": "6  Objetivos",
    "section": "6.1 Objetivos Generales",
    "text": "6.1 Objetivos Generales\nCrear sistema de detección de cambios no supervisado en ecosistemas naturales protegidos haciendo uso redes neuronales convolucionales, en imágenes satelitales tipo radar."
  },
  {
    "objectID": "objetivos.html#Objetivos-Espec",
    "href": "objetivos.html#Objetivos-Espec",
    "title": "6  Objetivos",
    "section": "6.2 Objetivos Específicos",
    "text": "6.2 Objetivos Específicos\n\nImplementar una red neuronal convolucional, que pueda detectar cambios en imágenes tipo radar en diferentes tiempos de forma no supervisada.\nComparar los resultados de la detección de cambio, de técnicas convencionales como los métodos algebraícos y de reducción de dimensionalidad con los obtenidos mediante el uso de redes neuronales convolucionales."
  },
  {
    "objectID": "a-estudio.html",
    "href": "a-estudio.html",
    "title": "7  Área de Estudio",
    "section": "",
    "text": "Considerando que se pretende realizar un sistema de detección de cambios por en ecosistemas naturales protegidos correspondientes a turberas de Chiloé, humedales urbanos y el bosque y matorral esclerófilo de la región Metropolitana Figure 7.1, a continuación se describir sus características."
  },
  {
    "objectID": "a-estudio.html#turbera",
    "href": "a-estudio.html#turbera",
    "title": "7  Área de Estudio",
    "section": "7.1 Turberas en Chiloé",
    "text": "7.1 Turberas en Chiloé\nEl concepto de turba debe ser entendido como un sedimento natural de tipo fitógeno, poroso, no consolidado, constituido por materia orgánica parcialmente descompuesta, acumulada en un ambiente saturado de agua. De esta forma, se puede entender al concepto de turbera como un depósito de turba con un espesor de, al menos, 30 cm. (Hauser 1996)\nLas turberas solo cubren el 3 % de la superficie terrestre del planeta pero almacenan más carbono que todos los bosques de la Tierra si se mantienen húmedas.\nSegún (Hauser 1996) (Hauser 1996), el origen de las turberas se encuentra en las eras glaciares del Pleistoceno, cuando grandes extensiones de casquetes glaciares cubrieron el valle central de la Región de Los Lagos, incluyendo a la Isla Grande de Chiloé. El posterior retiro de los glaciares dejó masas de agua tierra adentro, formando los grandes lagos y lagunas glaciares que en la actualidad componen el paisaje de la región.\nRevisar y papers:\nEvaluation of impacts of management in an anthropogenic peatland using field and remote sensing data (Cabezas et al. 2015)\nUsing aboveground vegetation attributes as proxies.pdf (Lopatin et al. 2019)\nDisturbance alters relationships between soil carb.pdf (Lopatin et al. 2022)"
  },
  {
    "objectID": "a-estudio.html#humedales",
    "href": "a-estudio.html#humedales",
    "title": "7  Área de Estudio",
    "section": "7.2 Humedales Urbanos",
    "text": "7.2 Humedales Urbanos\nPor desarrollar …"
  },
  {
    "objectID": "a-estudio.html#bosque-esc",
    "href": "a-estudio.html#bosque-esc",
    "title": "7  Área de Estudio",
    "section": "7.3 Bosque y matorral esclerófilo de la región Metropolitana",
    "text": "7.3 Bosque y matorral esclerófilo de la región Metropolitana\nPor desarrollar …\n\n\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus perspectivas de utilización.” Revista Geológica de Chile 23 (2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208."
  },
  {
    "objectID": "adquisicion.html",
    "href": "adquisicion.html",
    "title": "8  Adquisición de Datos",
    "section": "",
    "text": "La misión Sentinel-1 es el Observatorio Radar Europeo de la iniciativa conjunta Copernicus de la Comisión Europea (CE) y la Agencia Espacial Europea (ESA). Copernicus es una iniciativa europea para la puesta en marcha de servicios de información relacionados con el medio ambiente y la seguridad. Se basa en los datos de observación recibidos de los satélites de observación de la Tierra y la información terrestre.\nLa misión Sentinel-1 incluye imágenes en banda C que operan en cuatro modos de imagen exclusivos con diferente resolución (hasta 5 m) y cobertura (hasta 400 km). Ofrece capacidad de doble polarización, tiempos de revisita muy cortos y una rápida entrega de productos. Para cada observación, se dispone de mediciones precisas de la posición y la actitud de la nave espacial.\nEl radar de apertura sintética (SAR) tiene la ventaja de operar en longitudes de onda que no se ven obstaculizadas por la nubosidad o la falta de iluminación, y puede adquirir datos sobre un lugar durante el día o la noche en todas las condiciones meteorológicas. Sentinel-1, con su instrumento C-SAR, puede ofrecer una vigilancia fiable y repetida de una zona amplia.\nLa misión está compuesta por una constelación de dos satélites, Sentinel-1A y Sentinel-1B, que comparten el mismo plano orbital.\nSentinel-1 está diseñado para trabajar en un modo de operación preprogramado y libre de conflictos, obteniendo imágenes de todas las masas terrestres mundiales, zonas costeras y rutas marítimas en alta resolución y cubriendo el océano mundial con viñetas. Esto garantiza la fiabilidad del servicio requerida por los servicios operativos y un archivo de datos consistente a largo plazo construido para aplicaciones basadas en series temporales largas."
  },
  {
    "objectID": "adquisicion.html#resolución-temporal-sentinel-1",
    "href": "adquisicion.html#resolución-temporal-sentinel-1",
    "title": "8  Adquisición de Datos",
    "section": "8.1 Resolución Temporal Sentinel-1",
    "text": "8.1 Resolución Temporal Sentinel-1\nUn solo satélite Sentinel-1 podrá cartografiar el mundo entero una vez cada 12 días. La constelación de dos satélites ofrece un ciclo de repetición exacta de 6 días. La constelación tendrá una frecuencia de repetición (ascendente/descendente) de 3 días en el ecuador, menos de 1 día en el Ártico y se espera que proporcione cobertura sobre Europa, Canadá y las principales rutas marítimas en 1-3 días Figure 8.1, independientemente de las condiciones meteorológicas. Los datos del radar se entregarán a los servicios de Copernicus una hora después de su adquisición.\n\n\n\nFigure 8.1: Tiempo de Revisista en días del Satélite Sentinel-1\n\n\nSentinel-1 se encuentra en una órbita casi polar, sincrónica al sol, con un ciclo de repetición de 12 días y 175 órbitas por ciclo para un solo satélite. Tanto Sentinel-1A como Sentinel-1B comparten el mismo plano orbital con una diferencia de fase orbital de 180°. La cobertura geográfica es la que describe en la siguiente imagen Figure 8.2:\n\n\n\nFigure 8.2: Cobertura Geográfica de Satélite Sentinel-1"
  },
  {
    "objectID": "adquisicion.html#intrumentos-a-bordo-sentinel-1",
    "href": "adquisicion.html#intrumentos-a-bordo-sentinel-1",
    "title": "8  Adquisición de Datos",
    "section": "8.2 Intrumentos a Bordo Sentinel-1",
    "text": "8.2 Intrumentos a Bordo Sentinel-1\nSentinel-1 lleva un único instrumento de radar de apertura sintética en banda C que opera a una frecuencia central de 5,405 GHz. Incluye una antena activa phased array de orientación derecha que proporciona un rápido escaneo en elevación y azimut, una capacidad de almacenamiento de datos de 1 410 Gb y una capacidad de enlace descendente en banda X de 520 Mbit/s.\nEl instrumento C-SAR soporta el funcionamiento en polarización dual (HH+HV, VV+VH) implementado a través de una cadena de transmisión (conmutable a H o V) y dos cadenas de recepción paralelas para la polarización H y V. Los datos de doble polarización son útiles para la clasificación de la cubierta terrestre y las aplicaciones del hielo marino.\nSentinel-1 funciona en cuatro modos de adquisición exclusivos Figure 8.3:\n\n\n\nFigure 8.3: Modos de adquisión de imágenes de Sentinel-1\n\n\n\n\nModo Stripmap (SM) {#sm}:\n\nEl modo de imagen Stripmap se proporciona para la continuidad con las misiones ERS y Envisat. El modo Stripmap proporciona una cobertura con una resolución de 5 m por 5 m sobre una estrecha franja de 80 km. Se puede seleccionar una de las seis franjas de imágenes cambiando el ángulo de incidencia del haz y el ancho del haz de elevación.\n\n\n\nModo de hilera ancha interferométrica (IW):\n\nEl modo interferométrico de franja ancha (IW) permite combinar una gran anchura de franja (250 km) con una resolución geométrica moderada (5 m por 20 m). El modo IW toma imágenes de tres sub-bandas utilizando la Observación del Terreno con Escáneres Progresivos SAR (TOPSAR). Con la técnica TOPSAR, además de dirigir el haz en el rango como en SCANSAR, el haz también se dirige electrónicamente de atrás hacia adelante en la dirección acimut para cada ráfaga, evitando el festoneado y dando como resultado una imagen de mayor calidad. La interferometría está garantizada por un solapamiento suficiente del espectro Doppler (en el dominio acimutino) y del espectro del número de onda (en el dominio de la elevación). La técnica TOPSAR garantiza una calidad de imagen homogénea en toda la franja. El modo IW es el modo de adquisición por defecto sobre tierra.\n\n\n\nModo de barrido extra ancho (EW)\n\nEl modo de imagen de franja extra ancha está destinado a los servicios operativos marítimos, de hielo y de zonas polares en los que se requiere una amplia cobertura y tiempos de revisita cortos. El modo EW funciona de forma similar al modo IW, empleando una técnica TOPSAR que utiliza cinco sub-surcos en lugar de tres, lo que resulta en una resolución menor (20 m por 40 m). El modo EW también se puede utilizar para la interferometría como en el modo IW.\n\n\n\nModo Onda (WV)\n\nEl modo Wave del SENTINEL-1, junto con los modelos globales de olas oceánicas, puede ayudar a determinar la dirección, la longitud de onda y las alturas de las olas en los océanos abiertos. Las adquisiciones en el modo de onda se componen de imágenes de mapa de franjas de 20 km por 20 km, adquiridas alternativamente en dos ángulos de incidencia diferentes. Las imágenes de olas se adquieren cada 100 km, con imágenes en el mismo ángulo de incidencia separadas por 200 km."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "13  References",
    "section": "",
    "text": "Aiazzi, Bruno, Luciano Alparone, Stefano Baronti, Andrea Garzelli, and\nClaudia Zoppetti. 2013. “Nonparametric Change Detection in\nMultitemporal SAR Images Based on Mean-Shift Clustering.”\nIEEE Transactions on Geoscience and Remote Sensing 51: 2022–31.\nhttps://doi.org/10.1109/TGRS.2013.2238946.\n\n\nAmitrano, Donato, Raffaella Guida, and Pasquale Iervolino. 2021.\n“Semantic Unsupervised Change Detection of Natural Land Cover with\nMultitemporal Object-Based Analysis on SAR Images.” IEEE\nTransactions on Geoscience and Remote Sensing 59 (7): 5494–514. https://doi.org/10.1109/TGRS.2020.3029841.\n\n\nAraya-López, Rocío A., Javier Lopatin, Fabian E. Fassnacht, and H. Jaime\nHernández. 2018. “Monitoring Andean High Altitude Wetlands in\nCentral Chile with Seasonal Optical Data: A Comparison Between\nWorldview-2 and Sentinel-2 Imagery.” ISPRS Journal of\nPhotogrammetry and Remote Sensing, SI: Latin America Issue, 145\n(November): 213–24. https://doi.org/10.1016/j.isprsjprs.2018.04.001.\n\n\nBazi, Y., L. Bruzzone, and F. Melgani. 2006. “Automatic\nIdentification of the Number and Values of Decision Thresholds in the\nLog-Ratio Image for Change Detection in SAR Images.” IEEE\nGeoscience and Remote Sensing Letters 3 (3): 349–53. https://doi.org/10.1109/LGRS.2006.869973.\n\n\nBovolo, Francesca, and Lorenzo Bruzzone. 2005. “A\nDetail-Preserving Scale-Driven Approach to Change Detection in\nMultitemporal SAR Images.” IEEE T. Geoscience and Remote\nSensing 43 (January): 2963–72.\n\n\nBruzzone, L., and D. F. Prieto. 2000. “Automatic Analysis of the\nDifference Image for Unsupervised Change Detection.” IEEE\nTransactions on Geoscience and Remote Sensing 38 (3): 1171–82. https://doi.org/10.1109/36.843009.\n\n\nCabezas, Juli’an, Mauricio Galleguillos, Ariel Vald’es, Juan P. Fuentes,\nCecilia P’erez, and Jorge F. Perez-Quezada. 2015. “Evaluation of\nImpacts of Management in an Anthropogenic Peatland Using Field and\nRemote Sensing Data.” Ecosphere 6 (12): 1–24. https://doi.org/10.1890/ES15-00232.1.\n\n\nCelik, Turgay. 2009. “Unsupervised Change Detection in Satellite\nImages Using Principal Component Analysis and k-Means Clustering.” IEEE\nGeoscience and Remote Sensing Letters 6 (4): 772–76. https://doi.org/10.1109/LGRS.2009.2025059.\n\n\nChen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu\nLiu, and Haifeng Li. 2021. “DASNet: Dual\nAttentive Fully Convolutional Siamese Networks for Change Detection of\nHigh Resolution Satellite Images.” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing 14:\n1194–1206. https://doi.org/10.1109/JSTARS.2020.3037893.\n\n\nDíaz, María F., Juan Larraín, Gabriela Zegers, and Carolina Tapia. 2008.\n“Floristic and Hydrological Characterization of Chiloé Island\nPeatlands, Chile.” Revista Chilena de Historia Natural\n81 (4): 455–68. https://doi.org/10.4067/S0716-078X2008000400002.\n\n\nDong, Huihui, Wenping Ma, Licheng Jiao, Fang Liu, and LingLing Li. 2022.\n“A Multiscale Self-Attention Deep Clustering for Change Detection\nin SAR Images.” IEEE Transactions on Geoscience and Remote\nSensing 60: 1–16. https://doi.org/10.1109/TGRS.2021.3073562.\n\n\nFassnacht, Fabian Ewald, Javiera Poblete-Olivares, Lucas Rivero, Javier\nLopatin, Andrés Ceballos-Comisso, and Mauricio Galleguillos. 2021.\n“Using Sentinel-2 and Canopy Height Models to Derive a\nLandscape-Level Biomass Map Covering Multiple Vegetation Types.”\nInternational Journal of Applied Earth Observation and\nGeoinformation 94 (February): 102236. https://doi.org/10.1016/j.jag.2020.102236.\n\n\nGallant, Alisa L. 2015. “The Challenges of Remote Monitoring of\nWetlands.” Remote Sensing 7 (8): 10938–50. https://doi.org/10.3390/rs70810938.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021a. “SAR\nImage Change Detection Based on Multiscale Capsule Network.”\nIEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\n———. 2021b. “SAR Image Change Detection Based on Multiscale\nCapsule Network.” IEEE Geoscience and Remote Sensing\nLetters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nGong, Maoguo, Yu Cao, and Qiaodi Wu. 2012a. “A Neighborhood-Based\nRatio Approach for Change Detection in SAR Images.” IEEE\nGeoscience and Remote Sensing Letters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\n———. 2012b. “A Neighborhood-Based Ratio Approach for Change\nDetection in SAR Images.” IEEE Geoscience and Remote Sensing\nLetters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\nGong, Maoguo, Hailun Yang, and Puzhao Zhang. 2017. “Feature\nLearning and Change Feature Classification Based on Deep Learning for\nTernary Change Detection in SAR Images.” ISPRS\nJournal of Photogrammetry and Remote Sensing 129: 212–25. https://doi.org/10.1016/j.isprsjprs.2017.05.001.\n\n\nGong, Maoguo, Jiaojiao Zhao, Jia Liu, Qiguang Miao, and Licheng Jiao.\n2016. “Change Detection in Synthetic Aperture\nRadar Images Based on Deep Neural Networks.”\nIEEE Transactions on Neural Networks and Learning Systems 27\n(1): 125–38. https://doi.org/10.1109/TNNLS.2015.2435783.\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus\nperspectivas de utilización.” Revista Geológica de Chile 23\n(2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208.\n\n\nHou, Biao, Qian Wei, Yaoguo Zheng, and Shuang Wang. 2014.\n“Unsupervised Change Detection in SAR Image Based on Gauss-Log\nRatio Image Fusion and Compressed Projection.” IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing\n7 (8): 3297–317. https://doi.org/10.1109/JSTARS.2014.2328344.\n\n\nHou, Bin, Yunhong Wang, and Qingjie Liu. 2017. “Change Detection\nBased on Deep Features and Low Rank.” IEEE Geoscience and\nRemote Sensing Letters 14 (12): 2418–22. https://doi.org/10.1109/LGRS.2017.2766840.\n\n\nJia, Lu, Ming Li, Peng Zhang, Yan Wu, and Huahui Zhu. 2016. “SAR\nImage Change Detection Based on Multiple Kernel k-Means Clustering with\nLocal-Neighborhood Information.” IEEE Geoscience and Remote\nSensing Letters 13 (6): 856–60. https://doi.org/10.1109/LGRS.2016.2550666.\n\n\nJiang, Mi, Andy Hooper, Xin Tian, Jia Xu, Sai-Nan Chen, Zhang-Feng Ma,\nand Xiao Cheng. 2020. “Delineation of Built-up Land Change from\nSAR Stack by Analysing the Coefficient of\nVariation.” ISPRS Journal of Photogrammetry and Remote\nSensing 169: 93–108. https://doi.org/10.1016/j.isprsjprs.2020.08.023.\n\n\nLopatin, Javier, Roc’ıo ArayaNANAL’opez, Mauricio Galleguillos, and\nJorge F. PereNAzNAQueza. 2022. “Disturbance Alters Relationships\nBetween Soil Carbon Pools and Aboveground Vegetation Attributes in an\nAnthropogenic Peatland in Patagonia.” Ecology\nand Evolution 12 (3). https://doi.org/10.1002/ece3.8694.\n\n\nLopatin, Javier, Teja Kattenborn, Mauricio Galleguillos, Jorge F.\nPerez-Quezada, and Sebastian Schmidtlein. 2019. “Using Aboveground\nVegetation Attributes as Proxies for Mapping Peatland Belowground Carbon\nStocks.” Remote Sensing of Environment 231 (September):\n111217. https://doi.org/10.1016/j.rse.2019.111217.\n\n\nMiranda, Alejandro, Adison Altamirano, Luis Cayuela, Antonio Lara, and\nMauro González. 2017. “Native Forest Loss in the Chilean\nBiodiversity Hotspot: Revealing the Evidence.” Regional\nEnvironmental Change 17 (1): 285–97. https://doi.org/10.1007/s10113-016-1010-7.\n\n\nMishra, Niladri, Susmita Ghosh, and Ashish Ghosh. 2012. “Fuzzy\nClustering Algorithms Incorporating Local Information for Change\nDetection in Remotely Sensed Images.” Applied Soft\nComputing 12 (August): 2683–92. https://doi.org/10.1016/j.asoc.2012.03.060.\n\n\nMyers, Norman, Russell A. Mittermeier, Cristina G. Mittermeier, Gustavo\nA. B. da Fonseca, and Jennifer Kent. 2000. “Biodiversity Hotspots\nfor Conservation Priorities.” Nature 403 (6772): 853–58.\nhttps://doi.org/10.1038/35002501.\n\n\nPham, Minh-Tan, Grégoire Mercier, and Julien Michel. 2016. “Change\nDetection Between SAR Images Using a Pointwise Approach and Graph\nTheory.” IEEE Transactions on Geoscience and Remote\nSensing 54 (4): 2020–32. https://doi.org/10.1109/TGRS.2015.2493730.\n\n\nQu, Xiaofan, Feng Gao, Junyu Dong, Qian Du, and Heng-Chao Li. 2021.\n“Change Detection in Synthetic Aperture Radar Images Using a\nDual-Domain Network,” April.\n\n\nSaha, Sudipan, Francesca Bovolo, and Lorenzo Bruzzone. 2021.\n“Building Change Detection in VHR SAR Images via Unsupervised Deep\nTranscoding.” IEEE Transactions on Geoscience and Remote\nSensing 59 (3): 1917–29. https://doi.org/10.1109/TGRS.2020.3000296.\n\n\nSchofield W.B. 1985. “Introduction to Bryology.”\n\n\nSteffen, Will, Wendy Broadgate, Lisa Deutsch, Owen Gaffney, and Cornelia\nLudwig. 2015. “The Trajectory of the Anthropocene: The Great\nAcceleration.” The Anthropocene Review 2 (1): 81–98. https://doi.org/10.1177/2053019614564785.\n\n\nSumaiya, M. N., and R. Shantha Selva Kumari. 2016. “Logarithmic\nMean-Based Thresholding for SAR Image Change Detection.” IEEE\nGeoscience and Remote Sensing Letters 13 (11): 1726–28. https://doi.org/10.1109/LGRS.2016.2606119.\n\n\nTreimun, John. 2017. “Turberas de Chiloé, Ministerio del Medio\nAmbiente, Chile.”\n\n\nVolpi, Michele, Devis Tuia, Francesca Bovolo, Mikhail Kanevski, and\nLorenzo Bruzzone. 2013. “Supervised Change Detection in VHR Images\nUsing Contextual Information and Support Vector Machines.”\nInternational Journal of Applied Earth Observation and\nGeoinformation, Earth Observation and Geoinformation for\nEnvironmental Monitoring, 20 (February): 77–85. https://doi.org/10.1016/j.jag.2011.10.013.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022.\n“Change Detection from Synthetic Aperture Radar Images via\nGraph-Based Knowledge Supplement Network.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167.\n\n\nWang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022.\n“Self-Supervised Vision Transformers for Joint SAR-optical Representation Learning.”\narXiv. http://arxiv.org/abs/2204.05381.\n\n\nWu, Hui, Heng-Chao Lit, Gang Yang, and Wen Yang. 2019. “Change\nDetection of Remote Sensing Images Based on Weighted Nonnegative Matrix\nFactorization.” In 2019 10th International Workshop on the\nAnalysis of Multitemporal Remote Sensing Images (MultiTemp), 1–4.\nhttps://doi.org/10.1109/Multi-Temp.2019.8866946.\n\n\nYan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. “Fully\nTransformer Network for Change Detection of\nRemote Sensing Images.” arXiv. http://arxiv.org/abs/2210.00757.\n\n\nZegers, Gabriela, Juan Larraín, María Francisca Díaz, and Juan Armesto.\n2006. “Impacto Ecológico y Social de La Explotación de Pomponales\ny Turberas de Sphagnum En La Isla Grande de Chiloé.” http://biblioteca.cehum.org/handle/CEHUM2018/1389.\n\n\nZhan, Yang, Kun Fu, Menglong Yan, Xian Sun, Hongqi Wang, and Xiaosong\nQiu. 2017. “Change Detection Based on Deep Siamese Convolutional\nNetwork for Optical Aerial Images.” IEEE Geoscience and\nRemote Sensing Letters 14 (10): 1845–49. https://doi.org/10.1109/LGRS.2017.2738149.\n\n\nZhang, Xiao, Xin Su, Qiangqiang Yuan, and Qing Wang. 2021.\n“Spatial-Temporal Gray-Level Co-Occurrence Aware CNN for SAR Image\nChange Detection.” IEEE Geoscience and Remote Sensing\nLetters PP (September): 1–5. https://doi.org/10.1109/LGRS.2021.3110302.\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter\nM. Atkinson. 2020. “Robust Unsupervised\nSmall Area Change\nDetection from SAR Imagery\nUsing Deep Learning.”"
  },
  {
    "objectID": "rev_papers.html",
    "href": "rev_papers.html",
    "title": "Revisión papers",
    "section": "",
    "text": "En está sección se registrará los aspectos generales de la revisión bibliográfica a modo de conocer el estado del arte en técnicas de identificación de cambios en imagenes satelitales utilizado redes neuronales convolucionales.\nPrincipales Desafíos:\nPosibles Soluciones"
  },
  {
    "objectID": "rev_papers.html#detección-de-cambios-con-imágenes-radar",
    "href": "rev_papers.html#detección-de-cambios-con-imágenes-radar",
    "title": "Revisión papers",
    "section": "Detección de Cambios con Imágenes Radar",
    "text": "Detección de Cambios con Imágenes Radar\n\nMs-CapsNet\nSAR Image Change Detection Based on Multiscale Capsule Network (Gao et al. 2021)\n\nResumen:\n\nLos métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los retos del ruido de moteado y la sensibilidad a la deformación. Para mitigar estos problemas, propone una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se ha diseñado un módulo de convolución de fusión adaptativa (AFC) para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.\n\nIndex Terms:\n\nChange detection, multiscale capsule network, synthetic aperture radar, deep learning.\n\n\n\n\n\nMs-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala\n\n\nResultados y Análisis de Experimentos.\n\n\n\nVisualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet."
  },
  {
    "objectID": "rev_papers.html#graph-based-knowledge-supplement",
    "href": "rev_papers.html#graph-based-knowledge-supplement",
    "title": "Revisión papers",
    "section": "Graph-Based Knowledge Supplement",
    "text": "Graph-Based Knowledge Supplement\nChange Detection From Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network (J. Wang et al. 2022)\n\nResumen:\n\nLa detección de cambios en las imágenes del radar de apertura sintética (SAR) es una tarea vital pero difícil en el campo del análisis de imágenes de teledetección. La mayoría de los trabajos anteriores adoptan un método auto-supervisado que utiliza muestras pseudo-etiquetadas para guiar el entrenamiento y las pruebas subsecuentes. Sin embargo, las redes profundas suelen requerir muchas muestras de alta calidad para la optimización de los parámetros. El ruido en las pseudo-etiquetas afecta inevitablemente al rendimiento final de la detección de cambios. Para resolver el problema, proponemos una red de complemento de conocimiento basada en grafos (GKSNet). Para ser más específicos, extraemos información discriminatoria del conjunto de datos etiquetados existente como conocimiento adicional, para suprimir hasta cierto punto los efectos adversos de las muestras ruidosas. A continuación, diseñamos un módulo de transferencia de grafos para destilar información contextual de forma atenta desde el conjunto de datos etiquetados al conjunto de datos de destino, lo que permite salvar la correlación de características entre los conjuntos de datos. Para validar el método propuesto, realizamos amplios experimentos con cuatro conjuntos de datos de SAR, que demostraron la superioridad de la GKSNet propuesta en comparación con varias líneas de base del estado de la técnica.\n\nIndex Terms:\n\nChange detection, graph dependency fusion, knowledge supplement network, synthetic aperture radar (SAR).\n\n\n\n\n\nSchematic illustration of the proposed GKSNet. The image features extracted by deep CNNs are projected into a graph representation. Then, the graph representations are transferred and fused via graph transfer module across different datasets. Finally, features from different graphs are fused via intergraph fusion module. Through feature fusion, the model exploits the common knowledge and bridge the feature correlation from different datasets. The obtained evolved features are capable of improving the change detection performance."
  },
  {
    "objectID": "rev_papers.html#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022",
    "href": "rev_papers.html#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022",
    "title": "Revisión papers",
    "section": "Fully Transformer Network for Change Detection of Remote Sensing Images (Yan, Wan, and Zhang 2022)",
    "text": "Fully Transformer Network for Change Detection of Remote Sensing Images (Yan, Wan, and Zhang 2022)\n\nResumen:\n\nRecientemente, la detección de cambios (CD) de las imágenes de teledetección ha logrado un gran progreso con los avances del aprendizaje profundo. Sin embargo, los métodos actuales generalmente ofrecen regiones de CD incompletas y límites de CD irregulares debido a la limitada capacidad de representación de las características visuales extraídas. Para aliviar estos problemas, en este trabajo proponemos un novedoso marco de aprendizaje llamado Fully Transformer Network (FTN) para la CD de imágenes de teledetección, que mejora la extracción de características desde una vista global y combina características visuales de varios niveles de forma piramidal. Más concretamente, el marco propuesto utiliza en primer lugar las ventajas de los transformadores en el modelado de dependencias de largo alcance. Puede ayudar a aprender más características discriminativas de nivel global y obtener regiones completas de CD. A continuación, introducimos una estructura piramidal para agregar características visuales multinivel a partir de Transformers para mejorar las características. La estructura piramidal injertada con un Módulo de Atención Progresiva (PAM) puede mejorar la capacidad de representación de características con interdependencias adicionales a través de atenciones de canal. Por último, para entrenar mejor el marco, utilizamos el aprendizaje supervisado en profundidad con múltiples funciones de pérdida de límites. Amplios experimentos demuestran que nuestro método propuesto alcanza un nuevo rendimiento de vanguardia en cuatro puntos de referencia públicos de CD.\n\n\n\n\n\n\n\nLa estructura general del marco propuesto. Fully Transformer Network for Change Detection of Remote Sensing Images\n\n\n\n\nIndex Terms: Fully Transformer Network, Change Detection, Remote Sensing Image."
  },
  {
    "objectID": "rev_papers.html#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022",
    "href": "rev_papers.html#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022",
    "title": "Revisión papers",
    "section": "Self-supervised Vision Transformers for Joint SAR-optical Representation Learning (Y. Wang, Albrecht, and Zhu 2022)",
    "text": "Self-supervised Vision Transformers for Joint SAR-optical Representation Learning (Y. Wang, Albrecht, and Zhu 2022)\n\nResumen:\n\nEl aprendizaje autosupervisado (SSL) ha suscitado un gran interés en la teledetección y la observación de la Tierra debido a su capacidad para aprender representaciones independientes de la tarea sin necesidad de anotaciones humanas. Mientras que la mayoría de los trabajos existentes sobre SSL en teledetección utilizan columnas vertebrales ConvNet y se centran en una sola modalidad, nosotros exploramos el potencial de los transformadores de visión (ViTs) para el aprendizaje conjunto de representaciones SAR-ópticas. Basándonos en DINO, un algoritmo SSL de última generación que destila conocimiento de dos vistas aumentadas de una imagen de entrada, combinamos imágenes SAR y ópticas concatenando todos los canales en una entrada unificada. Posteriormente, enmascaramos aleatoriamente los canales de una modalidad como estrategia de aumento de datos. Durante el entrenamiento, el modelo se alimenta de pares de imágenes ópticas, SAR y SAR-ópticas para aprender representaciones internas e intra-modales. Los resultados experimentales que emplean el conjunto de datos BigEarthNet-MM demuestran las ventajas tanto de los ejes ViT como del algoritmo SSL multimodal propuesto DINO-MM.\n\n\n\n\n\n\n\nDINO-MM: el algoritmo SSL conjunto SAR-óptico propuesto. La imagen SAR-óptica concatenada se toma como entrada bruta. Se transforma aleatoriamente en dos vistas aumentadas y se introduce en una red maestro-estudiante basada en DINO.\n\n\n\n\nIndex Terms: Self-supervised learning, vision trans- former, multimodal representation learning"
  },
  {
    "objectID": "rev_papers.html#detección-de-cambios-con-imágenes-de-alta-resolución",
    "href": "rev_papers.html#detección-de-cambios-con-imágenes-de-alta-resolución",
    "title": "Revisión papers",
    "section": "Detección de Cambios con Imágenes de Alta Resolución",
    "text": "Detección de Cambios con Imágenes de Alta Resolución\n\nDASNet\nDASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images(Chen et al. 2021)\n\nResumen:\n\nLa detección de cambios es una tarea básica del procesamiento de imágenes por teledetección. El objetivo de la investigación es identificar la información de cambio de interés y filtrar la información de cambio irrelevante como factores de interferencia. Recientemente, el aumento del aprendizaje profundo ha proporcionado nuevas herramientas para la detección de cambios, que han dado resultados impresionantes. Sin embargo, los métodos disponibles se centran principalmente en la información de diferencia entre las imágenes de teledetección multitemporal y carecen de robustez ante la información de pseudocambio. Para superar la falta de resistencia de los métodos actuales a los pseudocambios, en este trabajo proponemos un nuevo método, a saber, las redes siamesas totalmente convolucionales de atención dual (DASNet), para la detección de cambios en imágenes de alta resolución. A través del mecanismo de atención dual, se capturan las dependencias de largo alcance para obtener representaciones de características más discriminantes para mejorar el rendimiento de reconocimiento del modelo. Además, la muestra desequilibrada es un problema grave en la detección de cambios, es decir, las muestras sin cambios son mucho más abundantes que las muestras con cambios, lo que constituye una de las principales razones de los pseudocambios. Proponemos la pérdida contrastiva ponderada de doble margen para abordar este problema, castigando la atención a los pares de características sin cambios y aumentando la atención a los pares de características con cambios. Los resultados experimentales de nuestro método en el conjunto de datos de detección de cambios (CDD) y en el conjunto de datos de detección de cambios en edificios (BCDD) demuestran que, en comparación con otros métodos de referencia, el método propuesto consigue mejoras máximas del 2,9% y el 4,2%, respectivamente, en la puntuación F1. Nuestra implementación de PyTorch está disponible en https://github.com/lehaifeng/DASNet.\n\nIndex Terms:\n\nChange detection, high-resolution images, dual attention, Siamese network, weighted double-margin contrastive loss.\n\n\n\n\n\n\n\nDASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite\n\n\n\n\n\n\n\n\nChen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, and Haifeng Li. 2021. “DASNet: Dual Attentive Fully Convolutional Siamese Networks for Change Detection of High Resolution Satellite Images.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14: 1194–1206. https://doi.org/10.1109/JSTARS.2020.3037893.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022. “Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167.\n\n\nWang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022. “Self-Supervised Vision Transformers for Joint SAR-optical Representation Learning.” arXiv. http://arxiv.org/abs/2204.05381.\n\n\nYan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. “Fully Transformer Network for Change Detection of Remote Sensing Images.” arXiv. http://arxiv.org/abs/2210.00757."
  },
  {
    "objectID": "Ms-CapsNet.html",
    "href": "Ms-CapsNet.html",
    "title": "Ms-CapsNet",
    "section": "",
    "text": "Los métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los retos del ruido de moteado y la sensibilidad a la deformación. Para mitigar estos problemas, propusimos una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se diseña un módulo de convolución de fusión adaptativa (AFC) para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.\nRespositorio: https://github.com/summitgao/SAR_CD_MS_CapsNet.\n\n\n\nAunque se han propuesto muchas técnicas, la detección de cambios en las imágenes SAR sigue siendo una tarea difícil. La calidad de la imagen se ve deteriorada por el ruido de moteado que dificulta la interpretación meticulosa de los datos SAR. Se han implementado muchos métodos para abordar el problema del ruido de moteado. Suelen constar de tres pasos:\n\nCoregistro de imágenes: El corregistro de imágenes es una tarea fundamental para establecer las correspondencias espaciales entre las imágenes SAR multitemporales.\nGeneración de imágenes de diferencia (DI) : La DI se genera habitualmente mediante los operadores log- ratio, Gauss-ratio [5] y neighborhood-ratio [6].\nClasificación de DI: la mayoría de las investigaciones se dedican a construir un clasificador robusto. Se trata de una tarea no trivial, ya que un clasificador potente determina directamente la precisión de la detección de cambios.\n\nMuchos investigadores se dedican a desarrollar clasificadores potentes para la detección de cambios. Li et al. [7] diseñaron un algoritmo de clustering de dos niveles para la detección de cambios sin supervisión. En [8], la información de vecindad local se incorpora a la función objetivo de clustering para mejorar el rendimiento de la detección de cambios. Gong et al. [9] desarrollaron un campo aleatorio de Markov (MRF) mejorado basado en el clustering de c-medias difusas (FCM) para suprimir el ruido de moteado. En [4], se emplearon las máquinas de Boltzmann restringidas (RBM) apiladas para la detección de cambios en las imágenes SAR. Aunque los métodos anteriores lograron un rendimiento prometedor, las capacidades de representación de características siguen siendo limitadas.\nEn los últimos años, las redes neuronales convolucionales (CNN) han mejorado mucho el rendimiento de muchas tareas visuales. Se ha demostrado que es bastante eficaz para el aprendizaje robusto de características. Los modelos basados en CNN se han aplicado con éxito en la detección de cambios en imágenes de teledetección [10]. Wang et al. [11] propusieron un marco de CNN de extremo a extremo para aprender características discriminativas de la matriz de afinidad mixta para la detección de cambios.\nMás tarde, se desarrolló el modelado de ruido profundo no supervisado para la detección de cambios en imágenes hiperespectrales [12]. Liu et al. [13] propusieron una elegante CNN local restringida (LR-CNN) para la detección de cambios polarimétricos en SAR. En [14], se aplicó el aprendizaje profundo transferido a la detección de cambios en imágenes SAR de hielo marino basado en CNN. Aunque los métodos basados en CNN han logrado un excelente rendimiento en la detección de cambios, la precisión a veces se deteriora en el caso de la transformación, como las inclinaciones y rotaciones. En concreto, la CNN es incapaz de modelar la relación posicional entre los objetos del suelo.\nMás recientemente, Sabour y Hinton propusieron la red Capsule (CapsNet) para dar solución a los problemas en los que los modelos CNN son inadecuados [15]. En CapsNet, un vector de actividad de cápsulas representa los parámetros de instanciación de la entidad, como la pose, la textura y la deformación. La existencia de entidades se expresa mediante la longitud de los parámetros de instanciación. El mecanismo de enrutamiento dinámico se utiliza para la propagación de la información. Se ha comprobado empíricamente que CapsNet es eficaz para el análisis de imágenes de teledetección [16] [17]. Hasta donde sabemos, la literatura sobre la detección de cambios en SAR basada en CapsNet es muy escasa.\n\n\n\nMs-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala\n\n\nSostenemos que la debilidad de los enfoques existentes de detección de cambios en imágenes SAR proviene principalmente de dos aspectos: Uno es que la correlación de las características de diferentes posiciones no se puede modelar de forma efectiva. El otro es el ruido intrínseco del moteado en las imágenes SAR. Para hacer frente a los problemas mencionados, se propone una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre las imágenes SAR multitemporales. La Ms-CapsNet propuesta tiene una estructura similar a la Red de Cápsulas [15] sin el operador multiescala y el módulo de Convolución de Fusión Adaptativa (AFC). La Ms-CapsNet proporciona un grupo de parámetros de instanciación para capturar características de diferentes posiciones. Para hacer frente al problema del ruido de moteado, el módulo AFC está diseñado para convertir las intensidades de los píxeles en actividades de las características locales. De este modo, las características locales se vuelven robustas al ruido. Se realizan amplios experimentos con tres conjuntos de datos reales para demostrar la superioridad de nuestro método propuesto sobre cuatro trabajos del estado del arte.\nPara mayor claridad, las principales contribuciones se resumen como sigue:\n\nLa Ms-CapsNet propuesta tiene la capacidad de extraer características robustas de diferentes posiciones. Las propiedades equivariantes se pueden conseguir mediante el módulo de cápsulas. Por lo tanto, la demanda de una gran cantidad de muestras de entrenamiento se reduce por la información correlativa y completa.\nSe diseña un módulo AFC sencillo pero eficaz, que puede convertir eficazmente las intensidades de los píxeles en actividades de características locales. El módulo AFC extrae las características semánticas superiores y enfatiza las significativas mediante una estrategia basada en la atención. Por lo tanto, las características locales de actividad se vuelven más resistentes al ruido y se aceptan inmediatamente como entrada de la cápsula primaria.\nSe han realizado amplios experimentos con tres conjuntos de datos de SAR para validar la eficacia del método propuesto. Además, hemos publicado los códigos y la configuración para facilitar futuras investigaciones en el análisis de imágenes de teledetección multitemporal."
  },
  {
    "objectID": "Ms-CapsNet.html#metodología",
    "href": "Ms-CapsNet.html#metodología",
    "title": "Ms-CapsNet",
    "section": "Metodología",
    "text": "Metodología\n\nA. Adaptive Fusion Convolution Module (AFC)\n\n\n\nIllustration of the Adaptive Fusion Convolution (AFC) module.\n\n\n\n\nB. Capsule Module"
  },
  {
    "objectID": "Ms-CapsNet.html#resultados-y-análisis-de-experimentos.",
    "href": "Ms-CapsNet.html#resultados-y-análisis-de-experimentos.",
    "title": "Ms-CapsNet",
    "section": "Resultados y Análisis de Experimentos.",
    "text": "Resultados y Análisis de Experimentos.\n\n\n\nVisualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet.\n\n\n\nA. Dataset and Evaluation Criteria\n\n\nB. Parameters Analysis of the Proposed Ms-CapsNet\n\n\nC. Change Detection Results on Three Datasets"
  },
  {
    "objectID": "Ms-CapsNet.html#conclusion",
    "href": "Ms-CapsNet.html#conclusion",
    "title": "Ms-CapsNet",
    "section": "Conclusion",
    "text": "Conclusion\n\nReferencias\n[1] D. Burnner, G. Lemonie, and L. Bruzzone, “Earthquake damage assess- ment of buildings using VHR optical and SAR imagery,” IEEE Trans. Geosci. Remote Sens., vol. 48, no. 5, pp. 2403–2420, May 2010.\n[2] S. Quan, B. Xiong, D. Xiang, L. Zhao, S. Zhang, and G. Kuang, “Eigenvalue-based urban area extraction using polarimetric SAR data,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 11, no. 2, pp. 458–471, Feb. 2018.\n[3] R. J. Radke, S. Andra, O. Al-Kofahi, and B. Roysam, “Image change detection algorithms: A systematic survey,” IEEE Trans. Image Process., vol. 14, no. 3, pp. 294–307, Mar. 2005. [4] M. Gong, J. Zhao, J. Liu, Q. Miao, and L. Jiao, “Change detection in synthetic aperture radar images based on deep neural networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 1, pp. 125–138, Jan. 2016.\n[5] B. Hou et al., “Unsupervised change detection in SAR image based on gauss-log ratio image fusion and compressed projection,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 7, no. 8, pp. 3297–3317, 2014.\n[6] M. Gong, Y. Cao, and Q. Wu, “A neighborhood-based ratio approach for change detection in SAR images,” IEEE Geosci. Remote Sens. Lett., vol. 9, no. 2, pp. 307–311, 2012.\n[7] H. Li, T. Celik, N. Longbotham, and W. J. Emery, “Gabor feature based unsupervised change detection of multitemporal SAR images based on two-level clustering,” IEEE Geosci. Remote Sens. Lett., vol. 12, no. 12, pp. 2458–2462, Dec. 2015.\n[8] L. Jia, M. Li, P. Zhang, Y. Wu, and H. Zhu, “SAR image change detection based on multiple kernel k-means clustering with local- neighborhood information,” IEEE Geosci Remote Sens. Lett., vol. 13, no. 6, pp. 856–860, Jun. 2016.\n[9] M. Gong, Z. Zhou, and J. Ma. “Change detection in synthetic aperture radar images based on image fusion and fuzzy clustering,” IEEE Trans. Image Process., vol. 21, no. 4, pp. 2141–2151, Apr. 2012.\n[10] Q.Liu,R.Hang,H.Song,andZ.Li,“Learningmultiscaledeepfeatures for high-resolution satellite image scene classification,” IEEE Tran. Geosci. Remote Sens., vol. 56, no. 1, pp. 117–126, Jan. 2018.\n[11] Q. Wang, Z. Yuan, Q. Du, and X. Li, “GETNET: a general end-to-end 2-D CNN framework for hyperspectral image change detection,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 3–13, Jan. 2019.\n[12] X. Li, Z. Yuan, and Q. Wang, “Unsupervised deep noise modeling for hyperspectral image change detection,” Remote Sens., vol. 11, no. 3, 258, Jan. 2019.\n[13] F. Liu, L. Jiao, X. Tang, S. Yang, W. Ma, and B. Hou, “Local restricted convolutional neural network for change detection in polarimetric SAR images,” IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 3, pp. 1–16, Mar. 2019.\n[14] Y. Gao, F. Gao, J. Dong, and S. Wang. “Transferred deep learning for sea ice change detection from synthetic aperture radar images,” IEEE Geosci. Remote Sens. Lett., vol. 16, no. 10, pp. 1655–1659, Oct. 2019.\n[15] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3859— 3869.\n[16] M. E. Paoletti et al., “Capsule networks for hyperspectral image classi- fication,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 4, pp. 2145– 2160, Apr. 2019.\n[17] K. Zhu et al., “Deep convolutional capsule network for hyperspectral image spectral and spectral-spatial classification,” Remote Sens., vol. 11, no. 3, pp. 1–28, Mar. 2019, Art. no. 223.\n[18] J.Hu,L.Shen,andG.Sun,“Squeeze-and-excitationnetworks,”inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018, pp. 7132–7141.\n[19] F. Gao, J. Dong, B. Li, and Q. Xu, “Automatic change detection in synthetic aperture radar images based on PCANet,” IEEE Geosci. Remote Sens. Lett., vol. 13, no. 12, pp. 1792–1796, Dec. 2016.\n[20] W. Song, S. Li, L. Fang, and T. Lu, “Hyperspectral image classification with deep feature fusion network,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 6, pp. 3173–3184, Jun. 2018.\n\n\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838."
  },
  {
    "objectID": "RUSACD.html",
    "href": "RUSACD.html",
    "title": "RUSACD",
    "section": "",
    "text": "RUSACD: Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning (Zhang et al. 2020)"
  },
  {
    "objectID": "RUSACD.html#abstract",
    "href": "RUSACD.html#abstract",
    "title": "RUSACD",
    "section": "Abstract",
    "text": "Abstract\nLa detección de cambios en áreas pequeñas a partir de un radar de apertura sintética (SAR) es una tarea muy difícil. En este trabajo, se propone un enfoque robusto no supervisado para la detección de cambios en áreas pequeñas a partir de imágenes SAR multitemporales utilizando el aprendizaje profundo. En primer lugar, se desarrolla un método de reconstrucción de superpíxeles multiescala para generar una imagen de diferencia (DI), que puede suprimir el ruido de moteado de forma efectiva y mejorar los bordes mediante la explotación de información local y espacialmente homogénea. En segundo lugar, se propone un algoritmo de clustering de c-means difuso de dos etapas para dividir los píxeles de la DI en clases cambiadas, no cambiadas e intermedias con una estrategia de clustering paralela. A continuación, se construyen parches de imagen pertenecientes a las dos primeras clases como muestras de entrenamiento de pseudoetiquetas, y los parches de imagen de la clase intermedia se tratan como muestras de prueba. Por último, se diseña y entrena una red neuronal convolucional wavelet (CWNN) para clasificar las muestras de prueba en clases modificadas o no modificadas, Se combina con una red convolucional generativa adversarial (DCGAN) para aumentar el número de clases cambiadas dentro de las muestras de entrenamiento de las pseudo-etiquetas. Los experimentos numéricos en cuatro conjuntos de datos reales de SAR demuestran la validez y la solidez del enfoque propuesto, logrando una precisión de hasta el 99,61% para la detección de cambios en áreas pequeñas.\n\nKeywords:\n\nChange detection; Synthetic aperture radar; Difference image; Fuzzy c-means algorithm; Deep learning.\n\n\nRepostorio: https://github.com/River-sh/A-robust-unsupervised-small-area-change-detection"
  },
  {
    "objectID": "RUSACD.html#diagrama-general",
    "href": "RUSACD.html#diagrama-general",
    "title": "RUSACD",
    "section": "Diagrama General",
    "text": "Diagrama General\n\n\n\nFlowchart illustrating the proposed RUSACD methodology."
  },
  {
    "objectID": "RUSACD.html#pros-y-contras",
    "href": "RUSACD.html#pros-y-contras",
    "title": "RUSACD",
    "section": "Pros y Contras",
    "text": "Pros y Contras\nPros:\n\nNo supervisado\nÁreas pequeñas\nMejor performance casos con bajo ruido (moteado)\nEvaluación de resultados con otros métodos\n\nContras:\n\nNo testeado con Sentine-1 (SAR COSMO-SkyMed entre 2016 y 2017 y SAR ERS-2 entre 2003 y 2004)\nRepositorio en matlab"
  },
  {
    "objectID": "RUSACD.html#datasets",
    "href": "RUSACD.html#datasets",
    "title": "RUSACD",
    "section": "Datasets",
    "text": "Datasets\nSe utilizaron cuatro conjuntos de datos SAR multitemporales reales (Tabla 1) para evaluar el rendimiento del enfoque propuesto. Tres de estos cuatro conjuntos de datos fueron adquiridos sobre la provincia de Guizhou en China por el sensor SAR COSMO-SkyMed en junio de 2016 y abril de 2017. Como se muestra en la Fig. 6, el primer par de imágenes (conjunto de datos A) con el mapa de referencia del suelo consiste en montañas y un río, el segundo par de imágenes (conjunto de datos B) con el mapa de referencia del suelo incluye colinas, llanuras y edificios, y el tercer par de imágenes (conjunto de datos C) con el mapa de referencia del suelo es principalmente llanuras. El último par de imágenes (conjunto de datos D) fue adquirido sobre la ciudad de San Francisco, Estados Unidos, por el sensor SAR ERS-2 en agosto de 2003 y mayo de 2004 (Fig. 7). En los conjuntos de datos A, B y C se aprecia el ruido de moteado, que supone un gran reto para la detección de cambios. A partir de los mapas de referencia redondos de la Fig. 6-7, está claro que la proporción de píxeles cambiados es extremadamente pequeña en comparación con los píxeles no cambiados. El conjunto de datos D es un punto de referencia en el que hay menos ruido y los cambios no pueden considerarse como cambios de área pequeños. Este conjunto de datos se utiliza para demostrar la solidez de nuestro enfoque propuesto.\n\n\n\nDataset A, B and C. (a) Image acquired in June 2016, (b) Image acquired in April 2017. (c) Ground\n\n\n\n\n\nDataset D. (a) Image acquired in August 2003. (b) Image acquired in May 2004. (c) Ground reference map."
  },
  {
    "objectID": "RUSACD.html#resultados",
    "href": "RUSACD.html#resultados",
    "title": "RUSACD",
    "section": "Resultados",
    "text": "Resultados\nPara demostrar la eficacia del RUSACD propuesto, se compararon cuatro métodos de referencia, entre ellos: PCA k-means (PCAK) (Celik, 2009), relación basada en la vecindad y máquina de aprendizaje extremo (NRELM) (Gao et al., 2016), extracción de características de Gabor y FCM con PCANet (GFPCANet) (Gao et al., 2016), y FCM con CWNN (FCWNN) (Gao et al., 2019). También aplicamos TCCFCM para agrupar el MSRDI en dos categorías (cambiado y sin cambios) como método de detección de cambios MTCCFCM de referencia. Los resultados experimentales se muestran en la Fig. 8, mientras que las métricas de precisión cuantitativas se indican en la Tabla 2. Además, otros métodos utilizados para la comparación en el conjunto de datos D (Tabla 3) incluyen la detección guiada por saliencia con agrupación de k-means (SGK) (Zheng et al., 2017), el autoencoder apilado y FCM con CNN (SAEFCNN) (Gong et al., 2017), la red neuronal profunda guiada por saliencia (SGDNN) (Geng et al., 2019) y la prueba de relación de verosimilitud generalizada adaptativa (AGLRT) (Zhuang et al., 2020).\nTODO: Revisar y agregar bibliografia\n\n\n\nTable 2. Comparison of the final change maps on dataset A-D. (a) PCAK. (b) NRELM. (c) GFPCANet. (d) FCWNN. (e) MTCCFCM. (f) RUSACD. (g) Ground reference map.\n\n\n\n\n\nAccuracy metrics for the different change detection methods. Best results are shown in bold.\n\n\n\n\n\nTable 3. Accuracy metrics for the different change detection methods on dataset D. Best results are shown in bold.\n\n\n\n\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter M. Atkinson. 2020. “Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning.”"
  },
  {
    "objectID": "insumos.html",
    "href": "insumos.html",
    "title": "Tratamiento de Insumos",
    "section": "",
    "text": "A continuación se explica los procesos realizados a la base recibida llamada Turberas_COT_MMA_CHILOE.kml correspondiente a un archivo espacial de polígonos que representan tu turberas (Section 1) en el Archipiélago de Chiloé, en la Región de los Lagos Chile. Correspondiente a 858 sitios definidos como turberas.\n\n\n\nEn estapa se realizó una inspección visual a los 858 sitios definidos como turberas, y mediante fotointerpretación indetificar si existen cambios en dos periodos diferentes, el tipo y fecha aproximada de cambio, para lo cual se se utilizó el software de Google Earth Pro valiendose de la funcionalidad de serie de tiempo.\nDel procedimento anterior, se concluye que son muy pocos los que sitios que sufrieron alteración especificamente 28, y no todos los cambios correspondía a actividades extractivas de las turbas, sino que también por cambio de suelo (Figure 1), construcciones, deforestación ect.\n\n\n\n\n\n\n\n(a) Antes de cambio de suelo\n\n\n\n\n\n\n\n(b) Antes de cambio de suelo\n\n\n\n\nFigure 1: Cambio por Actividad Antrópica, pero no se observa que sea una tubera\n\n\nEn algunos casos efectivamente se trataba de cambió por actividades extractivas como se observa en la siguiente Figure 2\n\n\n\n\n\n\n\n(a) Antes de alteación\n\n\n\n\n\n\n\n(b) Después de la alteación\n\n\n\n\nFigure 2: Cambio por Actividad Antrópica correspondiente a actividades extractivas de turba\n\n\nEn el siguiente mapa Figure 3 se observa los 28 sitios que sufrieron cambios por actividad antrópica.\n\n\n\n\n\n\nFigure 3: Mapa de Sitios de Cambio en Turberas de Chiloé\n\n\n\nLa siguiente tabla Table 1 se observa todos los registros de sitios que han sufrido cambios por actividad antrópica con sus respectiva información de tipología y temporalidad.\n\n\n\n\nTable 1:  Tabla de Cambios por actividades antrópica \n \n  \n    Name \n    Description \n    ID \n    Origen_ID \n    Mes \n    Year \n    Observacion \n  \n \n\n  \n    change_1 \n     \n    1 \n    149 \n    NA \n    2020 \n    Construcción \n  \n  \n    change_2 \n     \n    2 \n    197 \n    10 \n    2020 \n    Desforestación \n  \n  \n    change_3 \n     \n    3 \n    234 \n    12 \n    2021 \n    Desforestación \n  \n  \n    change_4 \n     \n    4 \n    232 \n    10 \n    2019 \n    Camino \n  \n  \n    change_5 \n     \n    5 \n    231 \n    10 \n    2019 \n    Camino \n  \n  \n    change_6 \n     \n    6 \n    249 \n    01 \n    2022 \n    Parcelación \n  \n  \n    change_7 \n     \n    7 \n    363 \n    03 \n    2021 \n    Agua \n  \n  \n    change_8 \n     \n    8 \n    372 \n    10 \n    2013 \n    Extracción \n  \n  \n    change_9 \n     \n    9 \n    372 \n    09 \n    2013 \n    Por Definir \n  \n  \n    change_10 \n     \n    10 \n    479 \n    03 \n    2021 \n    Deforestación \n  \n  \n    change_11 \n     \n    11 \n    423 \n    NA \n    NA \n    Por Definir \n  \n  \n    change_13 \n     \n    13 \n    439 \n    NA \n    NA \n    Empresa \n  \n  \n    change_14 \n     \n    14 \n    612 \n    01 \n    2017 \n    Parcelación \n  \n  \n    change_15 \n     \n    15 \n    554 \n    11 \n    2019 \n    Extracción \n  \n  \n    change_16 \n     \n    16 \n    551 \n    11 \n    2019 \n    Extracción \n  \n  \n    change_17 \n     \n    17 \n    521 \n    01 \n    2017 \n    Extracción \n  \n  \n    change_18 \n     \n    18 \n    582 \n    04 \n    2018 \n    Extracción \n  \n  \n    change_19 \n     \n    19 \n    565 \n    03 \n    2021 \n    Cambio Color \n  \n  \n    change_20 \n     \n    20 \n    595 \n    03 \n    2021 \n    Extracción \n  \n  \n    change_21 \n     \n    21 \n    680 \n    03 \n    2021 \n    Cambio Color \n  \n  \n    change_22 \n     \n    22 \n    685 \n    03 \n    2021 \n    Alta Tensión \n  \n  \n    change_23 \n     \n    23 \n    720 \n    03 \n    2021 \n    Extracción \n  \n  \n    change_24 \n     \n    24 \n    658 \n    10 \n    2011 \n    Cultivo \n  \n  \n    change_25 \n     \n    25 \n    689 \n    03 \n    2021 \n    Por Definir \n  \n  \n    change_26 \n     \n    26 \n    NA \n    03 \n    2021 \n    Extracción \n  \n  \n    change_27 \n     \n    27 \n    829 \n    08 \n    2021 \n    Construcción \n  \n  \n    change_28 \n     \n    28 \n    844 \n    01 \n    2018 \n    Extracción \n  \n\n\n\n\n\n\n\n\n\nVisualización de los cambios detectados por actividad antrópica Figure 4\n\n\n\n\n\n\nFigure 4: Mapa de Sitios de Cambio en Turberas de Chiloé por extracción"
  },
  {
    "objectID": "remote_sensing.html",
    "href": "remote_sensing.html",
    "title": "Conceptos básicos de Percepción Remota",
    "section": "",
    "text": "La teledetección es el proceso de captura, medición y análisis de imágenes y representaciones digitales de patrones energéticos derivados de dispositivos sensores sin contacto con el fin de recopilar información fidedigna sobre las cosas físicas y el medio ambiente. La tecnología de teledetección ha penetrado en todos los segmentos de los recursos naturales, ya que proporciona información clara en modo de imagen.\nTodos los objetos del universo reflejan, irradian o dispersan radiación electromagnética. La teledetección es una técnica o herramienta similar a las matemáticas. Se utilizan sensores para medir la cantidad de radiación irradiada por un objeto o área geográfica a distancia y, con la ayuda de algoritmos basados en estadísticas y matemáticas, se extrae información valiosa.\nEn la Figure 1 se muestra un modelo de interacción que describe la relación de la teledetección con las ciencias sociales, físicas y biológicas y con las matemáticas y la lógica. El proceso general de teledetección utilizado por los científicos para extraer información a partir de datos obtenidos por teledetección se desarrolla en cuatro pasos, como se muestra en la figura 2. En esta subsección, se han tratado diferentes formas de adquisición de datos de teledetección y procesos de detección de cambios."
  },
  {
    "objectID": "remote_sensing.html#remote-sensing-data-acquisition-methods",
    "href": "remote_sensing.html#remote-sensing-data-acquisition-methods",
    "title": "Conceptos básicos de Percepción Remota",
    "section": "Remote Sensing Data Acquisition Methods",
    "text": "Remote Sensing Data Acquisition Methods\nRemotely sensed data collection is the most important and expensive job in the remote sensing process. It is essential to have remotely sensed images in a digital format to apply digital image processing. There are two fundamental ways to acquire the digital image:\n\nCapture the image with a remote sensing device in an analogue format, then convert it to digital format.\nTake a digital image of the remotely sensed image, such as one obtained by the Landsat 7 Thematic Mapper sensor system.\n\nThe ability of a system to render information at the smallest discretely separable quantity in terms of space (spatial), EMR wavelength band (spectral), time (temporal), and/or radiation quantity is characterized as resolution (radiometric). Some of the major parameters that determine the nature of the collected remote sensing data are discussed in the subsections below.\n\nSpectral Information and Resolution\nThe dimension (size) and the number of specific bands or channels in the electromag- netic spectrum to which a remote sensing sensor is sensitive are spectral resolutions. The data are collected in many bands of the electromagnetic spectrum by a multispectral remote sensing sensor. The data are collected in hundreds of spectral bands by a hyperspectral remote sensing device. Energy is recorded in hundreds of bands by an ultra-spectral remote sensing system. The bands are usually chosen to maximize the contrast between the object and the background. As a result, appropriate band selection may increase the likelihood of retrieving the needed information from the remote sensor data. The greater the narrowness of the band, the greater the spectral resolution."
  },
  {
    "objectID": "remote_sensing.html#spatial-information-and-resolution",
    "href": "remote_sensing.html#spatial-information-and-resolution",
    "title": "Conceptos básicos de Percepción Remota",
    "section": "Spatial Information and Resolution",
    "text": "Spatial Information and Resolution\nThe lowest angular or linear gap between two objects that a remote sensing system can resolve is called the spatial resolution. Using the spatial information in the image, the amount of spectral autocorrelation may be estimated. A pixel is a small spot on the earth’s surface that a sensor can observe as being distinct from its surroundings. It is a detector element or a slit whenever projected onto the ground. In other words, the ground segment sensed at any one time is the scanner’s spatial resolution. It has sometimes been referred to as the ground resolution element (GRE). The spatial resolution at which data is obtained affects the ability to distinguish diverse features and measure their extent. According to the usual rule, the spatial resolution should be less than half the size of the smallest object of interest.\n\nTemporal Information and Resolution\nThe temporal resolution refers to how often a sensor records an image of a specific area. For many applications, a high temporal resolution is critical. Temporal resolution refers to the satellite’s capacity to extract the same area from the same viewing angle at different times. A sensor’s temporal resolution is determined by several elements, including the satellite/sensor capabilities, swath overlap, and latitude.\n\n\nRadiometric Information and Resolution\nThe sensitivity of detectors to minor variations in electromagnetic energy is known as radiometric resolution. High radiometric resolution improves the likelihood of more accurate remote sensing of phenomena. A sensor measurement allows it to distinguish the tiniest variation in spectral reflectance/remittance between different targets. The number of quantization levels and the saturation radiance determine the radiometric resolution. The more tiers there are, the more detailed the information that becomes digitalized.\n\n\nPolarization Information\nThe polarization characteristics of electromagnetic radiation collected by remote sens- ing systems can be utilized to investigate the earth’s resources. In general, the stronger the polarization, the smoother the object’s surface.\n\n\nAngular Information\nThe angle of incidence has traditionally been related to the incoming energy that illuminates the landscape and the angle of exitance from the terrain to the sensor system. The bidirectional nature of the remote sensing data collection influences the spectrum and polarization of the sensor light collected by the remote sensing system [3,4]."
  },
  {
    "objectID": "remote_sensing.html#remote-sensing-data-acquisition",
    "href": "remote_sensing.html#remote-sensing-data-acquisition",
    "title": "Conceptos básicos de Percepción Remota",
    "section": "Remote Sensing Data Acquisition",
    "text": "Remote Sensing Data Acquisition\nThe two important methods of remote sensing data acquisition are aerial photography and satellite image data collection. Aerial photographs are the snapshots of the earth taken by calibrated cameras at a particular instant of time in an analog format. Through the process of digitization, this analog format is converted into a digital format. Aerial photography can be taken from space, from high- or low-altitude airplanes, or from platforms close to the ground. Each aerial shot has vital information for the user in the margin.\n\nVertical Photographs: The photographic device, i.e., the camera, is pinned as straight down as possible when taking a vertical shot. The allowed tolerance from the plumb (perpendicular) line to the camera axis is normally +3 degrees. The lens axis is nearly perpendicular to the earth’s surface, covers a limited area on a single vertical shot, and closely resembles a square or a rectangle [4].\nHigh oblique: The camera is tilted roughly 60 degrees from the vertical when taking a high oblique shot. It is mostly employed in the creation of aeronautical charts and has a minor military application. The ground area covered is trapezoid in high oblique images; the horizon is always visible in high oblique photographs.\nLow Oblique: This picture is taken with the camera at a 30-degree angle to the vertical. It has been used to research an area before an attack, as a replacement for a map, or as a supplement to a map. The ground area covered is a trapezoid in a low oblique area and coverage is quite limited [5].\nTrimetrogon: It is a composite of three photos shot simultaneously, one vertical and two high obliques, in a direction perpendicular to the flight path. The oblique images, taken at a 60-degree angle from the vertical, side lap the vertical photography, resulting in composites that run from horizon to horizon."
  },
  {
    "objectID": "remote_sensing.html#remote-sensing-sensors",
    "href": "remote_sensing.html#remote-sensing-sensors",
    "title": "Conceptos básicos de Percepción Remota",
    "section": "Remote Sensing Sensors",
    "text": "Remote Sensing Sensors\ncontinuar …\nConcept Paper Change Detection in Remote Sensing Image Data Comparing Algebraic and Machine Learning Methods"
  },
  {
    "objectID": "sar_sentinel_s1.html",
    "href": "sar_sentinel_s1.html",
    "title": "Sentinel 1",
    "section": "",
    "text": "Sentinel-1 vista general"
  },
  {
    "objectID": "sar_sentinel_s1.html#resumen",
    "href": "sar_sentinel_s1.html#resumen",
    "title": "Sentinel 1",
    "section": "Resumen",
    "text": "Resumen\nLa misión Sentinel-1 es el Observatorio Radar Europeo de la iniciativa conjunta Copernicus de la Comisión Europea (CE) y la Agencia Espacial Europea (ESA). Copernicus es una iniciativa europea para la puesta en marcha de servicios de información relacionados con el medio ambiente y la seguridad. Se basa en los datos de observación recibidos de los satélites de observación de la Tierra y la información terrestre.\nLa misión Sentinel-1 incluye imágenes en banda C que operan en cuatro modos de imagen exclusivos con diferente resolución (hasta 5 m) y cobertura (hasta 400 km). Ofrece capacidad de doble polarización, tiempos de revisita muy cortos y una rápida entrega de productos. Para cada observación, se dispone de mediciones precisas de la posición y la actitud de la nave espacial.\nEl radar de apertura sintética (SAR) tiene la ventaja de operar en longitudes de onda que no se ven obstaculizadas por la nubosidad o la falta de iluminación, y puede adquirir datos sobre un lugar durante el día o la noche en todas las condiciones meteorológicas. Sentinel-1, con su instrumento C-SAR, puede ofrecer una vigilancia fiable y repetida de una zona amplia.\nLa misión está compuesta por una constelación de dos satélites, Sentinel-1A y Sentinel-1B, que comparten el mismo plano orbital.\nSentinel-1 está diseñado para trabajar en un modo de operación preprogramado y libre de conflictos, obteniendo imágenes de todas las masas terrestres mundiales, zonas costeras y rutas marítimas en alta resolución y cubriendo el océano mundial con viñetas. Esto garantiza la fiabilidad del servicio requerida por los servicios operativos y un archivo de datos consistente a largo plazo construido para aplicaciones basadas en series temporales largas."
  },
  {
    "objectID": "sar_sentinel_s1.html#objetivos-de-la-misión",
    "href": "sar_sentinel_s1.html#objetivos-de-la-misión",
    "title": "Sentinel 1",
    "section": "Objetivos de la misión",
    "text": "Objetivos de la misión\nLa misión proporciona una capacidad operativa independiente para la cartografía radar continua de la Tierra.\nLa misión Sentinel-1 está diseñada para proporcionar una mayor frecuencia de revisita, cobertura, puntualidad y fiabilidad para los servicios operativos y las aplicaciones que requieren series temporales largas.\nLa misión proporcionará una capacidad de interferometría operativa gracias a los estrictos requisitos impuestos a la precisión de la actitud, al conocimiento de la actitud y de la órbita, y a la precisión de los tiempos de toma de datos.\nLa constelación cubrirá la totalidad de las masas terrestres del mundo con periodicidad quincenal, las zonas de hielo marino, las zonas costeras de Europa y las rutas marítimas con periodicidad diaria y el océano abierto de forma continua mediante imágenes de las olas.\nEl instrumento SAR del Sentinel-1 y su corto tiempo de revisita harán avanzar enormemente las capacidades de los usuarios y proporcionarán datos de forma rutinaria y sistemática para la vigilancia marítima y terrestre, la respuesta a emergencias, el cambio climático y la seguridad.\nSe espera que cada satélite Sentinel-1 transmita datos de observación de la Tierra durante al menos 7 años y tenga combustible a bordo para 12 años.\nEl Documento de Requisitos de la Misión (MRD) del Sentinel-1 describe en detalle todos los requisitos específicos de la misión."
  },
  {
    "objectID": "sar_sentinel_s1.html#orbita",
    "href": "sar_sentinel_s1.html#orbita",
    "title": "Sentinel 1",
    "section": "Orbita",
    "text": "Orbita\nSentinel-1 se encuentra en una órbita casi polar, sincrónica al sol, con un ciclo de repetición de 12 días y 175 órbitas por ciclo para un solo satélite. Tanto Sentinel-1A como Sentinel-1B comparten el mismo plano orbital con una diferencia de fase orbital de 180°. Con ambos satélites en funcionamiento, el ciclo de repetición es de seis días.\n\n\n\nRevisit time for S-1A and S-1B in Days per Revisit\n\n\nEn particular, para la interferometría, Sentinel-1 requiere un estricto control de la órbita. El posicionamiento del satélite a lo largo de la órbita debe ser preciso, con apuntamiento y sincronización entre pares interferométricos. El control del posicionamiento de la órbita de Sentinel-1 se define mediante un “tubo” orbital fijo en la Tierra, de 50 m (RMS) de radio, alrededor de una trayectoria operativa nominal. El satélite se mantiene dentro de este “tubo” durante la mayor parte de su vida operativa.\n\n\n\nTubo orbital para la interferometría"
  },
  {
    "objectID": "sar_sentinel_s1.html#escenario-de-producción",
    "href": "sar_sentinel_s1.html#escenario-de-producción",
    "title": "Sentinel 1",
    "section": "Escenario de producción",
    "text": "Escenario de producción\nLas operaciones del segmento terrestre del Sentinel-1 implementan un escenario de producción de la misión predefinido. Este escenario prevé el procesamiento sistemático y la difusión en línea de todos los datos del Sentinel-1 adquiridos en los modos IW, EW y SM en productos GRD de Nivel 0 y Nivel 1.\nEsto se complementa con el procesamiento sistemático y la difusión en línea de:\n\nProductos SLC de Nivel-1 para todos los datos adquiridos en Modo Onda (activos desde octubre de 2016 para Sentinel-1B y desde mayo de 2017 para Sentinel-1A).\nProductos SLC de Nivel-1 para todos los datos adquiridos en modo IW sobre áreas regionales específicas, que ha evolucionado durante las operaciones para cubrir todos los datos adquiridos en modo IW\nProductos oceánicos de nivel 2 para todos los datos adquiridos en modo Ola (activos desde julio de 2015 para Sentinel-1A y desde octubre de 2016 para Sentinel-1B)\nProductos oceánicos de nivel 2 para todos los datos adquiridos en modo IW y EW sobre zonas regionales específicas\n\nLas zonas geográficas en las que se generan sistemáticamente los productos SLC de nivel 1 de Sentinel-1A han evolucionado gradualmente desde la apertura del acceso a los datos en línea el 3 de octubre de 2014 y se han ido incrementando paulatinamente en los meses siguientes con el objetivo de que los productos SLC estén disponibles para todos los datos adquiridos en modo IW.\nLa evolución de las zonas en las que se han generado sistemáticamente productos SLC de nivel 1 de Sentinel-1A y se han puesto a disposición para la descarga de datos en línea desde la apertura del acceso a los datos se resume en una serie de mapas etiquetados en el tiempo (descargar la evolución de los mapas SLC).\nCada mapa está asociado a la fecha a partir de la cual los datos IW adquiridos sobre las nuevas áreas enumeradas en el mapa han sido sistemáticamente procesados y puestos a disposición como productos SLC de nivel 1. Cada vez que un área se añadía al escenario de procesamiento sistemático del SLC, seguía formando parte de este escenario en el futuro.\nA continuación se ofrece un resumen de la producción sistemática de IW de Sentinel-1 y del acceso a los datos en línea:\nPara Sentinel-1A:\n\nDesde el 28.07.2015: Todos los datos de IW sobre masas de hielo y tierra están disponibles sistemáticamente como SLC\nDesde el 14.04.2016: Todos los datos de IW están disponibles sistemáticamente como SLC\n\nPara Sentinel-1B:\n\nDesde el 26.09.2016 (apertura del acceso a los datos S1B): Todos los datos de IW están disponibles sistemáticamente como SLC\n\nAdemás, todos los datos de Sentinel-1A adquiridos antes del 28 de julio de 2015 y no procesados originalmente a SLC (es decir, los datos adquiridos fuera de las áreas regionales predefinidas de SLC) se han procesado hacia atrás durante 2016 y se han puesto a disposición para el acceso a los datos en línea. Como resultado, los productos SLC de nivel 1 están disponibles para todos los datos de Sentinel-1 adquiridos en modo IW.\nLas áreas geográficas en las que los datos de Sentinel-1 IW y EW se procesan sistemáticamente a productos OCN de nivel 2 y se ponen a disposición para el acceso a los datos en línea también han evolucionado con el tiempo. Esta evolución se resume en una serie de mapas etiquetados en el tiempo (descargar la evolución de los mapas IW/EW OCN).\nEl 100% de los datos IW y SM adquiridos sobre masas terrestres en todo el mundo se elaboran sistemáticamente a productos OCN de nivel 1 y se ponen a disposición."
  },
  {
    "objectID": "sar_sentinel_s1.html#cobertura-geográfica",
    "href": "sar_sentinel_s1.html#cobertura-geográfica",
    "title": "Sentinel 1",
    "section": "Cobertura geográfica",
    "text": "Cobertura geográfica\nUn solo satélite Sentinel-1 podrá cartografiar el mundo entero una vez cada 12 días. La constelación de dos satélites ofrece un ciclo de repetición exacta de 6 días. La constelación tendrá una frecuencia de repetición (ascendente/descendente) de 3 días en el ecuador, menos de 1 día en el Ártico y se espera que proporcione cobertura sobre Europa, Canadá y las principales rutas marítimas en 1-3 días, independientemente de las condiciones meteorológicas. Los datos del radar se entregarán a los servicios de Copernicus una hora después de su adquisición.\n\n\n\nCobertura Geogràfica"
  },
  {
    "objectID": "sar_sentinel_s1.html#intrumentos-a-bordo",
    "href": "sar_sentinel_s1.html#intrumentos-a-bordo",
    "title": "Sentinel 1",
    "section": "Intrumentos a bordo",
    "text": "Intrumentos a bordo\nSentinel-1 lleva un único instrumento de radar de apertura sintética en banda C que opera a una frecuencia central de 5,405 GHz. Incluye una antena activa phased array de orientación derecha que proporciona un rápido escaneo en elevación y azimut, una capacidad de almacenamiento de datos de 1 410 Gb y una capacidad de enlace descendente en banda X de 520 Mbit/s.\nEl instrumento C-SAR soporta el funcionamiento en polarización dual (HH+HV, VV+VH) implementado a través de una cadena de transmisión (conmutable a H o V) y dos cadenas de recepción paralelas para la polarización H y V. Los datos de doble polarización son útiles para la clasificación de la cubierta terrestre y las aplicaciones del hielo marino.\nSentinel-1 funciona en cuatro modos de adquisición exclusivos:\n\nStripmap (SM)\nInterferométrico de banda ancha (IW)\nEspectro extra ancho (EW)\nModo de ondas (WV).\n\n\n\n\nModos de adquisión de imágenes de Sentinel-1\n\n\nLos principales modos sin conflicto son IW sobre tierra y WV sobre mar abierto.\n\nModo Stripmap (SM)\nEl modo de imagen Stripmap se proporciona para la continuidad con las misiones ERS y Envisat. El modo Stripmap proporciona una cobertura con una resolución de 5 m por 5 m sobre una estrecha franja de 80 km. Se puede seleccionar una de las seis franjas de imágenes cambiando el ángulo de incidencia del haz y el ancho del haz de elevación.\n\n\nModo de hilera ancha interferométrica (IW)\nEl modo interferométrico de franja ancha (IW) permite combinar una gran anchura de franja (250 km) con una resolución geométrica moderada (5 m por 20 m). El modo IW toma imágenes de tres sub-bandas utilizando la Observación del Terreno con Escáneres Progresivos SAR (TOPSAR). Con la técnica TOPSAR, además de dirigir el haz en el rango como en SCANSAR, el haz también se dirige electrónicamente de atrás hacia adelante en la dirección acimut para cada ráfaga, evitando el festoneado y dando como resultado una imagen de mayor calidad. La interferometría está garantizada por un solapamiento suficiente del espectro Doppler (en el dominio acimutino) y del espectro del número de onda (en el dominio de la elevación). La técnica TOPSAR garantiza una calidad de imagen homogénea en toda la franja.\nEl modo IW es el modo de adquisición por defecto sobre tierra.\n\n\nModo de barrido extra ancho (EW)\nEl modo de imagen de franja extra ancha está destinado a los servicios operativos marítimos, de hielo y de zonas polares en los que se requiere una amplia cobertura y tiempos de revisita cortos. El modo EW funciona de forma similar al modo IW, empleando una técnica TOPSAR que utiliza cinco sub-surcos en lugar de tres, lo que resulta en una resolución menor (20 m por 40 m). El modo EW también se puede utilizar para la interferometría como en el modo IW.\n\n\nModo Onda (WV)\nEl modo Wave del Sentinel-1, junto con los modelos globales de olas oceánicas, puede ayudar a determinar la dirección, la longitud de onda y las alturas de las olas en los océanos abiertos.\nLas adquisiciones en el modo de olas se componen de imágenes de mapa de franjas de 20 km por 20 km, adquiridas alternativamente en dos ángulos de incidencia diferentes. Las imágenes de olas se adquieren cada 100 km, con imágenes en el mismo ángulo de incidencia separadas por 200 km."
  },
  {
    "objectID": "sar_sentinel_s1.html#información-general",
    "href": "sar_sentinel_s1.html#información-general",
    "title": "Sentinel 1",
    "section": "Información General",
    "text": "Información General"
  },
  {
    "objectID": "sar_sentinel_s1.html#referencias",
    "href": "sar_sentinel_s1.html#referencias",
    "title": "Sentinel 1",
    "section": "Referencias",
    "text": "Referencias\nhttps://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-1-sar\nhttps://sentinels.copernicus.eu/web/sentinel/missions/sentinel-1/instrument-payload\nhttps://medium.com/@HarelDan/x-marks-the-spot-579cdb1f534b"
  },
  {
    "objectID": "ecosistemas.html",
    "href": "ecosistemas.html",
    "title": "Ecosistemas Protegidos",
    "section": "",
    "text": "Las turberas solo cubren el 3 % de la superficie terrestre del planeta pero almacenan más carbono que todos los bosques de la Tierra – si se mantienen humedas.\n\nDefinición:\n\nEl término turba debe ser entendido como un sedimento natural de tipo fitógeno, poroso, no consolidado, constituido por materia orgánica parcialmente descompuesta, acumulada en un ambiente saturado de agua. De esta forma, se puede entender al concepto de turbera como un depósito de turba con un espesor de, al menos, 30 cm (Hauser 1996)\n\nFormación:\n\nSegún (Hauser 1996), el origen de las turberas se encuentra en las eras glaciares del Pleistoceno, cuando grandes extensiones de casquetes glaciares cubrieron el valle central de la Región de Los Lagos, incluyendo a la Isla Grande de Chiloé. El posterior retiro de los glaciares dejó masas de agua tierra adentro, formando los grandes lagos y lagunas glaciares que en la actualidad componen el paisaje de la región.\nEn el caso de Chiloé, zona en la que se establecieron las condiciones climáticas ideales para el desarrollo del musgo del género Sphagnum, lo que permitió la acumulación de materia orgánica en depresiones del relieve de la isla con alto contenido de humedad Figure 1 (a). Este proceso de acumulación del musgo se consolidó en la formación de extensas turberas Figure 1 (b) y Figure 1 (c) . [Hauser (1996)]\n\n\n\n\n\nProceso de formación de turberas de origen glaciar (caso de Chiloé). Fuente: (Schofield W.B 1985)\n\n\n\nBotánicamente (Chiloé):\n\nBotánicamente, el pompón pertenece al Reino de las Plantas, a la División Bryophyta, a la Clase Musci y a la Familia de las Sphagnaceas. Esta familia comprende sólo un género, Sphagnum, compuesto por más de 300 especies descritas. En el archipiélago de Chiloé conviven varias especies de este género. La más abundante es S. magellanicum, que se caracteriza por su color rojo, talla relativamente robusta y hojas con ápice obtuso. Suele cubrir grandes superficies con mal drenaje en terrenos abiertos o cubriendo el suelo de los tepuales (bosques formados por la mirtácea Tepualia stipularis), donde se desarrolla con extraordinario vigor. Existen además, al menos 4 especies que se han identificado en la zona norte de la Isla: S. fimbriatum, S. falcatulum, S. recurvum y S. cuspidatum var. cuspidatum. Adicionalmente, la literatura cita otras 2 especies para la Isla: S. acutifolium y S. subnitens.Todas estas especies son de difícil identificación, siendo su morfología celular y la anatomía foliar la base de su clasificación (Zegers et al. 2006)\n\nCaracterización:\n\nUna de las características relevantes de las turberas de Sphagnum es que presenta una matriz continua superficial de musgos sobre una capa de turba que puede alcanzar varios metros de profundidad (Díaz et al. 2008). Según el mismo autor, entre otras características relevantes de este tipo de turberas se encuentran:\n\nLa turba que la compone es de origen vegetal y se encuentra en distintos estados de descomposición anaeróbica. Figure 1\nEl estrato superficial es biológicamente activo, conformado por asociaciones de especies, entre las que predominan plantas con gran capacidad para retener humedad Figure 1 (b).\nEl musgo Sphagnum forma un ambiente pobre en nutrientes (baja concentración de nitrógeno), ácido, anóxico y frío, lo que previene la presencia de hongos y bacterias que descomponen al material muerto Figure 1 (c).\nTiene una gran capacidad de absorción de agua (hasta 20 veces su peso seco en agua) Figure 1 (e)\nSu fuente de agua proviene de ríos y/o de la lluvia Figure 1 (d).\nEs un ecosistema de humedal con flora y fauna única y especializada.\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\nFigure 1: Fuente: (Treimun 2017)\n\n\n\n\nExtracción:\n\nComo Problema …\n\n\nTO-DO:\n\nDescripción de prdocesod e extracción y conexión con problema medio ambiental\nDescripción Completa Bosque Esclerofilo (? ROI)\nDescripción Completa Humedales Urbanos\n\nAgregar conceptos papers:\n\ncabezas_evaluation_2015 Evaluation of impacts of management in an anthropogenic peatland using field and remote sensing data (Cabezas et al. 2015)\nLopatin et al. - 2019 - Using aboveground vegetation attributes as proxies.pdf (Lopatin et al. 2019)\nLopatin et al. - 2022 - Disturbance alters relationships between soil carb.pdf (Lopatin et al. 2022)\n\n\n\n\n\nCabezas, Juli’an, Mauricio Galleguillos, Ariel Vald’es, Juan P. Fuentes, Cecilia P’erez, and Jorge F. Perez-Quezada. 2015. “Evaluation of Impacts of Management in an Anthropogenic Peatland Using Field and Remote Sensing Data.” Ecosphere 6 (12): 1–24. https://doi.org/10.1890/ES15-00232.1.\n\n\nDíaz, María F., Juan Larraín, Gabriela Zegers, and Carolina Tapia. 2008. “Floristic and Hydrological Characterization of Chiloé Island Peatlands, Chile.” Revista Chilena de Historia Natural 81 (4): 455–68. https://doi.org/10.4067/S0716-078X2008000400002.\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus perspectivas de utilización.” Revista Geológica de Chile 23 (2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208.\n\n\nLopatin, Javier, Roc’ıo ArayaNANAL’opez, Mauricio Galleguillos, and Jorge F. PereNAzNAQueza. 2022. “Disturbance Alters Relationships Between Soil Carbon Pools and Aboveground Vegetation Attributes in an Anthropogenic Peatland in Patagonia.” Ecology and Evolution 12 (3). https://doi.org/10.1002/ece3.8694.\n\n\nLopatin, Javier, Teja Kattenborn, Mauricio Galleguillos, Jorge F. Perez-Quezada, and Sebastian Schmidtlein. 2019. “Using Aboveground Vegetation Attributes as Proxies for Mapping Peatland Belowground Carbon Stocks.” Remote Sensing of Environment 231 (September): 111217. https://doi.org/10.1016/j.rse.2019.111217.\n\n\nSchofield W.B. 1985. “Introduction to Bryology.”\n\n\nTreimun, John. 2017. “Turberas de Chiloé, Ministerio del Medio Ambiente, Chile.”\n\n\nZegers, Gabriela, Juan Larraín, María Francisca Díaz, and Juan Armesto. 2006. “Impacto Ecológico y Social de La Explotación de Pomponales y Turberas de Sphagnum En La Isla Grande de Chiloé.” http://biblioteca.cehum.org/handle/CEHUM2018/1389."
  },
  {
    "objectID": "data-cube.html",
    "href": "data-cube.html",
    "title": "Data Cube",
    "section": "",
    "text": "Repositorio contiene una serie de notebooks para funcionar con el DataCube Chile. Está dividido en niveles de dificultad y por ahora, sólo está disponible el entrenamiento básico, que sienta los primeros lineamientos para comenzar a trabajar en el cubo de datos.\nhttps://github.com/Data-Observatory/DataCubeTrainingBasic"
  },
  {
    "objectID": "recursos.html",
    "href": "recursos.html",
    "title": "Recursos",
    "section": "",
    "text": "Geospatial deep learning with TorchGeo\n\nTorchGeo es una biblioteca de dominio de PyTorch que proporciona conjuntos de datos, muestreadores, transformaciones y modelos pre-entrenados específicos para datos geoespaciales.\nLink:\n\nPágina Oficial\nGithub Proyecto"
  },
  {
    "objectID": "recursos.html#raster-vision",
    "href": "recursos.html#raster-vision",
    "title": "Recursos",
    "section": "Raster Vision",
    "text": "Raster Vision\nGithub"
  },
  {
    "objectID": "recursos.html#links-por-explorar",
    "href": "recursos.html#links-por-explorar",
    "title": "Recursos",
    "section": "Links Por explorar",
    "text": "Links Por explorar\nhttps://dymaxionlabs-eng.medium.com/how-to-create-a-land-cover-model-for-south-america-in-4-steps-67e57d3dae64\n\nhttps://courses.spatialthoughts.com/end-to-end-gee.html#module-4-change-detection\nhttps://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1\nhttps://www.youtube.com/results?search_query=change+detection+gee\narquiologíahttps://code.earthengine.google.com/?scriptPath=users%2Fdenisberroeta%2FGEE_CIT_dbg%3Acc\nhttps://www.youtube.com/watch?v=wDBcTOTAwOc\nhttps://www.youtube.com/watch?v=5oONMB0UPWc\nhttps://developers.google.cn/earth-engine/tutorials/tutorial_forest_01\nhttps://appliedsciences.nasa.gov/join-mission/training/english/arset-using-google-earth-engine-land-monitoring-applications\nhttps://appliedsciences.nasa.gov/sites/default/files/2021-06/GEE_Land_Part3.pdf\nhttps://www.youtube.com/watch?v=KyjNhAvQS2s\nhttps://paperswithcode.com/task/change-detection-for-remote-sensing-images\n\nhttps://github.com/likyoo/Siam-NestedUNet\ncontaminación\n\nhttps://appliedsciences.nasa.gov/join-mission/training/english/arset-high-resolution-no2-monitoring-space-tropomi\n\nPrograma de la NASA https://appliedsciences.nasa.gov/join-mission/training/english/arset-using-google-earth-engine-land-monitoring-applications\n\nbigearth net\nsentinel 1 Change detection no supervisado S1 si existe cambio algorimo global, unet - rmask\ntorch geo"
  }
]