[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CNN Change Detection",
    "section": "",
    "text": "Preface\nEste documento tiene como objetivo principal de ir registrando los avances en el proceso de desarrollo de la Tesis: “Sistema de Detección de cambios en imágenes satelitales tipo Radar en ambientes naturales protegidos en Chile utilizando redes neuronales convolucionales.”, para optar al Master of Science in Data Science 2022-2023 de la Universidad Adolfo Ibáñez."
  },
  {
    "objectID": "abstract.html",
    "href": "abstract.html",
    "title": "1  Abstract",
    "section": "",
    "text": "El monitoreo ambiental utilizando imágenes satelitales de radar de apertura sintética (SAR) es una técnica flexible para obtener información sobre la superficie terrestre en cualquier condición atmosférica. Sin embargo, el uso de este tipo de productos satelitales presenta desafíos como el ruido moteado y la sensibilidad a la deformación espacial.\nPara abordar estos problemas y detectar cambios en ecosistemas naturales protegidos, se propone utilizar redes neuronales convolucionales para transformar las imágenes en un nuevo espacio de características que facilite la selección de áreas candidatas a haber sufrido cambios debidos a la intervención humana en una estructura de clasificación no supervisada.\nEstos métodos se compararán con técnicas convencionales de filtrado y reducción de dimensionalidad, utilizando métricas de precisión, rendimiento y escalabilidad en diferentes condiciones climáticas."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introducción",
    "section": "",
    "text": "Uno de los grandes problemas de actualmente en el mundo es la degradación de sistemas naturales, como los bosques y matorrales mediterráneos y humedales, los que en muchos casos contienen especies vegetales de especial interés por su endemismo, y alto nivel de amenaza y presión antrópica (Araya-López et al. 2018), (Fassnacht et al. 2021), las que pueden causar cambios en el hábitat, los cuales pueden llevar a la extinción de especies, así como a la reducción de la biodiversidad. Por lo anterior se hace necesario monitorear las áreas naturales protegidas en el país.\nEn Chile, la Superintendencia del Medio Ambiente (SMA) es el principal órgano estatal encargado de velar por el cumplimiento de la normativa ambiental. En sus 8 años de vida ha tenido el gran desafío de fiscalizar más de 17.887 unidades a lo largo del país que tienen al menos una normativa ambiental que los regula. Sin embargo, la institución no cuenta con la infraestructura o los recursos necesarios para fiscalizar adecuadamente todas estas unidades. Para que el rol fiscalizador de la SMA sea eficaz y continuo en el tiempo, se requiere de una infraestructura de monitoreo de gran escala a nivel país.\nEntonces, se hace necesario contar con insumos satelitales de fuentes abiertas y cuyo análisis se automatizado. \nLa creciente disponibilidad de productos satelitales y los avances tecnológicos han fomentado una nueva era de observación de la tierra y monitoreo de los recursos naturales por parte de los gobiernos, creando nuevas oportunidades y desafíos que deben ser abordados. Debido a esta motivación, surge la necesidad de crear un sistema monitoreo satelital, que permita identificar cambios en la estructura de la vegetación en humedales urbanos, turberas de Chiloé y el bosque y matorral esclerófilo de la región Metropolitana para ayudar a su preservación. \nLa utilización de metodologías de análisis de imágenes satelitales para detectar cambios en ecosistemas naturales se ha hecho más común debido al acceso de fuentes abiertas de productos satelitales, y la capacidad de procesamiento computacional. La aplicabilidad de productos satelitales en detección de cambios en el tiempo, destaca el satélite Sentinel-1 de tipo SAR, tiene la ventaja de operar en longitudes de onda que no se ven obstaculizadas por la nubosidad, y puede adquirir datos sobre un lugar durante el día o la noche en todas las condiciones meteorológicas (Wang et al. 2022). Lo anterior lo hace idóneo para la detección de cambios en el territorio, como lo son ecosistemas de turberas, bosques esclerófilos y humedales urbanos, los cuales se ubican en diferentes zonas del país con diversas condiciones climáticas y topográficas. \nAl trabajar con imágenes SAR presentan dos tipos de dificultades para las tarea de detección de cambios, que son el ruido de moteado (speckle noise) y la deformación en los extremos, lo que se puede solucionar con un post procesamiento (Gao et al. 2021) o en su defecto evaluar diferentes técnicas que permitan reducir el efecto de estas dificultados. \nLos investigadores han dedicado grandes esfuerzos a proponer métodos robustos de detección de cambios. Estos métodos pueden clasificarse a grandes rasgos en dos corrientes principales: 1) métodos supervisados y 2) métodos no supervisados. Un método supervisado requiere un conocimiento previo sobre los tipos de cobertura del suelo o un gran número de muestras etiquetadas de alta calidad (Volpi et al. 2013, wang2022). Para el caso del proyecto SAMSARA y en específico en la primera etapa se necesita que sea un método , ya que el método constantemente necesita estar capturando información satelital y comparándola con registros históricos, proceso que debe ser eficiente computacionalmente y sin supervisión experta. \nPara cumplir con lo anterior, en esta tesis se propone la utilización de una red neuronal convolucional del tipo no supervisada, pueda obtener información que permita información relevante y útil de la imágenes satelitales, que facilite el proceso de detección diferencias, a una escala territorial variable de acuerdo a las carctersticas territoriales de los ecosistemas a monitorear. \n\n\n\n\nAraya-López, Rocío A., Javier Lopatin, Fabian E. Fassnacht, and H. Jaime Hernández. 2018. “Monitoring Andean High Altitude Wetlands in Central Chile with Seasonal Optical Data: A Comparison Between Worldview-2 and Sentinel-2 Imagery.” ISPRS Journal of Photogrammetry and Remote Sensing, SI: Latin America Issue, 145 (November): 213–24. https://doi.org/10.1016/j.isprsjprs.2018.04.001.\n\n\nFassnacht, Fabian Ewald, Javiera Poblete-Olivares, Lucas Rivero, Javier Lopatin, Andrés Ceballos-Comisso, and Mauricio Galleguillos. 2021. “Using Sentinel-2 and Canopy Height Models to Derive a Landscape-Level Biomass Map Covering Multiple Vegetation Types.” International Journal of Applied Earth Observation and Geoinformation 94 (February): 102236. https://doi.org/10.1016/j.jag.2020.102236.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nVolpi, Michele, Devis Tuia, Francesca Bovolo, Mikhail Kanevski, and Lorenzo Bruzzone. 2013. “Supervised Change Detection in VHR Images Using Contextual Information and Support Vector Machines.” International Journal of Applied Earth Observation and Geoinformation, Earth Observation and Geoinformation for Environmental Monitoring, 20 (February): 77–85. https://doi.org/10.1016/j.jag.2011.10.013.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022. “Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167."
  },
  {
    "objectID": "problema.html",
    "href": "problema.html",
    "title": "3  Problema u Oportunidad",
    "section": "",
    "text": "La degradación de los sistemas naturales ha sido catalogada como uno de los grandes problemas de la humanidad (Steffen et al. 2015). Los estados alrededor del mundo han sido incapaces de detener estas tendencias presentando constantes de pérdida de bosque nativo, intervención de humedales y un aumento en la contaminación de aire, agua y vegetación. Por lo tanto, es indispensable monitorear de manera continua los recursos territoriales en los países, para así mejorar su gestión y cuidado.\nEl bosque y matorral esclerófilo de Chile es uno de los hotspot mundiales de biodiversidad debido a su gran cantidad de especies endémicas (Myers et al. 2000). Estos ambientes han sido tradicionalmente amenazados y su cobertura ha sido reemplazada por otros usos productivos, como la industria forestal o la producción agrícola (Miranda et al. 2017).\nMonitorear los humedales es desafiante ya que son sistemas en constante cambio debido al efecto de inundaciones y sus procesos internos (Gallant 2015). Esto hace que analizar series de datos temporales sea complejo, ya que cambios en la señal satelital puede ser debido a estos factores y no a una perturbación que se necesite fiscalizar. \nEntre los desafíos más relevantes se encuentran para las instituciones del estado para una eficiente fiscalización y protección de ecosistemas protegidos se encuentran : a) la limitada capacidad de la institución para desarrollar proyectos de este tipo, con alrededor de dos aplicaciones por año (número por debajo a la demanda existente); y b) el procesamiento de las imágenes satelitales sigue siendo costoso. Esto último se debe a que incluso cuando se ha privilegiado el uso de datos libres y herramientas de procesamiento en la nube, con un nivel de automatización que aún no alcanza la madurez. Por lo anterior, se hace necesario diseñar y crear sistemas de monitoreo robustos metodológicamente y eficiente, con capacidad de escalamiento.\nEl análisis de imágenes satelitales permite obtener datos espaciales de manera masiva y remota a lo largo de todo el país y permiten generar alertas de cambios ambientales, bajo una arquitectura de clasificación no supervisada, utilizando principalmente datos de SAR, e.g. Sentinel-1, ya que permite identificar el grado de humedad del humedal y a evitar problemas con la alta presencia de días nublados en el sur del país (i.e., los datos de SAR son capaces de atravesar las nubes, por lo que el clima no es un problema para su uso e implementación) . Sin embargo, la combinación e integración de estos dos tipos de datos satelitales es poco común, por lo que no hay metodologías estándares para implementar de forma directa para cada situación (Gallant 2015).\nEn los últimos años, las redes neuronales convolucionales (CNN) han mejorado mucho el rendimiento de muchas tareas visuales. Se ha demostrado que es bastante eficaz para el aprendizaje robusto de características. Los modelos basados en CNN se han aplicado con éxito en la detección de cambios en imágenes de teledetección (Bazi, Bruzzone, and Melgani 2006, wang2022). En este sentido, (Gong, Cao, and Wu 2012) propone un marco de CNN de extremo a extremo para aprender características discriminativas de la matriz de afinidad mixta para la detección de cambios.\nConsiderando los objetivos de la presente tesis es desarrollar un método automatizado de detección en tiempo real de cambios en la estructura de la vegetación de humedales urbanos, turberas y bosque esclerófilo basado en productos satelitales y algoritmos de inteligencia artificial, a modo de trabajos futuros y si los resultados de detección de cambios automatizados son suficientemente precisos y suficientemente eficiente, estos serán insumos para un siguiente sistema identifique y segmente la zona que se considera como un cambio resultante por intervención humana, puedan ser visualizados en una plataforma de monitoreo que apoye la gestión de Superintendencia del Medio Ambiente, como sistema de aleta temprana. \n\n\n\n\nBazi, Y., L. Bruzzone, and F. Melgani. 2006. “Automatic Identification of the Number and Values of Decision Thresholds in the Log-Ratio Image for Change Detection in SAR Images.” IEEE Geoscience and Remote Sensing Letters 3 (3): 349–53. https://doi.org/10.1109/LGRS.2006.869973.\n\n\nGallant, Alisa L. 2015. “The Challenges of Remote Monitoring of Wetlands.” Remote Sensing 7 (8): 10938–50. https://doi.org/10.3390/rs70810938.\n\n\nGong, Maoguo, Yu Cao, and Qiaodi Wu. 2012. “A Neighborhood-Based Ratio Approach for Change Detection in SAR Images.” IEEE Geoscience and Remote Sensing Letters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\nMiranda, Alejandro, Adison Altamirano, Luis Cayuela, Antonio Lara, and Mauro González. 2017. “Native Forest Loss in the Chilean Biodiversity Hotspot: Revealing the Evidence.” Regional Environmental Change 17 (1): 285–97. https://doi.org/10.1007/s10113-016-1010-7.\n\n\nMyers, Norman, Russell A. Mittermeier, Cristina G. Mittermeier, Gustavo A. B. da Fonseca, and Jennifer Kent. 2000. “Biodiversity Hotspots for Conservation Priorities.” Nature 403 (6772): 853–58. https://doi.org/10.1038/35002501.\n\n\nSteffen, Will, Wendy Broadgate, Lisa Deutsch, Owen Gaffney, and Cornelia Ludwig. 2015. “The Trajectory of the Anthropocene: The Great Acceleration.” The Anthropocene Review 2 (1): 81–98. https://doi.org/10.1177/2053019614564785."
  },
  {
    "objectID": "e-arte.html",
    "href": "e-arte.html",
    "title": "4  Estado del Arte",
    "section": "",
    "text": "Entre las aplicaciones de la Percepción Remota, la detección de cambios tiene como objetivo reconocer la información modificada de una misma región mediante el análisis de imágenes mutlitemporales. En los últimos años ha despertado un gran interés (Amitrano, Guida, and Iervolino 2021). \nEn un proceso general de detección de cambios SAR, primero se crea la imagen de diferencia (DI), y la DI se clasifica en píxeles cambiados o no cambiados de forma supervisada o no supervisada (Sumaiya and Shantha Selva Kumari 2016). Sin embargo, la existencia de ruido de moteado es un problema no despreciable en la tarea de detección de cambios SAR. El ruido moteado (speckle) en imágenes SAR aparece como una forma de ruido multiplicativo y degrada la calidad de la imagen. Normalmente se producen falsas alarmas debido a la existencia de ruido speckle (Wu et al. 2019). Por lo tanto, la clasificación de DI para imágenes SAR sigue siendo una tarea difícil (Pham, Mercier, and Michel 2016). Por lo tanto , es crítico diseñar técnicas robustas de detección de cambios que sean efectivas en la supresión del ruido de moteado (speckle) (Saha, Bovolo, and Bruzzone 2021). \nEn los últimos años, los investigadores han dedicado grandes esfuerzos a resolver o paliar el efecto del ruido de moteado. Existen dos tipos de técnicas de detección de cambios: métodos supervisados y no supervisados. En comparación, los métodos no supervisados son más populares ya que comparan dos imágenes SAR multitemporales sin ningún conocimiento previo o muestras etiquetadas manualmente (Gong, Yang, and Zhang 2017). \nLos métodos existentes de detección de cambios sin supervisión se centran principalmente en la generación y clasificación de DI. En la generación de DI se utilizan los operadores log-ratio (Bovolo and Bruzzone 2005), Gauss-ratio (Biao Hou et al. 2014) o neighborhood-based ratio (Gong, Cao, and Wu 2012). Además, se utiliza el coeficiente de variación basado en series temporales de imágenes SAR para evitar el ruido de moteado (Jiang et al. 2020). Para la clasificación de DI, se han empleado muchos métodos de clustering, como fuzzy c-means (FCM) (Mishra, Ghosh, and Ghosh 2012), k- means clustering (Celik 2009), multiple kernel clustering (Jia et al. 2016), y algoritmo de mean-shift (Aiazzi et al. 2013). Además, los métodos de aprendizaje automático basados en campos aleatorios de Markov también se aplican a la tarea del moteado (Bruzzone and Prieto 2000). \nPara mejorar aún más el rendimiento de la clasificación DI, los investigadores incorporan clasificadores basados en aprendizaje profundo al modelo tradicional no supervisado. Gong et al. asignaron primero pseudoetiquetas a los píxeles en DI mediante un clasificador conjunto basado en FCM ((Gong et al. 2016). A continuación, se entrenaron máquinas de Boltzmann restringidas (RBM) para la generación de mapas de cambio. Hou et al. (Bin Hou, Wang, and Liu 2017) presentaron un método de detección de cambios combinando el cálculo de saliencia y el algoritmo de bajo rango. Zhan et al. (Zhan et al. 2017) propusieron un modelo CNN siamés profundo para extraer características discriminantes para la detección de cambios. Zhang et al. (Zhang et al. 2021) presentaron una CNN profunda de co-ocurrencia espacio-temporal de nivel de gris, que es capaz de explotar la información espacio-temporal de imágenes mutlitemporales. Dong et al. (Dong et al. 2022) integraron clustering no supervisado con CNN para aprender representaciones de características amigables con el clustering a partir de imágenes SAR multitemporales. Qu et al. (Qu et al. 2021) propusieron una red de dominio dual. Las características de los dominios de frecuencia y espacial se explotan para mitigar el ruido de moteado.\n\n\n\n\nAiazzi, Bruno, Luciano Alparone, Stefano Baronti, Andrea Garzelli, and Claudia Zoppetti. 2013. “Nonparametric Change Detection in Multitemporal SAR Images Based on Mean-Shift Clustering.” IEEE Transactions on Geoscience and Remote Sensing 51: 2022–31. https://doi.org/10.1109/TGRS.2013.2238946.\n\n\nAmitrano, Donato, Raffaella Guida, and Pasquale Iervolino. 2021. “Semantic Unsupervised Change Detection of Natural Land Cover with Multitemporal Object-Based Analysis on SAR Images.” IEEE Transactions on Geoscience and Remote Sensing 59 (7): 5494–514. https://doi.org/10.1109/TGRS.2020.3029841.\n\n\nBovolo, Francesca, and Lorenzo Bruzzone. 2005. “A Detail-Preserving Scale-Driven Approach to Change Detection in Multitemporal SAR Images.” IEEE T. Geoscience and Remote Sensing 43 (January): 2963–72.\n\n\nBruzzone, L., and D. F. Prieto. 2000. “Automatic Analysis of the Difference Image for Unsupervised Change Detection.” IEEE Transactions on Geoscience and Remote Sensing 38 (3): 1171–82. https://doi.org/10.1109/36.843009.\n\n\nCelik, Turgay. 2009. “Unsupervised Change Detection in Satellite Images Using Principal Component Analysis and k-Means Clustering.” IEEE Geoscience and Remote Sensing Letters 6 (4): 772–76. https://doi.org/10.1109/LGRS.2009.2025059.\n\n\nDong, Huihui, Wenping Ma, Licheng Jiao, Fang Liu, and LingLing Li. 2022. “A Multiscale Self-Attention Deep Clustering for Change Detection in SAR Images.” IEEE Transactions on Geoscience and Remote Sensing 60: 1–16. https://doi.org/10.1109/TGRS.2021.3073562.\n\n\nGong, Maoguo, Yu Cao, and Qiaodi Wu. 2012. “A Neighborhood-Based Ratio Approach for Change Detection in SAR Images.” IEEE Geoscience and Remote Sensing Letters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\nGong, Maoguo, Hailun Yang, and Puzhao Zhang. 2017. “Feature Learning and Change Feature Classification Based on Deep Learning for Ternary Change Detection in SAR Images.” ISPRS Journal of Photogrammetry and Remote Sensing 129: 212–25. https://doi.org/10.1016/j.isprsjprs.2017.05.001.\n\n\nGong, Maoguo, Jiaojiao Zhao, Jia Liu, Qiguang Miao, and Licheng Jiao. 2016. “Change Detection in Synthetic Aperture Radar Images Based on Deep Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems 27 (1): 125–38. https://doi.org/10.1109/TNNLS.2015.2435783.\n\n\nHou, Biao, Qian Wei, Yaoguo Zheng, and Shuang Wang. 2014. “Unsupervised Change Detection in SAR Image Based on Gauss-Log Ratio Image Fusion and Compressed Projection.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 7 (8): 3297–317. https://doi.org/10.1109/JSTARS.2014.2328344.\n\n\nHou, Bin, Yunhong Wang, and Qingjie Liu. 2017. “Change Detection Based on Deep Features and Low Rank.” IEEE Geoscience and Remote Sensing Letters 14 (12): 2418–22. https://doi.org/10.1109/LGRS.2017.2766840.\n\n\nJia, Lu, Ming Li, Peng Zhang, Yan Wu, and Huahui Zhu. 2016. “SAR Image Change Detection Based on Multiple Kernel k-Means Clustering with Local-Neighborhood Information.” IEEE Geoscience and Remote Sensing Letters 13 (6): 856–60. https://doi.org/10.1109/LGRS.2016.2550666.\n\n\nJiang, Mi, Andy Hooper, Xin Tian, Jia Xu, Sai-Nan Chen, Zhang-Feng Ma, and Xiao Cheng. 2020. “Delineation of Built-up Land Change from SAR Stack by Analysing the Coefficient of Variation.” ISPRS Journal of Photogrammetry and Remote Sensing 169: 93–108. https://doi.org/10.1016/j.isprsjprs.2020.08.023.\n\n\nMishra, Niladri, Susmita Ghosh, and Ashish Ghosh. 2012. “Fuzzy Clustering Algorithms Incorporating Local Information for Change Detection in Remotely Sensed Images.” Applied Soft Computing 12 (August): 2683–92. https://doi.org/10.1016/j.asoc.2012.03.060.\n\n\nPham, Minh-Tan, Grégoire Mercier, and Julien Michel. 2016. “Change Detection Between SAR Images Using a Pointwise Approach and Graph Theory.” IEEE Transactions on Geoscience and Remote Sensing 54 (4): 2020–32. https://doi.org/10.1109/TGRS.2015.2493730.\n\n\nQu, Xiaofan, Feng Gao, Junyu Dong, Qian Du, and Heng-Chao Li. 2021. “Change Detection in Synthetic Aperture Radar Images Using a Dual-Domain Network,” April.\n\n\nSaha, Sudipan, Francesca Bovolo, and Lorenzo Bruzzone. 2021. “Building Change Detection in VHR SAR Images via Unsupervised Deep Transcoding.” IEEE Transactions on Geoscience and Remote Sensing 59 (3): 1917–29. https://doi.org/10.1109/TGRS.2020.3000296.\n\n\nSumaiya, M. N., and R. Shantha Selva Kumari. 2016. “Logarithmic Mean-Based Thresholding for SAR Image Change Detection.” IEEE Geoscience and Remote Sensing Letters 13 (11): 1726–28. https://doi.org/10.1109/LGRS.2016.2606119.\n\n\nWu, Hui, Heng-Chao Lit, Gang Yang, and Wen Yang. 2019. “Change Detection of Remote Sensing Images Based on Weighted Nonnegative Matrix Factorization.” In 2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp), 1–4. https://doi.org/10.1109/Multi-Temp.2019.8866946.\n\n\nZhan, Yang, Kun Fu, Menglong Yan, Xian Sun, Hongqi Wang, and Xiaosong Qiu. 2017. “Change Detection Based on Deep Siamese Convolutional Network for Optical Aerial Images.” IEEE Geoscience and Remote Sensing Letters 14 (10): 1845–49. https://doi.org/10.1109/LGRS.2017.2738149.\n\n\nZhang, Xiao, Xin Su, Qiangqiang Yuan, and Qing Wang. 2021. “Spatial-Temporal Gray-Level Co-Occurrence Aware CNN for SAR Image Change Detection.” IEEE Geoscience and Remote Sensing Letters PP (September): 1–5. https://doi.org/10.1109/LGRS.2021.3110302."
  },
  {
    "objectID": "hipotesis.html",
    "href": "hipotesis.html",
    "title": "5  Hipótesis",
    "section": "",
    "text": "El uso de redes neuronales convolucionales en imágenes satelitales tipo radar de apertura sintética (SAR) mejorará la detección de cambios y selección de zonas afectadas por la intervención humana en ecosistemas naturales protegidos en comparación con técnicas convencionales como los métodos algebraicos y de reducción de dimensionalidad."
  },
  {
    "objectID": "objetivos.html#Objetivos-Generales",
    "href": "objetivos.html#Objetivos-Generales",
    "title": "6  Objetivos",
    "section": "6.1 Objetivos Generales",
    "text": "6.1 Objetivos Generales\nCrear sistema de detección de cambios no supervisado en ecosistemas naturales protegidos haciendo uso redes neuronales convolucionales, en imágenes satelitales tipo radar."
  },
  {
    "objectID": "objetivos.html#Objetivos-Espec",
    "href": "objetivos.html#Objetivos-Espec",
    "title": "6  Objetivos",
    "section": "6.2 Objetivos Específicos",
    "text": "6.2 Objetivos Específicos\n\nImplementar una red neuronal convolucional, que pueda detectar cambios en imágenes tipo radar en diferentes tiempos de forma no supervisada.\nComparar los resultados de la detección de cambio, de técnicas convencionales como los métodos algebraícos y de reducción de dimensionalidad con los obtenidos mediante el uso de redes neuronales convolucionales."
  },
  {
    "objectID": "a-estudio.html#turbera",
    "href": "a-estudio.html#turbera",
    "title": "7  Área de Estudio",
    "section": "7.1 Turberas en Chiloé",
    "text": "7.1 Turberas en Chiloé\nEl concepto de turba debe ser entendido como un sedimento natural de tipo fitógeno, poroso, no consolidado, constituido por materia orgánica parcialmente descompuesta, acumulada en un ambiente saturado de agua. De esta forma, se puede entender al concepto de turbera como un depósito de turba con un espesor de, al menos, 30 cm. (Hauser 1996)\nLas turberas solo cubren el 3 % de la superficie terrestre del planeta pero almacenan más carbono que todos los bosques de la Tierra si se mantienen húmedas.\nSegún (Hauser 1996) (Hauser 1996), el origen de las turberas se encuentra en las eras glaciares del Pleistoceno, cuando grandes extensiones de casquetes glaciares cubrieron el valle central de la Región de Los Lagos, incluyendo a la Isla Grande de Chiloé. El posterior retiro de los glaciares dejó masas de agua tierra adentro, formando los grandes lagos y lagunas glaciares que en la actualidad componen el paisaje de la región.\nRevisar y papers:\nEvaluation of impacts of management in an anthropogenic peatland using field and remote sensing data (Cabezas et al. 2015)\nUsing aboveground vegetation attributes as proxies.pdf (Lopatin et al. 2019)\nDisturbance alters relationships between soil carb.pdf (Lopatin et al. 2022)"
  },
  {
    "objectID": "a-estudio.html#humedales",
    "href": "a-estudio.html#humedales",
    "title": "7  Área de Estudio",
    "section": "7.2 Humedales Urbanos",
    "text": "7.2 Humedales Urbanos\nPor desarrollar …"
  },
  {
    "objectID": "a-estudio.html#bosque-esc",
    "href": "a-estudio.html#bosque-esc",
    "title": "7  Área de Estudio",
    "section": "7.3 Bosque y matorral esclerófilo de la región Metropolitana",
    "text": "7.3 Bosque y matorral esclerófilo de la región Metropolitana\nPor desarrollar …\n\n\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus perspectivas de utilización.” Revista Geológica de Chile 23 (2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208."
  },
  {
    "objectID": "adquisicion.html#resolución-temporal-sentinel-1",
    "href": "adquisicion.html#resolución-temporal-sentinel-1",
    "title": "8  Adquisición de Datos",
    "section": "8.1 Resolución Temporal Sentinel-1",
    "text": "8.1 Resolución Temporal Sentinel-1\nUn solo satélite Sentinel-1 podrá cartografiar el mundo entero una vez cada 12 días. La constelación de dos satélites ofrece un ciclo de repetición exacta de 6 días. La constelación tendrá una frecuencia de repetición (ascendente/descendente) de 3 días en el ecuador, menos de 1 día en el Ártico y se espera que proporcione cobertura sobre Europa, Canadá y las principales rutas marítimas en 1-3 días Figure 8.1, independientemente de las condiciones meteorológicas. Los datos del radar se entregarán a los servicios de Copernicus una hora después de su adquisición.\n\n\n\nFigure 8.1: Tiempo de Revisista en días del Satélite Sentinel-1\n\n\nSentinel-1 se encuentra en una órbita casi polar, sincrónica al sol, con un ciclo de repetición de 12 días y 175 órbitas por ciclo para un solo satélite. Tanto Sentinel-1A como Sentinel-1B comparten el mismo plano orbital con una diferencia de fase orbital de 180°. La cobertura geográfica es la que describe en la siguiente imagen Figure 8.2:\n\n\n\nFigure 8.2: Cobertura Geográfica de Satélite Sentinel-1"
  },
  {
    "objectID": "adquisicion.html#intrumentos-a-bordo-sentinel-1",
    "href": "adquisicion.html#intrumentos-a-bordo-sentinel-1",
    "title": "8  Adquisición de Datos",
    "section": "8.2 Intrumentos a Bordo Sentinel-1",
    "text": "8.2 Intrumentos a Bordo Sentinel-1\nSentinel-1 lleva un único instrumento de radar de apertura sintética en banda C que opera a una frecuencia central de 5,405 GHz. Incluye una antena activa phased array de orientación derecha que proporciona un rápido escaneo en elevación y azimut, una capacidad de almacenamiento de datos de 1 410 Gb y una capacidad de enlace descendente en banda X de 520 Mbit/s.\nEl instrumento C-SAR soporta el funcionamiento en polarización dual (HH+HV, VV+VH) implementado a través de una cadena de transmisión (conmutable a H o V) y dos cadenas de recepción paralelas para la polarización H y V. Los datos de doble polarización son útiles para la clasificación de la cubierta terrestre y las aplicaciones del hielo marino.\nSentinel-1 funciona en cuatro modos de adquisición exclusivos Figure 8.3:\n\n\n\nFigure 8.3: Modos de adquisión de imágenes de Sentinel-1\n\n\n\n\nModo Stripmap (SM) {#sm}:\n\nEl modo de imagen Stripmap se proporciona para la continuidad con las misiones ERS y Envisat. El modo Stripmap proporciona una cobertura con una resolución de 5 m por 5 m sobre una estrecha franja de 80 km. Se puede seleccionar una de las seis franjas de imágenes cambiando el ángulo de incidencia del haz y el ancho del haz de elevación.\n\n\n\nModo de hilera ancha interferométrica (IW):\n\nEl modo interferométrico de franja ancha (IW) permite combinar una gran anchura de franja (250 km) con una resolución geométrica moderada (5 m por 20 m). El modo IW toma imágenes de tres sub-bandas utilizando la Observación del Terreno con Escáneres Progresivos SAR (TOPSAR). Con la técnica TOPSAR, además de dirigir el haz en el rango como en SCANSAR, el haz también se dirige electrónicamente de atrás hacia adelante en la dirección acimut para cada ráfaga, evitando el festoneado y dando como resultado una imagen de mayor calidad. La interferometría está garantizada por un solapamiento suficiente del espectro Doppler (en el dominio acimutino) y del espectro del número de onda (en el dominio de la elevación). La técnica TOPSAR garantiza una calidad de imagen homogénea en toda la franja. El modo IW es el modo de adquisición por defecto sobre tierra.\n\n\n\nModo de barrido extra ancho (EW)\n\nEl modo de imagen de franja extra ancha está destinado a los servicios operativos marítimos, de hielo y de zonas polares en los que se requiere una amplia cobertura y tiempos de revisita cortos. El modo EW funciona de forma similar al modo IW, empleando una técnica TOPSAR que utiliza cinco sub-surcos en lugar de tres, lo que resulta en una resolución menor (20 m por 40 m). El modo EW también se puede utilizar para la interferometría como en el modo IW.\n\n\n\nModo Onda (WV)\n\nEl modo Wave del SENTINEL-1, junto con los modelos globales de olas oceánicas, puede ayudar a determinar la dirección, la longitud de onda y las alturas de las olas en los océanos abiertos. Las adquisiciones en el modo de onda se componen de imágenes de mapa de franjas de 20 km por 20 km, adquiridas alternativamente en dos ángulos de incidencia diferentes. Las imágenes de olas se adquieren cada 100 km, con imágenes en el mismo ángulo de incidencia separadas por 200 km."
  },
  {
    "objectID": "convencionales.html#insumos",
    "href": "convencionales.html#insumos",
    "title": "10  Métodos Convencionales",
    "section": "10.1 Insumos",
    "text": "10.1 Insumos\nPara explorar diferentes métodos convencionales de detección de cambios, utilizaremos tres imágenes de muestra adquiridas en la provincia de Guizhou, China, por el sensor SAR COSMO-SkyMed en junio de 2016 y abril de 2017. Como se puede ver en la Figure 10.1, estas imágenes antes y después, sumadas al mapa de referencia, incluyen montañas y un río. Estos elementos fueron utilizados en la investigación de Zhang et al. (2020).\n\n\n\n\n\n\n\n\n\n\n(a) Imagen adquirida junio 2016\n\n\n\n\n\n\n\n(b) Imagen adquirida Abril 2017\n\n\n\n\n\n\n\n(c) Mapa de referencias reales\n\n\n\n\nFigure 10.1: imágenes de prueba de métodos convencionales"
  },
  {
    "objectID": "convencionales.html#diferencia-directa-dd",
    "href": "convencionales.html#diferencia-directa-dd",
    "title": "10  Métodos Convencionales",
    "section": "10.2 Diferencia Directa (DD)",
    "text": "10.2 Diferencia Directa (DD)\nEl cálculo de la diferencia directa corresponde a la resta de ambas imágenes retornando sus valores absolutos de diferencia.\n\n\n\n\n\n\n\n\nFigure 10.2: Resultado del Cálculo de Diferencia Directa\n\n\n\n\n\n\n\n\n\nFigure 10.3: Histograma del resultado de cálculo de Diferencia Directa"
  },
  {
    "objectID": "convencionales.html#diferencia-mejorada",
    "href": "convencionales.html#diferencia-mejorada",
    "title": "10  Métodos Convencionales",
    "section": "10.3 Diferencia Mejorada",
    "text": "10.3 Diferencia Mejorada\nEl método propuesto para crear la imagen de diferencia mejorada combina la consistencia de la diferencia entre los píxeles individuales y sus vecindarios. Esto se logra mediante el uso de la relación logarítmica (LR) y la relación de verosimilitud logarítmica (LLR). La LR refleja la diferencia entre los píxeles individuales, mientras que la LLR refleja la diferencia entre los vecindarios.\n\n10.3.1 Relación logarítmica (LR) (Dekker, 1998)\nRefleja la diferencia entre los píxeles individuales. Esto se calcula utilizando la siguiente formula:\nD_{LR} (i, j) = log_{10}\\frac{I_2(i, j)}{I_1(i, j)}\nLos elementos I_1 y I_2 corrponde a las imágenes.\n\n\n\n\n\n\n\n\nFigure 10.4: Resultado de la Relación Logarítmica (LR)\n\n\n\n\n\n\n\n\n\nFigure 10.5: Histograma del resultado de la Relación Logarítmica\n\n\n\n\n\n\n10.3.2 Relación de verosimilitud logarítmica (LLR) (Cui et al., 2019)\nRefleja la diferencia entre los vecindarios. Se calcula utilizando las características estadísticas del vecindario del píxel para construir la relación de verosimilitud y se expresa con la siguiente ecuación:\nD_{LLR}(i, j)= log_{10}\\left(\\frac{(\\sum_{(m,n)\\in\\Omega{i,j}}I_1(m,n)+\\sum_{(m,n)\\in\\Omega{i,j}}I_2(m,n))^2}{4\\times \\sum_{(m,n)\\in\\Omega{i,j}}I_1(m,n)\\times\\sum_{(m,n)\\in\\Omega{i,j}}I_2(m,n)}\\right)\nDonde:\n\\Omega_{i, j} es el vecindario de (i, j),\nI{i,j(m,n)} es laintensidad de pixeles en \\Omega_{i, j}.\nI_{(m,n)} es el valor de píxel de la imagen original en la posición (m,n).\nSe calcula utilizando las características estadísticas del vecindario del píxel para construir la relación de verosimilitud\n\n\n\n`\n\n\n\n\n\n\n\n\nFigure 10.6: Resultado de Relación de Verosimilitud Logarítmica (LLR)\n\n\n\n\n\n\n\n\n\nFigure 10.7: Histograma de Relación de Verosimilitud Logarítmica (LLR)"
  },
  {
    "objectID": "convencionales.html#enhanced-difference-image-edi",
    "href": "convencionales.html#enhanced-difference-image-edi",
    "title": "10  Métodos Convencionales",
    "section": "10.4 Enhanced difference image (EDI)",
    "text": "10.4 Enhanced difference image (EDI)\nEl método propuesto para crear la imagen de diferencia mejorada combina la consistencia de la diferencia entre los píxeles individuales y sus vecindarios. Esto se logra mediante el uso de la relación logarítmica (LR) y la relación de verosimilitud logarítmica (LLR). La LR refleja la diferencia entre los píxeles individuales, mientras que la LLR refleja la diferencia entre los vecindarios.\n\nD(i, j) = D{LR} (i, j) \\times D_{LLR} (i, j)\n\n\n\n\n\n\n\n\n\nFigure 10.8: Resultado de Enhanced difference image\n\n\n\n\n\n\n\n\n\nFigure 10.9: Histograma de Enhanced difference image"
  },
  {
    "objectID": "convencionales.html#triangular-threshold-segmentation",
    "href": "convencionales.html#triangular-threshold-segmentation",
    "title": "10  Métodos Convencionales",
    "section": "10.5 Triangular Threshold Segmentation",
    "text": "10.5 Triangular Threshold Segmentation\nEl método de segmentación de umbral triangular se basa en la forma única del histograma DI y utiliza un método de Douglas-Peucker (Douglas y Peucker, 1973) para segmentar los datos en diferentes clases. Esto se logra mediante el uso de los diferentes niveles de gris y sus frecuencias correspondientes.\n\n\n\nIlustración esquemáticas del umbral triangular\n\n\nTODO: agregar ecuación\nEste método consiste en:\n\nCalcular el histograma de la imagen DI.\nCalcular el valor umbral inicial (T) como el punto medio entre el valor mínimo y máximo de la imagen DI.\nInicializar una lista vacía para almacenar las clases resultantes.\nRepetir los pasos 5-7 hasta que el valor umbral no cambie significativamente.\nCalcular las medias de las clases formadas por los píxeles con valores por encima y por debajo del umbral actual.\nCalcular el nuevo valor umbral como el promedio entre las medias de las clases.\nActualizar las clases resultantes dividiendo los píxeles en dos grupos: aquellos con valores por encima del umbral y aquellos con valores por debajo del umbral.\nDevolver las clases resultantes.\n\n\n10.5.1 Método Douglas-Peucker a imagen Diferencia Mejorada (EDI)\n\n\n\n\n\n\n\n\n\n\n(a) EDI bajo el umbral triangular de Douglas-Peucker\n\n\n\n\n\n\n\n(b) EDI sobre el umbral triangular de Douglas-Peucker\n\n\n\n\nFigure 10.10: Imágenes de Diferencia Directa con segmentación triangular\n\n\n\n\n10.5.2 Método Douglas-Peucker a imagen Diferencia Directa (DDI)\n\n\n\n\n\n\n\n\n\n\n(a) DDI bajo el umbral triangular de Douglas-Peucker\n\n\n\n\n\n\n\n(b) DDI sobre el umbral triangular de Douglas-Peucker\n\n\n\n\nFigure 10.11: Imágenes de Diferencia Directa con segmentación triangular"
  },
  {
    "objectID": "convencionales.html#log-mean-ratio-lmr",
    "href": "convencionales.html#log-mean-ratio-lmr",
    "title": "10  Métodos Convencionales",
    "section": "10.6 Log Mean Ratio (LMR)",
    "text": "10.6 Log Mean Ratio (LMR)\nEl operador LMR se define como la relación entre la media logarítmica de dos imágenes, I_1 y I_2. Esta relación se calcula para cada píxel (i,j) en el vecindario \\Omega.\nD_{LMR}(i, j)= log(max(\\frac{\\mu(\\Omega_{I_1})}{\\mu(\\Omega_{I_2})}, \\frac{\\mu(\\Omega_{I_2})}{\\mu(\\Omega_{I_1})}))\nDonde \\mu(\\Omega_{I_1}) y \\mu(\\Omega_{I_2}) presenta el promedio del vecindario en I_1 y I_2, respectivamente.\n\n\n\n\n\n\n\n\nFigure 10.12: Resultado de Log Mean Ratio (LMR) de ambas imágenes neighbourhood = 9\n\n\n\n\n\n\n\n\n\nFigure 10.13: Histogrma de Log Mean Ratio (LMR) de ambas imágenes neighbourhood = 9"
  },
  {
    "objectID": "convencionales.html#pca-k-means",
    "href": "convencionales.html#pca-k-means",
    "title": "10  Métodos Convencionales",
    "section": "10.7 PCA k-means",
    "text": "10.7 PCA k-means\n\n\n\nIlustración esquemáticas del umbral triangular\n\n\nPCA-Kmeans Celik (2009) divide la imagen de diferencia en bloques no solapados. Los vectores propios ortonormales se extraen mediante PCA del conjunto de bloques no solapados para crear un espacio de vectores propios. Cada píxel de la imagen de diferencia se representa con un vector de características S-dimensional que es la proyección de los datos de la imagen de diferencia en el espacio de vectores propios generado. La detección de cambios se consigue particionando el espacio de vectores de características en dos clusters mediante k-means.\n\n\n\n\n\n\n\n\nFigure 10.14: Resultado del PCA K-Means de la Imagen de Diferencias\n\n\n\n\n\n\n\n\n\nFigure 10.15: Histograma del PCA K-Means de la Imagen de Diferencias"
  },
  {
    "objectID": "convencionales.html#multi-hierarchical-fcm",
    "href": "convencionales.html#multi-hierarchical-fcm",
    "title": "10  Métodos Convencionales",
    "section": "10.8 Multi-hierarchical FCM",
    "text": "10.8 Multi-hierarchical FCM\nimplementar a caso de uso, codigo agregar jeraquizado\n\n\n\n\nCelik, Turgay. 2009. “Unsupervised Change Detection in Satellite Images Using Principal Component Analysis and k-Means Clustering.” IEEE Geoscience and Remote Sensing Letters 6 (4): 772–76. https://doi.org/10.1109/LGRS.2009.2025059.\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter M. Atkinson. 2020. “Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning.”"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "14  References",
    "section": "",
    "text": "Aiazzi, Bruno, Luciano Alparone, Stefano Baronti, Andrea Garzelli, and\nClaudia Zoppetti. 2013. “Nonparametric Change Detection in\nMultitemporal SAR Images Based on Mean-Shift Clustering.”\nIEEE Transactions on Geoscience and Remote Sensing 51: 2022–31.\nhttps://doi.org/10.1109/TGRS.2013.2238946.\n\n\nAmitrano, Donato, Raffaella Guida, and Pasquale Iervolino. 2021.\n“Semantic Unsupervised Change Detection of Natural Land Cover with\nMultitemporal Object-Based Analysis on SAR Images.” IEEE\nTransactions on Geoscience and Remote Sensing 59 (7): 5494–514. https://doi.org/10.1109/TGRS.2020.3029841.\n\n\nAraya-López, Rocío A., Javier Lopatin, Fabian E. Fassnacht, and H. Jaime\nHernández. 2018. “Monitoring Andean High Altitude Wetlands in\nCentral Chile with Seasonal Optical Data: A Comparison Between\nWorldview-2 and Sentinel-2 Imagery.” ISPRS Journal of\nPhotogrammetry and Remote Sensing, SI: Latin America Issue, 145\n(November): 213–24. https://doi.org/10.1016/j.isprsjprs.2018.04.001.\n\n\nBazi, Y., L. Bruzzone, and F. Melgani. 2006. “Automatic\nIdentification of the Number and Values of Decision Thresholds in the\nLog-Ratio Image for Change Detection in SAR Images.” IEEE\nGeoscience and Remote Sensing Letters 3 (3): 349–53. https://doi.org/10.1109/LGRS.2006.869973.\n\n\nBovolo, Francesca, and Lorenzo Bruzzone. 2005. “A\nDetail-Preserving Scale-Driven Approach to Change Detection in\nMultitemporal SAR Images.” IEEE T. Geoscience and Remote\nSensing 43 (January): 2963–72.\n\n\nBruzzone, L., and D. F. Prieto. 2000. “Automatic Analysis of the\nDifference Image for Unsupervised Change Detection.” IEEE\nTransactions on Geoscience and Remote Sensing 38 (3): 1171–82. https://doi.org/10.1109/36.843009.\n\n\nCabezas, Juli’an, Mauricio Galleguillos, Ariel Vald’es, Juan P. Fuentes,\nCecilia P’erez, and Jorge F. Perez-Quezada. 2015. “Evaluation of\nImpacts of Management in an Anthropogenic Peatland Using Field and\nRemote Sensing Data.” Ecosphere 6 (12): 1–24. https://doi.org/10.1890/ES15-00232.1.\n\n\nCelik, Turgay. 2009. “Unsupervised Change Detection in Satellite\nImages Using Principal Component Analysis and k-Means Clustering.” IEEE\nGeoscience and Remote Sensing Letters 6 (4): 772–76. https://doi.org/10.1109/LGRS.2009.2025059.\n\n\nChen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu\nLiu, and Haifeng Li. 2021. “DASNet: Dual\nAttentive Fully Convolutional Siamese Networks for Change Detection of\nHigh Resolution Satellite Images.” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing 14:\n1194–1206. https://doi.org/10.1109/JSTARS.2020.3037893.\n\n\nDíaz, María F., Juan Larraín, Gabriela Zegers, and Carolina Tapia. 2008.\n“Floristic and Hydrological Characterization of Chiloé Island\nPeatlands, Chile.” Revista Chilena de Historia Natural\n81 (4): 455–68. https://doi.org/10.4067/S0716-078X2008000400002.\n\n\nDong, Huihui, Wenping Ma, Licheng Jiao, Fang Liu, and LingLing Li. 2022.\n“A Multiscale Self-Attention Deep Clustering for Change Detection\nin SAR Images.” IEEE Transactions on Geoscience and Remote\nSensing 60: 1–16. https://doi.org/10.1109/TGRS.2021.3073562.\n\n\nFassnacht, Fabian Ewald, Javiera Poblete-Olivares, Lucas Rivero, Javier\nLopatin, Andrés Ceballos-Comisso, and Mauricio Galleguillos. 2021.\n“Using Sentinel-2 and Canopy Height Models to Derive a\nLandscape-Level Biomass Map Covering Multiple Vegetation Types.”\nInternational Journal of Applied Earth Observation and\nGeoinformation 94 (February): 102236. https://doi.org/10.1016/j.jag.2020.102236.\n\n\nGallant, Alisa L. 2015. “The Challenges of Remote Monitoring of\nWetlands.” Remote Sensing 7 (8): 10938–50. https://doi.org/10.3390/rs70810938.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021a. “SAR\nImage Change Detection Based on Multiscale Capsule Network.”\nIEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\n———. 2021b. “SAR Image Change Detection Based on Multiscale\nCapsule Network.” IEEE Geoscience and Remote Sensing\nLetters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nGong, Maoguo, Yu Cao, and Qiaodi Wu. 2012a. “A Neighborhood-Based\nRatio Approach for Change Detection in SAR Images.” IEEE\nGeoscience and Remote Sensing Letters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\n———. 2012b. “A Neighborhood-Based Ratio Approach for Change\nDetection in SAR Images.” IEEE Geoscience and Remote Sensing\nLetters 9 (2): 307–11. https://doi.org/10.1109/LGRS.2011.2167211.\n\n\nGong, Maoguo, Hailun Yang, and Puzhao Zhang. 2017. “Feature\nLearning and Change Feature Classification Based on Deep Learning for\nTernary Change Detection in SAR Images.” ISPRS\nJournal of Photogrammetry and Remote Sensing 129: 212–25. https://doi.org/10.1016/j.isprsjprs.2017.05.001.\n\n\nGong, Maoguo, Jiaojiao Zhao, Jia Liu, Qiguang Miao, and Licheng Jiao.\n2016. “Change Detection in Synthetic Aperture\nRadar Images Based on Deep Neural Networks.”\nIEEE Transactions on Neural Networks and Learning Systems 27\n(1): 125–38. https://doi.org/10.1109/TNNLS.2015.2435783.\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus\nperspectivas de utilización.” Revista Geológica de Chile 23\n(2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208.\n\n\nHou, Biao, Qian Wei, Yaoguo Zheng, and Shuang Wang. 2014.\n“Unsupervised Change Detection in SAR Image Based on Gauss-Log\nRatio Image Fusion and Compressed Projection.” IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing\n7 (8): 3297–317. https://doi.org/10.1109/JSTARS.2014.2328344.\n\n\nHou, Bin, Yunhong Wang, and Qingjie Liu. 2017. “Change Detection\nBased on Deep Features and Low Rank.” IEEE Geoscience and\nRemote Sensing Letters 14 (12): 2418–22. https://doi.org/10.1109/LGRS.2017.2766840.\n\n\nJia, Lu, Ming Li, Peng Zhang, Yan Wu, and Huahui Zhu. 2016. “SAR\nImage Change Detection Based on Multiple Kernel k-Means Clustering with\nLocal-Neighborhood Information.” IEEE Geoscience and Remote\nSensing Letters 13 (6): 856–60. https://doi.org/10.1109/LGRS.2016.2550666.\n\n\nJiang, Mi, Andy Hooper, Xin Tian, Jia Xu, Sai-Nan Chen, Zhang-Feng Ma,\nand Xiao Cheng. 2020. “Delineation of Built-up Land Change from\nSAR Stack by Analysing the Coefficient of\nVariation.” ISPRS Journal of Photogrammetry and Remote\nSensing 169: 93–108. https://doi.org/10.1016/j.isprsjprs.2020.08.023.\n\n\nLopatin, Javier, Roc’ıo ArayaNANAL’opez, Mauricio Galleguillos, and\nJorge F. PereNAzNAQueza. 2022. “Disturbance Alters Relationships\nBetween Soil Carbon Pools and Aboveground Vegetation Attributes in an\nAnthropogenic Peatland in Patagonia.” Ecology\nand Evolution 12 (3). https://doi.org/10.1002/ece3.8694.\n\n\nLopatin, Javier, Teja Kattenborn, Mauricio Galleguillos, Jorge F.\nPerez-Quezada, and Sebastian Schmidtlein. 2019. “Using Aboveground\nVegetation Attributes as Proxies for Mapping Peatland Belowground Carbon\nStocks.” Remote Sensing of Environment 231 (September):\n111217. https://doi.org/10.1016/j.rse.2019.111217.\n\n\nMiranda, Alejandro, Adison Altamirano, Luis Cayuela, Antonio Lara, and\nMauro González. 2017. “Native Forest Loss in the Chilean\nBiodiversity Hotspot: Revealing the Evidence.” Regional\nEnvironmental Change 17 (1): 285–97. https://doi.org/10.1007/s10113-016-1010-7.\n\n\nMishra, Niladri, Susmita Ghosh, and Ashish Ghosh. 2012. “Fuzzy\nClustering Algorithms Incorporating Local Information for Change\nDetection in Remotely Sensed Images.” Applied Soft\nComputing 12 (August): 2683–92. https://doi.org/10.1016/j.asoc.2012.03.060.\n\n\nMyers, Norman, Russell A. Mittermeier, Cristina G. Mittermeier, Gustavo\nA. B. da Fonseca, and Jennifer Kent. 2000. “Biodiversity Hotspots\nfor Conservation Priorities.” Nature 403 (6772): 853–58.\nhttps://doi.org/10.1038/35002501.\n\n\nPham, Minh-Tan, Grégoire Mercier, and Julien Michel. 2016. “Change\nDetection Between SAR Images Using a Pointwise Approach and Graph\nTheory.” IEEE Transactions on Geoscience and Remote\nSensing 54 (4): 2020–32. https://doi.org/10.1109/TGRS.2015.2493730.\n\n\nQu, Xiaofan, Feng Gao, Junyu Dong, Qian Du, and Heng-Chao Li. 2021.\n“Change Detection in Synthetic Aperture Radar Images Using a\nDual-Domain Network,” April.\n\n\nSaha, Sudipan, Francesca Bovolo, and Lorenzo Bruzzone. 2021.\n“Building Change Detection in VHR SAR Images via Unsupervised Deep\nTranscoding.” IEEE Transactions on Geoscience and Remote\nSensing 59 (3): 1917–29. https://doi.org/10.1109/TGRS.2020.3000296.\n\n\nSchofield W.B. 1985. “Introduction to Bryology.”\n\n\nSteffen, Will, Wendy Broadgate, Lisa Deutsch, Owen Gaffney, and Cornelia\nLudwig. 2015. “The Trajectory of the Anthropocene: The Great\nAcceleration.” The Anthropocene Review 2 (1): 81–98. https://doi.org/10.1177/2053019614564785.\n\n\nSumaiya, M. N., and R. Shantha Selva Kumari. 2016. “Logarithmic\nMean-Based Thresholding for SAR Image Change Detection.” IEEE\nGeoscience and Remote Sensing Letters 13 (11): 1726–28. https://doi.org/10.1109/LGRS.2016.2606119.\n\n\nTreimun, John. 2017. “Turberas de Chiloé, Ministerio del Medio\nAmbiente, Chile.”\n\n\nVolpi, Michele, Devis Tuia, Francesca Bovolo, Mikhail Kanevski, and\nLorenzo Bruzzone. 2013. “Supervised Change Detection in VHR Images\nUsing Contextual Information and Support Vector Machines.”\nInternational Journal of Applied Earth Observation and\nGeoinformation, Earth Observation and Geoinformation for\nEnvironmental Monitoring, 20 (February): 77–85. https://doi.org/10.1016/j.jag.2011.10.013.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022.\n“Change Detection from Synthetic Aperture Radar Images via\nGraph-Based Knowledge Supplement Network.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167.\n\n\nWang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022.\n“Self-Supervised Vision Transformers for Joint SAR-optical Representation Learning.”\narXiv. http://arxiv.org/abs/2204.05381.\n\n\nWu, Hui, Heng-Chao Lit, Gang Yang, and Wen Yang. 2019. “Change\nDetection of Remote Sensing Images Based on Weighted Nonnegative Matrix\nFactorization.” In 2019 10th International Workshop on the\nAnalysis of Multitemporal Remote Sensing Images (MultiTemp), 1–4.\nhttps://doi.org/10.1109/Multi-Temp.2019.8866946.\n\n\nYan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. “Fully\nTransformer Network for Change Detection of\nRemote Sensing Images.” arXiv. http://arxiv.org/abs/2210.00757.\n\n\nZegers, Gabriela, Juan Larraín, María Francisca Díaz, and Juan Armesto.\n2006. “Impacto Ecológico y Social de La Explotación de Pomponales\ny Turberas de Sphagnum En La Isla Grande de Chiloé.” http://biblioteca.cehum.org/handle/CEHUM2018/1389.\n\n\nZhan, Yang, Kun Fu, Menglong Yan, Xian Sun, Hongqi Wang, and Xiaosong\nQiu. 2017. “Change Detection Based on Deep Siamese Convolutional\nNetwork for Optical Aerial Images.” IEEE Geoscience and\nRemote Sensing Letters 14 (10): 1845–49. https://doi.org/10.1109/LGRS.2017.2738149.\n\n\nZhang, Xiao, Xin Su, Qiangqiang Yuan, and Qing Wang. 2021.\n“Spatial-Temporal Gray-Level Co-Occurrence Aware CNN for SAR Image\nChange Detection.” IEEE Geoscience and Remote Sensing\nLetters PP (September): 1–5. https://doi.org/10.1109/LGRS.2021.3110302.\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter\nM. Atkinson. 2020. “Robust Unsupervised\nSmall Area Change\nDetection from SAR Imagery\nUsing Deep Learning.”"
  },
  {
    "objectID": "rev_papers.html#detección-de-cambios-con-imágenes-radar",
    "href": "rev_papers.html#detección-de-cambios-con-imágenes-radar",
    "title": "Revisión papers",
    "section": "Detección de Cambios con Imágenes Radar",
    "text": "Detección de Cambios con Imágenes Radar\n\nMs-CapsNet\nSAR Image Change Detection Based on Multiscale Capsule Network (Gao et al. 2021)\n\nResumen:\n\nLos métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los retos del ruido de moteado y la sensibilidad a la deformación. Para mitigar estos problemas, propone una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se ha diseñado un módulo de convolución de fusión adaptativa (AFC) para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.\n\nIndex Terms:\n\nChange detection, multiscale capsule network, synthetic aperture radar, deep learning.\n\n\n\n\n\nMs-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala\n\n\nResultados y Análisis de Experimentos.\n\n\n\nVisualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet."
  },
  {
    "objectID": "rev_papers.html#graph-based-knowledge-supplement",
    "href": "rev_papers.html#graph-based-knowledge-supplement",
    "title": "Revisión papers",
    "section": "Graph-Based Knowledge Supplement",
    "text": "Graph-Based Knowledge Supplement\nChange Detection From Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network (J. Wang et al. 2022)\n\nResumen:\n\nLa detección de cambios en las imágenes del radar de apertura sintética (SAR) es una tarea vital pero difícil en el campo del análisis de imágenes de teledetección. La mayoría de los trabajos anteriores adoptan un método auto-supervisado que utiliza muestras pseudo-etiquetadas para guiar el entrenamiento y las pruebas subsecuentes. Sin embargo, las redes profundas suelen requerir muchas muestras de alta calidad para la optimización de los parámetros. El ruido en las pseudo-etiquetas afecta inevitablemente al rendimiento final de la detección de cambios. Para resolver el problema, proponemos una red de complemento de conocimiento basada en grafos (GKSNet). Para ser más específicos, extraemos información discriminatoria del conjunto de datos etiquetados existente como conocimiento adicional, para suprimir hasta cierto punto los efectos adversos de las muestras ruidosas. A continuación, diseñamos un módulo de transferencia de grafos para destilar información contextual de forma atenta desde el conjunto de datos etiquetados al conjunto de datos de destino, lo que permite salvar la correlación de características entre los conjuntos de datos. Para validar el método propuesto, realizamos amplios experimentos con cuatro conjuntos de datos de SAR, que demostraron la superioridad de la GKSNet propuesta en comparación con varias líneas de base del estado de la técnica.\n\nIndex Terms:\n\nChange detection, graph dependency fusion, knowledge supplement network, synthetic aperture radar (SAR).\n\n\n\n\n\nSchematic illustration of the proposed GKSNet. The image features extracted by deep CNNs are projected into a graph representation. Then, the graph representations are transferred and fused via graph transfer module across different datasets. Finally, features from different graphs are fused via intergraph fusion module. Through feature fusion, the model exploits the common knowledge and bridge the feature correlation from different datasets. The obtained evolved features are capable of improving the change detection performance."
  },
  {
    "objectID": "rev_papers.html#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022",
    "href": "rev_papers.html#fully-transformer-network-for-change-detection-of-remote-sensing-images-yan_fully_2022",
    "title": "Revisión papers",
    "section": "Fully Transformer Network for Change Detection of Remote Sensing Images (Yan, Wan, and Zhang 2022)",
    "text": "Fully Transformer Network for Change Detection of Remote Sensing Images (Yan, Wan, and Zhang 2022)\n\nResumen:\n\nRecientemente, la detección de cambios (CD) de las imágenes de teledetección ha logrado un gran progreso con los avances del aprendizaje profundo. Sin embargo, los métodos actuales generalmente ofrecen regiones de CD incompletas y límites de CD irregulares debido a la limitada capacidad de representación de las características visuales extraídas. Para aliviar estos problemas, en este trabajo proponemos un novedoso marco de aprendizaje llamado Fully Transformer Network (FTN) para la CD de imágenes de teledetección, que mejora la extracción de características desde una vista global y combina características visuales de varios niveles de forma piramidal. Más concretamente, el marco propuesto utiliza en primer lugar las ventajas de los transformadores en el modelado de dependencias de largo alcance. Puede ayudar a aprender más características discriminativas de nivel global y obtener regiones completas de CD. A continuación, introducimos una estructura piramidal para agregar características visuales multinivel a partir de Transformers para mejorar las características. La estructura piramidal injertada con un Módulo de Atención Progresiva (PAM) puede mejorar la capacidad de representación de características con interdependencias adicionales a través de atenciones de canal. Por último, para entrenar mejor el marco, utilizamos el aprendizaje supervisado en profundidad con múltiples funciones de pérdida de límites. Amplios experimentos demuestran que nuestro método propuesto alcanza un nuevo rendimiento de vanguardia en cuatro puntos de referencia públicos de CD.\n\n\n\n\n\n\n\nLa estructura general del marco propuesto. Fully Transformer Network for Change Detection of Remote Sensing Images\n\n\n\n\nIndex Terms: Fully Transformer Network, Change Detection, Remote Sensing Image."
  },
  {
    "objectID": "rev_papers.html#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022",
    "href": "rev_papers.html#self-supervised-vision-transformers-for-joint-sar-optical-representation-learning-wang_self-supervised_2022",
    "title": "Revisión papers",
    "section": "Self-supervised Vision Transformers for Joint SAR-optical Representation Learning (Y. Wang, Albrecht, and Zhu 2022)",
    "text": "Self-supervised Vision Transformers for Joint SAR-optical Representation Learning (Y. Wang, Albrecht, and Zhu 2022)\n\nResumen:\n\nEl aprendizaje autosupervisado (SSL) ha suscitado un gran interés en la teledetección y la observación de la Tierra debido a su capacidad para aprender representaciones independientes de la tarea sin necesidad de anotaciones humanas. Mientras que la mayoría de los trabajos existentes sobre SSL en teledetección utilizan columnas vertebrales ConvNet y se centran en una sola modalidad, nosotros exploramos el potencial de los transformadores de visión (ViTs) para el aprendizaje conjunto de representaciones SAR-ópticas. Basándonos en DINO, un algoritmo SSL de última generación que destila conocimiento de dos vistas aumentadas de una imagen de entrada, combinamos imágenes SAR y ópticas concatenando todos los canales en una entrada unificada. Posteriormente, enmascaramos aleatoriamente los canales de una modalidad como estrategia de aumento de datos. Durante el entrenamiento, el modelo se alimenta de pares de imágenes ópticas, SAR y SAR-ópticas para aprender representaciones internas e intra-modales. Los resultados experimentales que emplean el conjunto de datos BigEarthNet-MM demuestran las ventajas tanto de los ejes ViT como del algoritmo SSL multimodal propuesto DINO-MM.\n\n\n\n\n\n\n\nDINO-MM: el algoritmo SSL conjunto SAR-óptico propuesto. La imagen SAR-óptica concatenada se toma como entrada bruta. Se transforma aleatoriamente en dos vistas aumentadas y se introduce en una red maestro-estudiante basada en DINO.\n\n\n\n\nIndex Terms: Self-supervised learning, vision trans- former, multimodal representation learning"
  },
  {
    "objectID": "rev_papers.html#detección-de-cambios-con-imágenes-de-alta-resolución",
    "href": "rev_papers.html#detección-de-cambios-con-imágenes-de-alta-resolución",
    "title": "Revisión papers",
    "section": "Detección de Cambios con Imágenes de Alta Resolución",
    "text": "Detección de Cambios con Imágenes de Alta Resolución\n\nDASNet\nDASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images(Chen et al. 2021)\n\nResumen:\n\nLa detección de cambios es una tarea básica del procesamiento de imágenes por teledetección. El objetivo de la investigación es identificar la información de cambio de interés y filtrar la información de cambio irrelevante como factores de interferencia. Recientemente, el aumento del aprendizaje profundo ha proporcionado nuevas herramientas para la detección de cambios, que han dado resultados impresionantes. Sin embargo, los métodos disponibles se centran principalmente en la información de diferencia entre las imágenes de teledetección multitemporal y carecen de robustez ante la información de pseudocambio. Para superar la falta de resistencia de los métodos actuales a los pseudocambios, en este trabajo proponemos un nuevo método, a saber, las redes siamesas totalmente convolucionales de atención dual (DASNet), para la detección de cambios en imágenes de alta resolución. A través del mecanismo de atención dual, se capturan las dependencias de largo alcance para obtener representaciones de características más discriminantes para mejorar el rendimiento de reconocimiento del modelo. Además, la muestra desequilibrada es un problema grave en la detección de cambios, es decir, las muestras sin cambios son mucho más abundantes que las muestras con cambios, lo que constituye una de las principales razones de los pseudocambios. Proponemos la pérdida contrastiva ponderada de doble margen para abordar este problema, castigando la atención a los pares de características sin cambios y aumentando la atención a los pares de características con cambios. Los resultados experimentales de nuestro método en el conjunto de datos de detección de cambios (CDD) y en el conjunto de datos de detección de cambios en edificios (BCDD) demuestran que, en comparación con otros métodos de referencia, el método propuesto consigue mejoras máximas del 2,9% y el 4,2%, respectivamente, en la puntuación F1. Nuestra implementación de PyTorch está disponible en https://github.com/lehaifeng/DASNet.\n\nIndex Terms:\n\nChange detection, high-resolution images, dual attention, Siamese network, weighted double-margin contrastive loss.\n\n\n\n\n\n\n\nDASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite\n\n\n\n\n\n\n\n\nChen, Jie, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu, Yu Liu, and Haifeng Li. 2021. “DASNet: Dual Attentive Fully Convolutional Siamese Networks for Change Detection of High Resolution Satellite Images.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14: 1194–1206. https://doi.org/10.1109/JSTARS.2020.3037893.\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838.\n\n\nWang, Junjie, Feng Gao, Junyu Dong, Shan Zhang, and Qian Du. 2022. “Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 15: 1823–36. https://doi.org/10.1109/JSTARS.2022.3146167.\n\n\nWang, Yi, Conrad M. Albrecht, and Xiao Xiang Zhu. 2022. “Self-Supervised Vision Transformers for Joint SAR-optical Representation Learning.” arXiv. http://arxiv.org/abs/2204.05381.\n\n\nYan, Tianyu, Zifu Wan, and Pingping Zhang. 2022. “Fully Transformer Network for Change Detection of Remote Sensing Images.” arXiv. http://arxiv.org/abs/2210.00757."
  },
  {
    "objectID": "Ms-CapsNet.html#ms-capsnet-sar-image-change-detection-based-on-multiscale-capsule-network-gao2021",
    "href": "Ms-CapsNet.html#ms-capsnet-sar-image-change-detection-based-on-multiscale-capsule-network-gao2021",
    "title": "Ms-CapsNet",
    "section": "Ms-CapsNet: SAR Image Change Detection Based on Multiscale Capsule Network (Gao et al. 2021)",
    "text": "Ms-CapsNet: SAR Image Change Detection Based on Multiscale Capsule Network (Gao et al. 2021)\n\nAbstract\nLos métodos tradicionales de detección de cambios en imágenes de radar de apertura sintética basados en redes neuronales convolucionales (CNN) se enfrentan a los retos del ruido de moteado y la sensibilidad a la deformación. Para mitigar estos problemas, propusimos una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre los píxeles cambiados y los no cambiados. Por un lado, el módulo de cápsula multiescala se emplea para explotar la relación espacial de las características. Por lo tanto, se pueden conseguir propiedades equivariantes agregando las características de diferentes posiciones. Por otro lado, se diseña un módulo de convolución de fusión adaptativa (AFC) para la Ms-CapsNet propuesta. Se pueden capturar características semánticas más altas para las cápsulas primarias. Las características extraídas por el módulo AFC mejoran significativamente la robustez frente al ruido de moteado. La eficacia de la Ms-CapsNet propuesta se verifica en tres conjuntos de datos SAR reales. Los experimentos de comparación con cuatro métodos de vanguardia demuestran la eficacia del método propuesto.\nRespositorio: https://github.com/summitgao/SAR_CD_MS_CapsNet.\n\n\nIntroducción\nAunque se han propuesto muchas técnicas, la detección de cambios en las imágenes SAR sigue siendo una tarea difícil. La calidad de la imagen se ve deteriorada por el ruido de moteado que dificulta la interpretación meticulosa de los datos SAR. Se han implementado muchos métodos para abordar el problema del ruido de moteado. Suelen constar de tres pasos:\n\nCoregistro de imágenes: El corregistro de imágenes es una tarea fundamental para establecer las correspondencias espaciales entre las imágenes SAR multitemporales.\nGeneración de imágenes de diferencia (DI) : La DI se genera habitualmente mediante los operadores log- ratio, Gauss-ratio [5] y neighborhood-ratio [6].\nClasificación de DI: la mayoría de las investigaciones se dedican a construir un clasificador robusto. Se trata de una tarea no trivial, ya que un clasificador potente determina directamente la precisión de la detección de cambios.\n\nMuchos investigadores se dedican a desarrollar clasificadores potentes para la detección de cambios. Li et al. [7] diseñaron un algoritmo de clustering de dos niveles para la detección de cambios sin supervisión. En [8], la información de vecindad local se incorpora a la función objetivo de clustering para mejorar el rendimiento de la detección de cambios. Gong et al. [9] desarrollaron un campo aleatorio de Markov (MRF) mejorado basado en el clustering de c-medias difusas (FCM) para suprimir el ruido de moteado. En [4], se emplearon las máquinas de Boltzmann restringidas (RBM) apiladas para la detección de cambios en las imágenes SAR. Aunque los métodos anteriores lograron un rendimiento prometedor, las capacidades de representación de características siguen siendo limitadas.\nEn los últimos años, las redes neuronales convolucionales (CNN) han mejorado mucho el rendimiento de muchas tareas visuales. Se ha demostrado que es bastante eficaz para el aprendizaje robusto de características. Los modelos basados en CNN se han aplicado con éxito en la detección de cambios en imágenes de teledetección [10]. Wang et al. [11] propusieron un marco de CNN de extremo a extremo para aprender características discriminativas de la matriz de afinidad mixta para la detección de cambios.\nMás tarde, se desarrolló el modelado de ruido profundo no supervisado para la detección de cambios en imágenes hiperespectrales [12]. Liu et al. [13] propusieron una elegante CNN local restringida (LR-CNN) para la detección de cambios polarimétricos en SAR. En [14], se aplicó el aprendizaje profundo transferido a la detección de cambios en imágenes SAR de hielo marino basado en CNN. Aunque los métodos basados en CNN han logrado un excelente rendimiento en la detección de cambios, la precisión a veces se deteriora en el caso de la transformación, como las inclinaciones y rotaciones. En concreto, la CNN es incapaz de modelar la relación posicional entre los objetos del suelo.\nMás recientemente, Sabour y Hinton propusieron la red Capsule (CapsNet) para dar solución a los problemas en los que los modelos CNN son inadecuados [15]. En CapsNet, un vector de actividad de cápsulas representa los parámetros de instanciación de la entidad, como la pose, la textura y la deformación. La existencia de entidades se expresa mediante la longitud de los parámetros de instanciación. El mecanismo de enrutamiento dinámico se utiliza para la propagación de la información. Se ha comprobado empíricamente que CapsNet es eficaz para el análisis de imágenes de teledetección [16] [17]. Hasta donde sabemos, la literatura sobre la detección de cambios en SAR basada en CapsNet es muy escasa.\n\n\n\nMs-CapsNet: Ilustración del método de detección de cambios propuesto basado en la red de cápsulas multiescala\n\n\nSostenemos que la debilidad de los enfoques existentes de detección de cambios en imágenes SAR proviene principalmente de dos aspectos: Uno es que la correlación de las características de diferentes posiciones no se puede modelar de forma efectiva. El otro es el ruido intrínseco del moteado en las imágenes SAR. Para hacer frente a los problemas mencionados, se propone una red de cápsulas multiescala (Ms-CapsNet) para extraer la información discriminativa entre las imágenes SAR multitemporales. La Ms-CapsNet propuesta tiene una estructura similar a la Red de Cápsulas [15] sin el operador multiescala y el módulo de Convolución de Fusión Adaptativa (AFC). La Ms-CapsNet proporciona un grupo de parámetros de instanciación para capturar características de diferentes posiciones. Para hacer frente al problema del ruido de moteado, el módulo AFC está diseñado para convertir las intensidades de los píxeles en actividades de las características locales. De este modo, las características locales se vuelven robustas al ruido. Se realizan amplios experimentos con tres conjuntos de datos reales para demostrar la superioridad de nuestro método propuesto sobre cuatro trabajos del estado del arte.\nPara mayor claridad, las principales contribuciones se resumen como sigue:\n\nLa Ms-CapsNet propuesta tiene la capacidad de extraer características robustas de diferentes posiciones. Las propiedades equivariantes se pueden conseguir mediante el módulo de cápsulas. Por lo tanto, la demanda de una gran cantidad de muestras de entrenamiento se reduce por la información correlativa y completa.\nSe diseña un módulo AFC sencillo pero eficaz, que puede convertir eficazmente las intensidades de los píxeles en actividades de características locales. El módulo AFC extrae las características semánticas superiores y enfatiza las significativas mediante una estrategia basada en la atención. Por lo tanto, las características locales de actividad se vuelven más resistentes al ruido y se aceptan inmediatamente como entrada de la cápsula primaria.\nSe han realizado amplios experimentos con tres conjuntos de datos de SAR para validar la eficacia del método propuesto. Además, hemos publicado los códigos y la configuración para facilitar futuras investigaciones en el análisis de imágenes de teledetección multitemporal."
  },
  {
    "objectID": "Ms-CapsNet.html#metodología",
    "href": "Ms-CapsNet.html#metodología",
    "title": "Ms-CapsNet",
    "section": "Metodología",
    "text": "Metodología\n\nA. Adaptive Fusion Convolution Module (AFC)\n\n\n\nIllustration of the Adaptive Fusion Convolution (AFC) module.\n\n\n\n\nB. Capsule Module"
  },
  {
    "objectID": "Ms-CapsNet.html#resultados-y-análisis-de-experimentos.",
    "href": "Ms-CapsNet.html#resultados-y-análisis-de-experimentos.",
    "title": "Ms-CapsNet",
    "section": "Resultados y Análisis de Experimentos.",
    "text": "Resultados y Análisis de Experimentos.\n\n\n\nVisualized results of different change detection methods on three datasets. (a) Image captured at t1. (b) Image captured at t2. (c) Ground truth image. (d) Result by PCANet. (e) Result by MLFN. (f) Result by DCNN. (g) Result by LR-CNN. (h) Result by the proposed Ms-CapsNet.\n\n\n\nA. Dataset and Evaluation Criteria\n\n\nB. Parameters Analysis of the Proposed Ms-CapsNet\n\n\nC. Change Detection Results on Three Datasets"
  },
  {
    "objectID": "Ms-CapsNet.html#conclusion",
    "href": "Ms-CapsNet.html#conclusion",
    "title": "Ms-CapsNet",
    "section": "Conclusion",
    "text": "Conclusion\n\nReferencias\n[1] D. Burnner, G. Lemonie, and L. Bruzzone, “Earthquake damage assess- ment of buildings using VHR optical and SAR imagery,” IEEE Trans. Geosci. Remote Sens., vol. 48, no. 5, pp. 2403–2420, May 2010.\n[2] S. Quan, B. Xiong, D. Xiang, L. Zhao, S. Zhang, and G. Kuang, “Eigenvalue-based urban area extraction using polarimetric SAR data,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 11, no. 2, pp. 458–471, Feb. 2018.\n[3] R. J. Radke, S. Andra, O. Al-Kofahi, and B. Roysam, “Image change detection algorithms: A systematic survey,” IEEE Trans. Image Process., vol. 14, no. 3, pp. 294–307, Mar. 2005. [4] M. Gong, J. Zhao, J. Liu, Q. Miao, and L. Jiao, “Change detection in synthetic aperture radar images based on deep neural networks,” IEEE Trans. Neural Netw. Learn. Syst., vol. 27, no. 1, pp. 125–138, Jan. 2016.\n[5] B. Hou et al., “Unsupervised change detection in SAR image based on gauss-log ratio image fusion and compressed projection,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 7, no. 8, pp. 3297–3317, 2014.\n[6] M. Gong, Y. Cao, and Q. Wu, “A neighborhood-based ratio approach for change detection in SAR images,” IEEE Geosci. Remote Sens. Lett., vol. 9, no. 2, pp. 307–311, 2012.\n[7] H. Li, T. Celik, N. Longbotham, and W. J. Emery, “Gabor feature based unsupervised change detection of multitemporal SAR images based on two-level clustering,” IEEE Geosci. Remote Sens. Lett., vol. 12, no. 12, pp. 2458–2462, Dec. 2015.\n[8] L. Jia, M. Li, P. Zhang, Y. Wu, and H. Zhu, “SAR image change detection based on multiple kernel k-means clustering with local- neighborhood information,” IEEE Geosci Remote Sens. Lett., vol. 13, no. 6, pp. 856–860, Jun. 2016.\n[9] M. Gong, Z. Zhou, and J. Ma. “Change detection in synthetic aperture radar images based on image fusion and fuzzy clustering,” IEEE Trans. Image Process., vol. 21, no. 4, pp. 2141–2151, Apr. 2012.\n[10] Q.Liu,R.Hang,H.Song,andZ.Li,“Learningmultiscaledeepfeatures for high-resolution satellite image scene classification,” IEEE Tran. Geosci. Remote Sens., vol. 56, no. 1, pp. 117–126, Jan. 2018.\n[11] Q. Wang, Z. Yuan, Q. Du, and X. Li, “GETNET: a general end-to-end 2-D CNN framework for hyperspectral image change detection,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 3–13, Jan. 2019.\n[12] X. Li, Z. Yuan, and Q. Wang, “Unsupervised deep noise modeling for hyperspectral image change detection,” Remote Sens., vol. 11, no. 3, 258, Jan. 2019.\n[13] F. Liu, L. Jiao, X. Tang, S. Yang, W. Ma, and B. Hou, “Local restricted convolutional neural network for change detection in polarimetric SAR images,” IEEE Trans. Neural Netw. Learn. Syst., vol. 30, no. 3, pp. 1–16, Mar. 2019.\n[14] Y. Gao, F. Gao, J. Dong, and S. Wang. “Transferred deep learning for sea ice change detection from synthetic aperture radar images,” IEEE Geosci. Remote Sens. Lett., vol. 16, no. 10, pp. 1655–1659, Oct. 2019.\n[15] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3859— 3869.\n[16] M. E. Paoletti et al., “Capsule networks for hyperspectral image classi- fication,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 4, pp. 2145– 2160, Apr. 2019.\n[17] K. Zhu et al., “Deep convolutional capsule network for hyperspectral image spectral and spectral-spatial classification,” Remote Sens., vol. 11, no. 3, pp. 1–28, Mar. 2019, Art. no. 223.\n[18] J.Hu,L.Shen,andG.Sun,“Squeeze-and-excitationnetworks,”inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2018, pp. 7132–7141.\n[19] F. Gao, J. Dong, B. Li, and Q. Xu, “Automatic change detection in synthetic aperture radar images based on PCANet,” IEEE Geosci. Remote Sens. Lett., vol. 13, no. 12, pp. 1792–1796, Dec. 2016.\n[20] W. Song, S. Li, L. Fang, and T. Lu, “Hyperspectral image classification with deep feature fusion network,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 6, pp. 3173–3184, Jun. 2018.\n\n\n\n\nGao, Yunhao, Feng Gao, Junyu Dong, and Heng-Chao Li. 2021. “SAR Image Change Detection Based on Multiscale Capsule Network.” IEEE Geoscience and Remote Sensing Letters 18 (3): 484–88. https://doi.org/10.1109/LGRS.2020.2977838."
  },
  {
    "objectID": "RUSACD.html#abstract",
    "href": "RUSACD.html#abstract",
    "title": "RUSACD",
    "section": "Abstract",
    "text": "Abstract\nLa detección de cambios en áreas pequeñas a partir de un radar de apertura sintética (SAR) es una tarea muy difícil. En este trabajo, se propone un enfoque robusto no supervisado para la detección de cambios en áreas pequeñas a partir de imágenes SAR multitemporales utilizando el aprendizaje profundo. En primer lugar, se desarrolla un método de reconstrucción de superpíxeles multiescala para generar una imagen de diferencia (DI), que puede suprimir el ruido de moteado de forma efectiva y mejorar los bordes mediante la explotación de información local y espacialmente homogénea. En segundo lugar, se propone un algoritmo de clustering de c-means difuso de dos etapas para dividir los píxeles de la DI en clases cambiadas, no cambiadas e intermedias con una estrategia de clustering paralela. A continuación, se construyen parches de imagen pertenecientes a las dos primeras clases como muestras de entrenamiento de pseudoetiquetas, y los parches de imagen de la clase intermedia se tratan como muestras de prueba. Por último, se diseña y entrena una red neuronal convolucional wavelet (CWNN) para clasificar las muestras de prueba en clases modificadas o no modificadas, Se combina con una red convolucional generativa adversarial (DCGAN) para aumentar el número de clases cambiadas dentro de las muestras de entrenamiento de las pseudo-etiquetas. Los experimentos numéricos en cuatro conjuntos de datos reales de SAR demuestran la validez y la solidez del enfoque propuesto, logrando una precisión de hasta el 99,61% para la detección de cambios en áreas pequeñas.\n\nKeywords:\n\nChange detection; Synthetic aperture radar; Difference image; Fuzzy c-means algorithm; Deep learning.\n\n\nRepostorio: https://github.com/River-sh/A-robust-unsupervised-small-area-change-detection"
  },
  {
    "objectID": "RUSACD.html#diagrama-general",
    "href": "RUSACD.html#diagrama-general",
    "title": "RUSACD",
    "section": "Diagrama General",
    "text": "Diagrama General\n\n\n\nFlowchart illustrating the proposed RUSACD methodology."
  },
  {
    "objectID": "RUSACD.html#pros-y-contras",
    "href": "RUSACD.html#pros-y-contras",
    "title": "RUSACD",
    "section": "Pros y Contras",
    "text": "Pros y Contras\nPros:\n\nNo supervisado\nÁreas pequeñas\nMejor performance casos con bajo ruido (moteado)\nEvaluación de resultados con otros métodos\n\nContras:\n\nNo testeado con Sentine-1 (SAR COSMO-SkyMed entre 2016 y 2017 y SAR ERS-2 entre 2003 y 2004)\nRepositorio en matlab"
  },
  {
    "objectID": "RUSACD.html#datasets",
    "href": "RUSACD.html#datasets",
    "title": "RUSACD",
    "section": "Datasets",
    "text": "Datasets\nSe utilizaron cuatro conjuntos de datos SAR multitemporales reales (Tabla 1) para evaluar el rendimiento del enfoque propuesto. Tres de estos cuatro conjuntos de datos fueron adquiridos sobre la provincia de Guizhou en China por el sensor SAR COSMO-SkyMed en junio de 2016 y abril de 2017. Como se muestra en la Fig. 6, el primer par de imágenes (conjunto de datos A) con el mapa de referencia del suelo consiste en montañas y un río, el segundo par de imágenes (conjunto de datos B) con el mapa de referencia del suelo incluye colinas, llanuras y edificios, y el tercer par de imágenes (conjunto de datos C) con el mapa de referencia del suelo es principalmente llanuras. El último par de imágenes (conjunto de datos D) fue adquirido sobre la ciudad de San Francisco, Estados Unidos, por el sensor SAR ERS-2 en agosto de 2003 y mayo de 2004 (Fig. 7). En los conjuntos de datos A, B y C se aprecia el ruido de moteado, que supone un gran reto para la detección de cambios. A partir de los mapas de referencia redondos de la Fig. 6-7, está claro que la proporción de píxeles cambiados es extremadamente pequeña en comparación con los píxeles no cambiados. El conjunto de datos D es un punto de referencia en el que hay menos ruido y los cambios no pueden considerarse como cambios de área pequeños. Este conjunto de datos se utiliza para demostrar la solidez de nuestro enfoque propuesto.\n\n\n\nDataset A, B and C. (a) Image acquired in June 2016, (b) Image acquired in April 2017. (c) Ground\n\n\n\n\n\nDataset D. (a) Image acquired in August 2003. (b) Image acquired in May 2004. (c) Ground reference map."
  },
  {
    "objectID": "RUSACD.html#resultados",
    "href": "RUSACD.html#resultados",
    "title": "RUSACD",
    "section": "Resultados",
    "text": "Resultados\nPara demostrar la eficacia del RUSACD propuesto, se compararon cuatro métodos de referencia, entre ellos: PCA k-means (PCAK) (Celik, 2009), relación basada en la vecindad y máquina de aprendizaje extremo (NRELM) (Gao et al., 2016), extracción de características de Gabor y FCM con PCANet (GFPCANet) (Gao et al., 2016), y FCM con CWNN (FCWNN) (Gao et al., 2019). También aplicamos TCCFCM para agrupar el MSRDI en dos categorías (cambiado y sin cambios) como método de detección de cambios MTCCFCM de referencia. Los resultados experimentales se muestran en la Fig. 8, mientras que las métricas de precisión cuantitativas se indican en la Tabla 2. Además, otros métodos utilizados para la comparación en el conjunto de datos D (Tabla 3) incluyen la detección guiada por saliencia con agrupación de k-means (SGK) (Zheng et al., 2017), el autoencoder apilado y FCM con CNN (SAEFCNN) (Gong et al., 2017), la red neuronal profunda guiada por saliencia (SGDNN) (Geng et al., 2019) y la prueba de relación de verosimilitud generalizada adaptativa (AGLRT) (Zhuang et al., 2020).\nTODO: Revisar y agregar bibliografia\n\n\n\nTable 2. Comparison of the final change maps on dataset A-D. (a) PCAK. (b) NRELM. (c) GFPCANet. (d) FCWNN. (e) MTCCFCM. (f) RUSACD. (g) Ground reference map.\n\n\n\n\n\nAccuracy metrics for the different change detection methods. Best results are shown in bold.\n\n\n\n\n\nTable 3. Accuracy metrics for the different change detection methods on dataset D. Best results are shown in bold.\n\n\n\n\n\n\nZhang, Xinzheng, Hang Su, Ce Zhang, Xiaowei Gu, Xiaoheng Tan, and Peter M. Atkinson. 2020. “Robust Unsupervised Small Area Change Detection from SAR Imagery Using Deep Learning.”"
  },
  {
    "objectID": "insumos.html#turberas",
    "href": "insumos.html#turberas",
    "title": "Tratamiento de Insumos",
    "section": "Turberas",
    "text": "Turberas\n\n\n\n\nIntroducción\nA continuación se explica los procesos realizados a la base recibida llamada Turberas_COT_MMA_CHILOE.kml correspondiente a un archivo espacial de polígonos que representan tu turberas (Section 1) en el Archipiélago de Chiloé, en la Región de los Lagos Chile. Correspondiente a 858 sitios definidos como turberas.\n\n\nRevisión de Cambios\nEn estapa se realizó una inspección visual a los 858 sitios definidos como turberas, y mediante fotointerpretación indetificar si existen cambios en dos periodos diferentes, el tipo y fecha aproximada de cambio, para lo cual se se utilizó el software de Google Earth Pro valiendose de la funcionalidad de serie de tiempo.\nDel procedimento anterior, se concluye que son muy pocos los que sitios que sufrieron alteración especificamente 28, y no todos los cambios correspondía a actividades extractivas de las turbas, sino que también por cambio de suelo (Figure 1), construcciones, deforestación ect.\n\n\n\n\n\n\n\n(a) Antes de cambio de suelo\n\n\n\n\n\n\n\n(b) Antes de cambio de suelo\n\n\n\n\nFigure 1: Cambio por Actividad Antrópica, pero no se observa que sea una tubera\n\n\nEn algunos casos efectivamente se trataba de cambió por actividades extractivas como se observa en la siguiente Figure 2\n\n\n\n\n\n\n\n(a) Antes de alteación\n\n\n\n\n\n\n\n(b) Después de la alteación\n\n\n\n\nFigure 2: Cambio por Actividad Antrópica correspondiente a actividades extractivas de turba\n\n\nEn el siguiente mapa Figure 3 se observa los 28 sitios que sufrieron cambios por actividad antrópica.\n\n\n\n\n\n\nFigure 3: Mapa de Sitios de Cambio en Turberas de Chiloé\n\n\n\nLa siguiente tabla Table 1 se observa todos los registros de sitios que han sufrido cambios por actividad antrópica con sus respectiva información de tipología y temporalidad.\n\n\n\n\nTable 1:  Tabla de Cambios por actividades antrópica \n \n  \n    Name \n    Description \n    ID \n    Origen_ID \n    Mes \n    Year \n    Observacion \n  \n \n\n  \n    change_1 \n     \n    1 \n    149 \n    NA \n    2020 \n    Construcción \n  \n  \n    change_2 \n     \n    2 \n    197 \n    10 \n    2020 \n    Desforestación \n  \n  \n    change_3 \n     \n    3 \n    234 \n    12 \n    2021 \n    Desforestación \n  \n  \n    change_4 \n     \n    4 \n    232 \n    10 \n    2019 \n    Camino \n  \n  \n    change_5 \n     \n    5 \n    231 \n    10 \n    2019 \n    Camino \n  \n  \n    change_6 \n     \n    6 \n    249 \n    01 \n    2022 \n    Parcelación \n  \n  \n    change_7 \n     \n    7 \n    363 \n    03 \n    2021 \n    Agua \n  \n  \n    change_8 \n     \n    8 \n    372 \n    10 \n    2013 \n    Extracción \n  \n  \n    change_9 \n     \n    9 \n    372 \n    09 \n    2013 \n    Por Definir \n  \n  \n    change_10 \n     \n    10 \n    479 \n    03 \n    2021 \n    Deforestación \n  \n  \n    change_11 \n     \n    11 \n    423 \n    NA \n    NA \n    Por Definir \n  \n  \n    change_13 \n     \n    13 \n    439 \n    NA \n    NA \n    Empresa \n  \n  \n    change_14 \n     \n    14 \n    612 \n    01 \n    2017 \n    Parcelación \n  \n  \n    change_15 \n     \n    15 \n    554 \n    11 \n    2019 \n    Extracción \n  \n  \n    change_16 \n     \n    16 \n    551 \n    11 \n    2019 \n    Extracción \n  \n  \n    change_17 \n     \n    17 \n    521 \n    01 \n    2017 \n    Extracción \n  \n  \n    change_18 \n     \n    18 \n    582 \n    04 \n    2018 \n    Extracción \n  \n  \n    change_19 \n     \n    19 \n    565 \n    03 \n    2021 \n    Cambio Color \n  \n  \n    change_20 \n     \n    20 \n    595 \n    03 \n    2021 \n    Extracción \n  \n  \n    change_21 \n     \n    21 \n    680 \n    03 \n    2021 \n    Cambio Color \n  \n  \n    change_22 \n     \n    22 \n    685 \n    03 \n    2021 \n    Alta Tensión \n  \n  \n    change_23 \n     \n    23 \n    720 \n    03 \n    2021 \n    Extracción \n  \n  \n    change_24 \n     \n    24 \n    658 \n    10 \n    2011 \n    Cultivo \n  \n  \n    change_25 \n     \n    25 \n    689 \n    03 \n    2021 \n    Por Definir \n  \n  \n    change_26 \n     \n    26 \n    NA \n    03 \n    2021 \n    Extracción \n  \n  \n    change_27 \n     \n    27 \n    829 \n    08 \n    2021 \n    Construcción \n  \n  \n    change_28 \n     \n    28 \n    844 \n    01 \n    2018 \n    Extracción \n  \n\n\n\n\n\n\n\n\n\nVisualización de los cambios detectados por actividad antrópica Figure 4\n\n\n\n\n\n\nFigure 4: Mapa de Sitios de Cambio en Turberas de Chiloé por extracción"
  },
  {
    "objectID": "remote_sensing.html#métodos-de-adquisición-de-datos-de-teledetección",
    "href": "remote_sensing.html#métodos-de-adquisición-de-datos-de-teledetección",
    "title": "Percepción Remota",
    "section": "Métodos de adquisición de datos de teledetección",
    "text": "Métodos de adquisición de datos de teledetección\nLa adquisición de datos de teledetección es el trabajo más importante y costoso del proceso de teledetección. Es esencial disponer de imágenes de teledetección en formato digital para aplicar el procesamiento digital de imágenes. Hay dos formas fundamentales de adquirir la imagen digital:\n\nCapturar la imagen con un dispositivo de teledetección en formato analógico y, a continuación, convertirla a formato digital.\nTomar una imagen digital de la imagen teledetectada, como la obtenida por el sistema de sensores Landsat 7 Thematic Mapper.\n\nSe denomina resolución (radiométrica) a la capacidad de un sistema de presentar la información en la menor cantidad discretamente separable en términos de espacio (espacial), banda de longitud de onda EMR (espectral), tiempo (temporal) y/o cantidad de radiación. Algunos de los principales parámetros que determinan la naturaleza de los datos de teledetección recogidos se analizan en las subsecciones siguientes.\n\nInformación espectral y resolución\nLa dimensión (tamaño) y el número de bandas o canales específicos del espectro electromagnético a los que es sensible un sensor de teledetección son resoluciones espectrales. Los datos se recogen en muchas bandas del espectro electromagnético mediante un sensor multiespectral de teledetección. Los datos son recogidos en cientos de bandas espectrales por un dispositivo hiperespectral de teledetección. La energía es registrada en cientos de bandas por un sistema de teledetección ultraespectral. Las bandas suelen elegirse para maximizar el contraste entre el objeto y el fondo. En consecuencia, una selección adecuada de las bandas puede aumentar la probabilidad de recuperar la información necesaria a partir de los datos del sensor remoto. Cuanto mayor sea la estrechez de la banda, mayor será la resolución espectral."
  },
  {
    "objectID": "remote_sensing.html#información-espacial-y-resolución",
    "href": "remote_sensing.html#información-espacial-y-resolución",
    "title": "Percepción Remota",
    "section": "Información espacial y resolución",
    "text": "Información espacial y resolución\nLa menor distancia angular o lineal entre dos objetos que puede resolver un sistema de teledetección se denomina resolución espacial. Utilizando la información espacial de la imagen, se puede estimar la cantidad de autocorrelación espectral. Un píxel es un pequeño punto de la superficie terrestre que un sensor puede observar como distinto de su entorno. Es un elemento detector o una rendija cuando se proyecta sobre el suelo. En otras palabras, el segmento de terreno detectado en un momento dado es la resolución espacial del escáner. A veces se ha denominado elemento de resolución del terreno (GRE). La resolución espacial con la que se obtienen los datos afecta a la capacidad de distinguir diversas características y medir su extensión. Según la regla habitual, la resolución espacial debe ser inferior a la mitad del tamaño del objeto de interés más pequeño.\n\nInformación temporal y resolución\nLa resolución temporal se refiere a la frecuencia con la que un sensor registra una imagen de un área específica. Para muchas aplicaciones, una alta resolución temporal es crítica. La resolución temporal se refiere a la capacidad del satélite para extraer la misma zona desde el mismo ángulo de visión en diferentes momentos. La resolución temporal de un sensor viene determinada por varios elementos, como las capacidades del satélite/sensor, el solapamiento de franjas y la latitud.\n\n\nInformación radiométrica y resolución\nLa sensibilidad de los detectores a pequeñas variaciones en la energía electromagnética se conoce como resolución radiométrica. Una alta resolución radiométrica mejora la probabilidad de una teledetección más precisa de los fenómenos. La medida de un sensor le permite distinguir la más mínima variación en la reflectancia/remitancia espectral entre diferentes objetivos. El número de niveles de cuantización y la radiancia de saturación determinan la resolución radiométrica. Cuantos más niveles haya, más detallada será la información que se digitalice.\n\n\nInformación de polarización\nLas características de polarización de la radiación electromagnética recogida por los sistemas de teledetección pueden utilizarse para investigar los recursos de la Tierra. En general, cuanto más fuerte es la polarización, más lisa es la superficie del objeto.\n\n\nInformación angular\nEl ángulo de incidencia se ha relacionado tradicionalmente con la energía entrante que ilumina el paisaje y el ángulo de salida del terreno hacia el sistema sensor. La naturaleza bidireccional de la adquisición de datos de teledetección influye en el espectro y la polarización de la luz del sensor recogida por el sistema de teledetección [3,4]."
  },
  {
    "objectID": "remote_sensing.html#adquisición-de-datos-de-teledetección",
    "href": "remote_sensing.html#adquisición-de-datos-de-teledetección",
    "title": "Percepción Remota",
    "section": "Adquisición de datos de teledetección",
    "text": "Adquisición de datos de teledetección\nLos dos métodos importantes de adquisición de datos de teledetección son la fotografía aérea y la adquisición de datos de imágenes de satélite. Las fotografías aéreas son las instantáneas de la tierra tomadas por cámaras calibradas en un instante concreto de tiempo en formato analógico. Mediante el proceso de digitalización, este formato analógico se convierte en formato digital. La fotografía aérea puede tomarse desde el espacio, desde aviones a gran o poca altura, o desde plataformas cercanas al suelo. Cada toma aérea contiene información vital para el usuario al margen.\n\nFotografías verticales: El dispositivo fotográfico, es decir, la cámara, se fija lo más recto posible hacia abajo cuando se toma una fotografía vertical. La tolerancia permitida desde la línea de plomada (perpendicular) al eje de la cámara es normalmente de +3 grados. El eje del objetivo es casi perpendicular a la superficie terrestre, cubre un área limitada en una sola toma vertical y se asemeja mucho a un cuadrado o un rectángulo [4].\nOblicuo alto: La cámara se inclina unos 60 grados con respecto a la vertical cuando se realiza una toma oblicua alta. Se emplea sobre todo en la creación de cartas aeronáuticas y tiene una aplicación militar menor. El área de terreno cubierta es trapezoidal en las imágenes oblicuas altas; el horizonte siempre es visible en las fotografías oblicuas altas.\nOblicuo bajo: Esta fotografía se toma con la cámara en un ángulo de 30 grados respecto a la vertical. Se ha utilizado para investigar una zona antes de un ataque, como sustituto de un mapa o como complemento de un mapa. El área de terreno cubierta es un trapecio en oblicuo bajo y la cobertura es bastante limitada [5].\nTrimetrogon: Es un compuesto de tres fotos tomadas simultáneamente, una vertical y dos oblicuas altas, en dirección perpendicular a la trayectoria de vuelo. Las imágenes oblicuas, tomadas en un ángulo de 60 grados respecto a la vertical, solapan la fotografía vertical, dando lugar a composiciones que van de horizonte a horizonte."
  },
  {
    "objectID": "remote_sensing.html#sensores-de-teledetección",
    "href": "remote_sensing.html#sensores-de-teledetección",
    "title": "Percepción Remota",
    "section": "Sensores de teledetección",
    "text": "Sensores de teledetección\ncontinuar …\nDocumento conceptual Detección de cambios en datos de imágenes de teledetección comparando métodos algebraicos y de aprendizaje automático"
  },
  {
    "objectID": "sar_sentinel_s1.html#resumen",
    "href": "sar_sentinel_s1.html#resumen",
    "title": "Sentinel 1",
    "section": "Resumen",
    "text": "Resumen\nLa misión Sentinel-1 es el Observatorio Radar Europeo de la iniciativa conjunta Copernicus de la Comisión Europea (CE) y la Agencia Espacial Europea (ESA). Copernicus es una iniciativa europea para la puesta en marcha de servicios de información relacionados con el medio ambiente y la seguridad. Se basa en los datos de observación recibidos de los satélites de observación de la Tierra y la información terrestre.\nLa misión Sentinel-1 incluye imágenes en banda C que operan en cuatro modos de imagen exclusivos con diferente resolución (hasta 5 m) y cobertura (hasta 400 km). Ofrece capacidad de doble polarización, tiempos de revisita muy cortos y una rápida entrega de productos. Para cada observación, se dispone de mediciones precisas de la posición y la actitud de la nave espacial.\nEl radar de apertura sintética (SAR) tiene la ventaja de operar en longitudes de onda que no se ven obstaculizadas por la nubosidad o la falta de iluminación, y puede adquirir datos sobre un lugar durante el día o la noche en todas las condiciones meteorológicas. Sentinel-1, con su instrumento C-SAR, puede ofrecer una vigilancia fiable y repetida de una zona amplia.\nLa misión está compuesta por una constelación de dos satélites, Sentinel-1A y Sentinel-1B, que comparten el mismo plano orbital.\nSentinel-1 está diseñado para trabajar en un modo de operación preprogramado y libre de conflictos, obteniendo imágenes de todas las masas terrestres mundiales, zonas costeras y rutas marítimas en alta resolución y cubriendo el océano mundial con viñetas. Esto garantiza la fiabilidad del servicio requerida por los servicios operativos y un archivo de datos consistente a largo plazo construido para aplicaciones basadas en series temporales largas."
  },
  {
    "objectID": "sar_sentinel_s1.html#objetivos-de-la-misión",
    "href": "sar_sentinel_s1.html#objetivos-de-la-misión",
    "title": "Sentinel 1",
    "section": "Objetivos de la misión",
    "text": "Objetivos de la misión\nLa misión proporciona una capacidad operativa independiente para la cartografía radar continua de la Tierra.\nLa misión Sentinel-1 está diseñada para proporcionar una mayor frecuencia de revisita, cobertura, puntualidad y fiabilidad para los servicios operativos y las aplicaciones que requieren series temporales largas.\nLa misión proporcionará una capacidad de interferometría operativa gracias a los estrictos requisitos impuestos a la precisión de la actitud, al conocimiento de la actitud y de la órbita, y a la precisión de los tiempos de toma de datos.\nLa constelación cubrirá la totalidad de las masas terrestres del mundo con periodicidad quincenal, las zonas de hielo marino, las zonas costeras de Europa y las rutas marítimas con periodicidad diaria y el océano abierto de forma continua mediante imágenes de las olas.\nEl instrumento SAR del Sentinel-1 y su corto tiempo de revisita harán avanzar enormemente las capacidades de los usuarios y proporcionarán datos de forma rutinaria y sistemática para la vigilancia marítima y terrestre, la respuesta a emergencias, el cambio climático y la seguridad.\nSe espera que cada satélite Sentinel-1 transmita datos de observación de la Tierra durante al menos 7 años y tenga combustible a bordo para 12 años.\nEl Documento de Requisitos de la Misión (MRD) del Sentinel-1 describe en detalle todos los requisitos específicos de la misión."
  },
  {
    "objectID": "sar_sentinel_s1.html#orbita",
    "href": "sar_sentinel_s1.html#orbita",
    "title": "Sentinel 1",
    "section": "Orbita",
    "text": "Orbita\nSentinel-1 se encuentra en una órbita casi polar, sincrónica al sol, con un ciclo de repetición de 12 días y 175 órbitas por ciclo para un solo satélite. Tanto Sentinel-1A como Sentinel-1B comparten el mismo plano orbital con una diferencia de fase orbital de 180°. Con ambos satélites en funcionamiento, el ciclo de repetición es de seis días.\n\n\n\nRevisit time for S-1A and S-1B in Days per Revisit\n\n\nEn particular, para la interferometría, Sentinel-1 requiere un estricto control de la órbita. El posicionamiento del satélite a lo largo de la órbita debe ser preciso, con apuntamiento y sincronización entre pares interferométricos. El control del posicionamiento de la órbita de Sentinel-1 se define mediante un “tubo” orbital fijo en la Tierra, de 50 m (RMS) de radio, alrededor de una trayectoria operativa nominal. El satélite se mantiene dentro de este “tubo” durante la mayor parte de su vida operativa.\n\n\n\nTubo orbital para la interferometría"
  },
  {
    "objectID": "sar_sentinel_s1.html#escenario-de-producción",
    "href": "sar_sentinel_s1.html#escenario-de-producción",
    "title": "Sentinel 1",
    "section": "Escenario de producción",
    "text": "Escenario de producción\nLas operaciones del segmento terrestre del Sentinel-1 implementan un escenario de producción de la misión predefinido. Este escenario prevé el procesamiento sistemático y la difusión en línea de todos los datos del Sentinel-1 adquiridos en los modos IW, EW y SM en productos GRD de Nivel 0 y Nivel 1.\nEsto se complementa con el procesamiento sistemático y la difusión en línea de:\n\nProductos SLC de Nivel-1 para todos los datos adquiridos en Modo Onda (activos desde octubre de 2016 para Sentinel-1B y desde mayo de 2017 para Sentinel-1A).\nProductos SLC de Nivel-1 para todos los datos adquiridos en modo IW sobre áreas regionales específicas, que ha evolucionado durante las operaciones para cubrir todos los datos adquiridos en modo IW\nProductos oceánicos de nivel 2 para todos los datos adquiridos en modo Ola (activos desde julio de 2015 para Sentinel-1A y desde octubre de 2016 para Sentinel-1B)\nProductos oceánicos de nivel 2 para todos los datos adquiridos en modo IW y EW sobre zonas regionales específicas\n\nLas zonas geográficas en las que se generan sistemáticamente los productos SLC de nivel 1 de Sentinel-1A han evolucionado gradualmente desde la apertura del acceso a los datos en línea el 3 de octubre de 2014 y se han ido incrementando paulatinamente en los meses siguientes con el objetivo de que los productos SLC estén disponibles para todos los datos adquiridos en modo IW.\nLa evolución de las zonas en las que se han generado sistemáticamente productos SLC de nivel 1 de Sentinel-1A y se han puesto a disposición para la descarga de datos en línea desde la apertura del acceso a los datos se resume en una serie de mapas etiquetados en el tiempo (descargar la evolución de los mapas SLC).\nCada mapa está asociado a la fecha a partir de la cual los datos IW adquiridos sobre las nuevas áreas enumeradas en el mapa han sido sistemáticamente procesados y puestos a disposición como productos SLC de nivel 1. Cada vez que un área se añadía al escenario de procesamiento sistemático del SLC, seguía formando parte de este escenario en el futuro.\nA continuación se ofrece un resumen de la producción sistemática de IW de Sentinel-1 y del acceso a los datos en línea:\nPara Sentinel-1A:\n\nDesde el 28.07.2015: Todos los datos de IW sobre masas de hielo y tierra están disponibles sistemáticamente como SLC\nDesde el 14.04.2016: Todos los datos de IW están disponibles sistemáticamente como SLC\n\nPara Sentinel-1B:\n\nDesde el 26.09.2016 (apertura del acceso a los datos S1B): Todos los datos de IW están disponibles sistemáticamente como SLC\n\nAdemás, todos los datos de Sentinel-1A adquiridos antes del 28 de julio de 2015 y no procesados originalmente a SLC (es decir, los datos adquiridos fuera de las áreas regionales predefinidas de SLC) se han procesado hacia atrás durante 2016 y se han puesto a disposición para el acceso a los datos en línea. Como resultado, los productos SLC de nivel 1 están disponibles para todos los datos de Sentinel-1 adquiridos en modo IW.\nLas áreas geográficas en las que los datos de Sentinel-1 IW y EW se procesan sistemáticamente a productos OCN de nivel 2 y se ponen a disposición para el acceso a los datos en línea también han evolucionado con el tiempo. Esta evolución se resume en una serie de mapas etiquetados en el tiempo (descargar la evolución de los mapas IW/EW OCN).\nEl 100% de los datos IW y SM adquiridos sobre masas terrestres en todo el mundo se elaboran sistemáticamente a productos OCN de nivel 1 y se ponen a disposición."
  },
  {
    "objectID": "sar_sentinel_s1.html#cobertura-geográfica",
    "href": "sar_sentinel_s1.html#cobertura-geográfica",
    "title": "Sentinel 1",
    "section": "Cobertura geográfica",
    "text": "Cobertura geográfica\nUn solo satélite Sentinel-1 podrá cartografiar el mundo entero una vez cada 12 días. La constelación de dos satélites ofrece un ciclo de repetición exacta de 6 días. La constelación tendrá una frecuencia de repetición (ascendente/descendente) de 3 días en el ecuador, menos de 1 día en el Ártico y se espera que proporcione cobertura sobre Europa, Canadá y las principales rutas marítimas en 1-3 días, independientemente de las condiciones meteorológicas. Los datos del radar se entregarán a los servicios de Copernicus una hora después de su adquisición.\n\n\n\nCobertura Geogràfica"
  },
  {
    "objectID": "sar_sentinel_s1.html#intrumentos-a-bordo",
    "href": "sar_sentinel_s1.html#intrumentos-a-bordo",
    "title": "Sentinel 1",
    "section": "Intrumentos a bordo",
    "text": "Intrumentos a bordo\nSentinel-1 lleva un único instrumento de radar de apertura sintética en banda C que opera a una frecuencia central de 5,405 GHz. Incluye una antena activa phased array de orientación derecha que proporciona un rápido escaneo en elevación y azimut, una capacidad de almacenamiento de datos de 1 410 Gb y una capacidad de enlace descendente en banda X de 520 Mbit/s.\nEl instrumento C-SAR soporta el funcionamiento en polarización dual (HH+HV, VV+VH) implementado a través de una cadena de transmisión (conmutable a H o V) y dos cadenas de recepción paralelas para la polarización H y V. Los datos de doble polarización son útiles para la clasificación de la cubierta terrestre y las aplicaciones del hielo marino.\nSentinel-1 funciona en cuatro modos de adquisición exclusivos:\n\nStripmap (SM)\nInterferométrico de banda ancha (IW)\nEspectro extra ancho (EW)\nModo de ondas (WV).\n\n\n\n\nModos de adquisión de imágenes de Sentinel-1\n\n\nLos principales modos sin conflicto son IW sobre tierra y WV sobre mar abierto.\n\nModo Stripmap (SM)\nEl modo de imagen Stripmap se proporciona para la continuidad con las misiones ERS y Envisat. El modo Stripmap proporciona una cobertura con una resolución de 5 m por 5 m sobre una estrecha franja de 80 km. Se puede seleccionar una de las seis franjas de imágenes cambiando el ángulo de incidencia del haz y el ancho del haz de elevación.\n\n\nModo de hilera ancha interferométrica (IW)\nEl modo interferométrico de franja ancha (IW) permite combinar una gran anchura de franja (250 km) con una resolución geométrica moderada (5 m por 20 m). El modo IW toma imágenes de tres sub-bandas utilizando la Observación del Terreno con Escáneres Progresivos SAR (TOPSAR). Con la técnica TOPSAR, además de dirigir el haz en el rango como en SCANSAR, el haz también se dirige electrónicamente de atrás hacia adelante en la dirección acimut para cada ráfaga, evitando el festoneado y dando como resultado una imagen de mayor calidad. La interferometría está garantizada por un solapamiento suficiente del espectro Doppler (en el dominio acimutino) y del espectro del número de onda (en el dominio de la elevación). La técnica TOPSAR garantiza una calidad de imagen homogénea en toda la franja.\nEl modo IW es el modo de adquisición por defecto sobre tierra.\n\n\nModo de barrido extra ancho (EW)\nEl modo de imagen de franja extra ancha está destinado a los servicios operativos marítimos, de hielo y de zonas polares en los que se requiere una amplia cobertura y tiempos de revisita cortos. El modo EW funciona de forma similar al modo IW, empleando una técnica TOPSAR que utiliza cinco sub-surcos en lugar de tres, lo que resulta en una resolución menor (20 m por 40 m). El modo EW también se puede utilizar para la interferometría como en el modo IW.\n\n\nModo Onda (WV)\nEl modo Wave del Sentinel-1, junto con los modelos globales de olas oceánicas, puede ayudar a determinar la dirección, la longitud de onda y las alturas de las olas en los océanos abiertos.\nLas adquisiciones en el modo de olas se componen de imágenes de mapa de franjas de 20 km por 20 km, adquiridas alternativamente en dos ángulos de incidencia diferentes. Las imágenes de olas se adquieren cada 100 km, con imágenes en el mismo ángulo de incidencia separadas por 200 km."
  },
  {
    "objectID": "sar_sentinel_s1.html#información-general",
    "href": "sar_sentinel_s1.html#información-general",
    "title": "Sentinel 1",
    "section": "Información General",
    "text": "Información General"
  },
  {
    "objectID": "sar_sentinel_s1.html#referencias",
    "href": "sar_sentinel_s1.html#referencias",
    "title": "Sentinel 1",
    "section": "Referencias",
    "text": "Referencias\nhttps://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-1-sar\nhttps://sentinels.copernicus.eu/web/sentinel/missions/sentinel-1/instrument-payload\nhttps://medium.com/@HarelDan/x-marks-the-spot-579cdb1f534b"
  },
  {
    "objectID": "ecosistemas.html#sec-turbera",
    "href": "ecosistemas.html#sec-turbera",
    "title": "Ecosistemas Protegidos",
    "section": "Turba",
    "text": "Turba\nLas turberas solo cubren el 3 % de la superficie terrestre del planeta pero almacenan más carbono que todos los bosques de la Tierra – si se mantienen humedas.\n\nDefinición:\n\nEl término turba debe ser entendido como un sedimento natural de tipo fitógeno, poroso, no consolidado, constituido por materia orgánica parcialmente descompuesta, acumulada en un ambiente saturado de agua. De esta forma, se puede entender al concepto de turbera como un depósito de turba con un espesor de, al menos, 30 cm (Hauser 1996)\n\nFormación:\n\nSegún (Hauser 1996), el origen de las turberas se encuentra en las eras glaciares del Pleistoceno, cuando grandes extensiones de casquetes glaciares cubrieron el valle central de la Región de Los Lagos, incluyendo a la Isla Grande de Chiloé. El posterior retiro de los glaciares dejó masas de agua tierra adentro, formando los grandes lagos y lagunas glaciares que en la actualidad componen el paisaje de la región.\nEn el caso de Chiloé, zona en la que se establecieron las condiciones climáticas ideales para el desarrollo del musgo del género Sphagnum, lo que permitió la acumulación de materia orgánica en depresiones del relieve de la isla con alto contenido de humedad Figure 1 (a). Este proceso de acumulación del musgo se consolidó en la formación de extensas turberas Figure 1 (b) y Figure 1 (c) . [Hauser (1996)]\n\n\n\n\n\nProceso de formación de turberas de origen glaciar (caso de Chiloé). Fuente: (Schofield W.B 1985)\n\n\n\nBotánicamente (Chiloé):\n\nBotánicamente, el pompón pertenece al Reino de las Plantas, a la División Bryophyta, a la Clase Musci y a la Familia de las Sphagnaceas. Esta familia comprende sólo un género, Sphagnum, compuesto por más de 300 especies descritas. En el archipiélago de Chiloé conviven varias especies de este género. La más abundante es S. magellanicum, que se caracteriza por su color rojo, talla relativamente robusta y hojas con ápice obtuso. Suele cubrir grandes superficies con mal drenaje en terrenos abiertos o cubriendo el suelo de los tepuales (bosques formados por la mirtácea Tepualia stipularis), donde se desarrolla con extraordinario vigor. Existen además, al menos 4 especies que se han identificado en la zona norte de la Isla: S. fimbriatum, S. falcatulum, S. recurvum y S. cuspidatum var. cuspidatum. Adicionalmente, la literatura cita otras 2 especies para la Isla: S. acutifolium y S. subnitens.Todas estas especies son de difícil identificación, siendo su morfología celular y la anatomía foliar la base de su clasificación (Zegers et al. 2006)\n\nCaracterización:\n\nUna de las características relevantes de las turberas de Sphagnum es que presenta una matriz continua superficial de musgos sobre una capa de turba que puede alcanzar varios metros de profundidad (Díaz et al. 2008). Según el mismo autor, entre otras características relevantes de este tipo de turberas se encuentran:\n\nLa turba que la compone es de origen vegetal y se encuentra en distintos estados de descomposición anaeróbica. Figure 1\nEl estrato superficial es biológicamente activo, conformado por asociaciones de especies, entre las que predominan plantas con gran capacidad para retener humedad Figure 1 (b).\nEl musgo Sphagnum forma un ambiente pobre en nutrientes (baja concentración de nitrógeno), ácido, anóxico y frío, lo que previene la presencia de hongos y bacterias que descomponen al material muerto Figure 1 (c).\nTiene una gran capacidad de absorción de agua (hasta 20 veces su peso seco en agua) Figure 1 (e)\nSu fuente de agua proviene de ríos y/o de la lluvia Figure 1 (d).\nEs un ecosistema de humedal con flora y fauna única y especializada.\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n(f)\n\n\n\n\nFigure 1: Fuente: (Treimun 2017)\n\n\n\n\nExtracción:\n\nComo Problema …\n\n\nTO-DO:\n\nDescripción de prdocesod e extracción y conexión con problema medio ambiental\nDescripción Completa Bosque Esclerofilo (? ROI)\nDescripción Completa Humedales Urbanos\n\nAgregar conceptos papers:\n\ncabezas_evaluation_2015 Evaluation of impacts of management in an anthropogenic peatland using field and remote sensing data (Cabezas et al. 2015)\nLopatin et al. - 2019 - Using aboveground vegetation attributes as proxies.pdf (Lopatin et al. 2019)\nLopatin et al. - 2022 - Disturbance alters relationships between soil carb.pdf (Lopatin et al. 2022)\n\n\n\n\n\nCabezas, Juli’an, Mauricio Galleguillos, Ariel Vald’es, Juan P. Fuentes, Cecilia P’erez, and Jorge F. Perez-Quezada. 2015. “Evaluation of Impacts of Management in an Anthropogenic Peatland Using Field and Remote Sensing Data.” Ecosphere 6 (12): 1–24. https://doi.org/10.1890/ES15-00232.1.\n\n\nDíaz, María F., Juan Larraín, Gabriela Zegers, and Carolina Tapia. 2008. “Floristic and Hydrological Characterization of Chiloé Island Peatlands, Chile.” Revista Chilena de Historia Natural 81 (4): 455–68. https://doi.org/10.4067/S0716-078X2008000400002.\n\n\nHauser, Arturo. 1996. “Los depósitos de turba en Chile y sus perspectivas de utilización.” Revista Geológica de Chile 23 (2) : 217-229., 13. http://www.andeangeology.cl/index.php/revista1/article/view/2208.\n\n\nLopatin, Javier, Roc’ıo ArayaNANAL’opez, Mauricio Galleguillos, and Jorge F. PereNAzNAQueza. 2022. “Disturbance Alters Relationships Between Soil Carbon Pools and Aboveground Vegetation Attributes in an Anthropogenic Peatland in Patagonia.” Ecology and Evolution 12 (3). https://doi.org/10.1002/ece3.8694.\n\n\nLopatin, Javier, Teja Kattenborn, Mauricio Galleguillos, Jorge F. Perez-Quezada, and Sebastian Schmidtlein. 2019. “Using Aboveground Vegetation Attributes as Proxies for Mapping Peatland Belowground Carbon Stocks.” Remote Sensing of Environment 231 (September): 111217. https://doi.org/10.1016/j.rse.2019.111217.\n\n\nSchofield W.B. 1985. “Introduction to Bryology.”\n\n\nTreimun, John. 2017. “Turberas de Chiloé, Ministerio del Medio Ambiente, Chile.”\n\n\nZegers, Gabriela, Juan Larraín, María Francisca Díaz, and Juan Armesto. 2006. “Impacto Ecológico y Social de La Explotación de Pomponales y Turberas de Sphagnum En La Isla Grande de Chiloé.” http://biblioteca.cehum.org/handle/CEHUM2018/1389."
  },
  {
    "objectID": "data-cube.html#datacube-entrenamiento",
    "href": "data-cube.html#datacube-entrenamiento",
    "title": "Data Cube",
    "section": "DataCube Entrenamiento",
    "text": "DataCube Entrenamiento\nRepositorio contiene una serie de notebooks para funcionar con el DataCube Chile. Está dividido en niveles de dificultad y por ahora, sólo está disponible el entrenamiento básico, que sienta los primeros lineamientos para comenzar a trabajar en el cubo de datos.\nhttps://github.com/Data-Observatory/DataCubeTrainingBasic"
  },
  {
    "objectID": "recursos.html#torchgeo",
    "href": "recursos.html#torchgeo",
    "title": "Recursos",
    "section": "TorchGeo",
    "text": "TorchGeo\nGeospatial deep learning with TorchGeo\n\nTorchGeo es una biblioteca de dominio de PyTorch que proporciona conjuntos de datos, muestreadores, transformaciones y modelos pre-entrenados específicos para datos geoespaciales.\nLink:\n\nPágina Oficial\nGithub Proyecto"
  },
  {
    "objectID": "recursos.html#raster-vision",
    "href": "recursos.html#raster-vision",
    "title": "Recursos",
    "section": "Raster Vision",
    "text": "Raster Vision\nGithub"
  },
  {
    "objectID": "recursos.html#links-por-explorar",
    "href": "recursos.html#links-por-explorar",
    "title": "Recursos",
    "section": "Links Por explorar",
    "text": "Links Por explorar\nhttps://dymaxionlabs-eng.medium.com/how-to-create-a-land-cover-model-for-south-america-in-4-steps-67e57d3dae64\n\nhttps://courses.spatialthoughts.com/end-to-end-gee.html#module-4-change-detection\nhttps://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1\nhttps://www.youtube.com/results?search_query=change+detection+gee\narquiologíahttps://code.earthengine.google.com/?scriptPath=users%2Fdenisberroeta%2FGEE_CIT_dbg%3Acc\nhttps://www.youtube.com/watch?v=wDBcTOTAwOc\nhttps://www.youtube.com/watch?v=5oONMB0UPWc\nhttps://developers.google.cn/earth-engine/tutorials/tutorial_forest_01\nhttps://appliedsciences.nasa.gov/join-mission/training/english/arset-using-google-earth-engine-land-monitoring-applications\nhttps://appliedsciences.nasa.gov/sites/default/files/2021-06/GEE_Land_Part3.pdf\nhttps://www.youtube.com/watch?v=KyjNhAvQS2s\nhttps://paperswithcode.com/task/change-detection-for-remote-sensing-images\n\nhttps://github.com/likyoo/Siam-NestedUNet\ncontaminación\n\nhttps://appliedsciences.nasa.gov/join-mission/training/english/arset-high-resolution-no2-monitoring-space-tropomi\n\nPrograma de la NASA https://appliedsciences.nasa.gov/join-mission/training/english/arset-using-google-earth-engine-land-monitoring-applications\n\nbigearth net\nsentinel 1 Change detection no supervisado S1 si existe cambio algorimo global, unet - rmask\ntorch geo"
  }
]